# Lesson 12: Introduction to Function Approximation Methods


**Unit 4: Learning Outcomes**  
By the end of this unit, you will be able to: 

1. **Apply** RL techniques to control an agent in complex environment representations.  
2. **Compare** the trade-offs of on-policy and off-policy learning algorithms.  
3. **Evaluate** the convergence properties of RL algorithms in both tabular and function approximation settings, considering their practical limitations.  

---
In this and subsequent lessons, we will learn about finding solutions for states represented by features (observations). In a real-world environment, it is hard and unrealistic to expect to be able to identify a state fully. Usually, we can only partially observe the state via a set of features that can help us to recognise or distinguish a state, but that does not mean that we have guarantees that the state is fully identifiable or that the features are unique for each different state. Nevertheless, we should be able to deal with these spaces. After all, RL is meant for real-world problems. We must come to terms with the issue that this partial observability can be dealt with effectively in most practical cases by using suitable features. Because the states that share inner properties usually tend to have similar features, by dealing with these states via these features, we usually succeed in generalising to other similar states. To that end, dealing with the features via a set of parameters is natural, as we did in several machine learning modules. Of course, we can bring along other non-parametric models, such as Gaussian processes. However, this falls outside the scope of our RL treatment. 
In some cases, we might need to generalise the RL framework from the MDP’s underlying assumption of full observability to Partially Observable Markov Decision Processes or POMDP. In most cases, this might be unnecessary, and we can get away without dealing with the intricacy of POMDP. Again, POMDP is outside the scope of our coverage.
What is left is then to move from a tabular to a parametric representation by adjusting the update rules we have dealt with so far to the parametric representation instead of the tabular one. 

## Plan
From a practical perspective, we start by generalising from a tabular to an equivalent vectorised form via a one-hot representation. In this representation, each vector component corresponds with a state in the state space, so the vector size is the same as the number of states, and we use a one-hot encoding. To encode (represent) a state, we turn on (set to 1) the corresponding component (all other features are 0’s) and the update for the weights’ parameters will be applied similar to what we did for the tabular. In fact, each component's weight is really the value function for the corresponding state. We should get results identical to those we obtained for the problems we tackled in the tabular form—random walk, the maze, the cliff walking, etc. The benefit of the one-hot encoding is that it is a vectorised version of the tabular representation and constitutes the first step towards generalising to a linear parametric model. Once we are satisfied that our vectorised form is working, we can then come up with different representations for the problems that have a dimensionality different than the state space, usually smaller and more concise than a continuous state space dimension, which can be infinite (countable or uncountable) or intractable, which is one of the advantages of parametric models. 

What is left is to find a suitable representation of the problem at hand. 
Consequently, we continue converging towards a more general representation by covering state aggregation. In state aggregation, we group a set of states and represent them all in one feature, and we use one-hot encoding again. This time, we turn a feature on whenever the agent is at *any* of the group of states corresponding to it. 
We then move to other constructions, including coarse coding and tile coding, to deal with state representation. The book chapter covers these very well, and you are advised to read about them in section 9.5. Selecting a construction is usually a matter of trial and error as well as preference. We can use a model that helps us automatically find suitable features for the problem at hand. 

This is where neural networks can come to the rescue.  Neural networks can automatically find a suitable set of features internally through their hidden layers, as we know. Whether to use a deep neural network or a shallow one depends on the complexity of the problem and the complexity that we would want our state representation to have. Neural networks help extract helpful features in the early layers that will be used in later layers to extract a correct value function. In this case, we do not need to use tile or similar coding because the network automatically learns the feature representation. The main issue that we often face, which acted as a deterrence for researchers to use neural networks for a long time, is that there are far fewer guarantees of convergence for neural networks, while several strong guarantees exist for linear regression models. Nevertheless, this impasse was broken with the introduction of DQN, and in practice, RL algorithms have proven resilient to neural networks in general. The picture is different for off-policy methods, and fewer guarantees exist for the tabular, let alone the function approximation methods. 


Note that we are dealing mainly with regression from an ML perspective. This is because the value function is just a function that maps the state to an actual number, so the answer is continuous values (not discrete). It is just a value, so we face a regression problem. When we deal with function approximation, TD (r + V(s')-V(s)) and other methods do not take the gradient of the target V(s'); we only differentiate V(s). This is because we are bootstrapping, and in ML(supervised learning), the target is a fixed value that is not differentiable (while in RL, it is). Remember, for example, that the update rule for the Monte Carlo method involves (Gt-V(s)), which, calculating its gradient, involves differentiating V(s) only. Because of this, we call the methods that do not take the target's gradient *a semi-gradient method*. We then use the action-value function to establish a suitable policy as we did earlier (ex., e-greedy). 

Finally, we move to a different type of RL *control* algorithms that deal directly with the policy and attempts to learn a policy directly without going through the intermediate step of fitting a value function. These methods might use regression or classification models called policy-gradient methods because they differentiate the policy $\pi$ itself, not $Q$. These methods are amongst the most promising methods in RL and have proven more resilience with more convergence guarantees than action-value functions methods. 

Ok, with all of that in mind, let us get started.

