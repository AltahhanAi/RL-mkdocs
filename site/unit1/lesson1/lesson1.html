
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
        <meta name="author" content="Dr Abdulrahman Altahhan">
      
      
      
        <link rel="prev" href="../../index.html">
      
      
        <link rel="next" href="../lesson2/lesson2.html">
      
      
      <link rel="icon" href="../../../docs/img/favicon.jpg">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.4">
    
    
      
        <title>1. Tabular Methods - Reinforcement Learning for Autonomous Agents..</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.8608ea7d.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../css/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#lesson-1-introduction-to-tabular-methods-in-reinforcement-learning" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
        <aside class="md-banner">
          <div class="md-banner__inner md-grid md-typeset">
            
            
    <div id="versionIndicator"><b>Version:</b> 04.06.21.a</div>
    <img id="customlogo" src="../../../img/logo.svg" alt="University of Leeds logo.">

          </div>
          
        </aside>
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../index.html" title="Reinforcement Learning for Autonomous Agents.." class="md-header__button md-logo" aria-label="Reinforcement Learning for Autonomous Agents.." data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Reinforcement Learning for Autonomous Agents..
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              1. Tabular Methods
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../index.html" class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="lesson1.html" class="md-tabs__link">
          
  
  Unit 1

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit2/lesson5/lesson5.html" class="md-tabs__link">
          
  
  Unit 2

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit3/lesson8/lesson8.html" class="md-tabs__link">
          
  
  Unit 3

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit4/lesson12/lesson12.html" class="md-tabs__link">
          
  
  Unit 4

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit5/lesson15/lesson15.html" class="md-tabs__link">
          
  
  Unit 5

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit6/lesson18/lesson18.html" class="md-tabs__link">
          
  
  Unit 6

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../index.html" title="Reinforcement Learning for Autonomous Agents.." class="md-nav__button md-logo" aria-label="Reinforcement Learning for Autonomous Agents.." data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Reinforcement Learning for Autonomous Agents..
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../index.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Unit 1
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Unit 1
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    1. Tabular Methods
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="lesson1.html" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    1. Tabular Methods
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#tabular-representation-and-example" class="md-nav__link">
    <span class="md-ellipsis">
      Tabular Representation and Example
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lesson2/lesson2.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2. K-Arm Bandit
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lesson3/lesson3.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    3. MDP
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lesson4/lesson4.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    4. ROS
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 2
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Unit 2
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit2/lesson5/lesson5.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    5. Dynamic Programming
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit2/lesson6/lesson6.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    6. Monte Carlo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit2/lesson7/lesson7.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    7. Mobile Robots
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 3
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Unit 3
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson8/lesson8.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    8. Temporal Difference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson9/lesson9.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    9. n-Step Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson10/lesson10.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    10. Planning in RL(optional)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson11/lesson11.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    11. Localisation and SLAM
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 4
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Unit 4
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit4/lesson12/lesson12.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    12. Function Approximation Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit4/lesson13/lesson13.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    13. Linear Approximation for Prediction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit4/lesson14/lesson14.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    14. Linear Approximation for Control
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 5
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Unit 5
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit5/lesson15/lesson15.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    15. Linear Approximation with Eligibility Traces(prediction and control)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit5/lesson16/lesson16.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    16. Nonlinear Approximation for Control
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit5/lesson17/lesson17.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    17. Application on Robot Navigation
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 6
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Unit 6
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit6/lesson18/lesson18.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    18. Application on Games(optional)
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#tabular-representation-and-example" class="md-nav__link">
    <span class="md-ellipsis">
      Tabular Representation and Example
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="lesson-1-introduction-to-tabular-methods-in-reinforcement-learning">Lesson 1: Introduction to Tabular Methods in Reinforcement Learning</h1>
<p>In this unit, we cover the main framework of reinforcement learning, namely Markov Decision Processes(MDPs). RL has gained a lot of attention in recent years due to its unmatched ability to tackle difficult control problems with a minimal assumption about the setting and the environment that an agent works in. Controlling an agent (simulated robot) is not trivial and can often require a specific setup and strong assumptions about its environment that make the corresponding solution sometimes either difficult to attain or impractical in real scenarios. In RL, we try to minimise these assumptions and require that only the environment adheres to the Markov property. In simple terms, the Markov property assumes that inferring what to do (taking action) in a specific state can be fully specified by looking at this state and does not depend on other states.</p>
<p>In RL, we deal with states, actions and rewards. State space is the space the agent operates in, whether physical or virtual. The state can represent something specific in the environment, an agent configuration or both. The actions are the set of decisions available for the agent to take. An RL agent's main aim is to attain, usually via learning, a cohesive policy that allows it to achieve a specific goal. This policy <span class="arithmatex">\(π\)</span> can take a simple form <span class="arithmatex">\(π(s)=a\)</span> or symbolically <span class="arithmatex">\(s → a\)</span>, which means if the agent is in state <span class="arithmatex">\(s\)</span>, then take action <span class="arithmatex">\(a\)</span>. This type of policy is deterministic because the agent will definitely take the action <span class="arithmatex">\(a\)</span> if it is in state <span class="arithmatex">\(s\)</span>. Another type of policy that we deal with is stochastic policy. A stochastic policy takes the form of <span class="arithmatex">\(π(a|s)\)</span>, which represents the probability of taking action <span class="arithmatex">\(a\)</span> given that the agent is in state <span class="arithmatex">\(s\)</span>. For such a policy, the agent draws from the set of available actions according to the conditional probability, which we call its policy.</p>
<p>The reward function can take the simple form of <span class="arithmatex">\(r(s,a)\)</span> or symbolically <span class="arithmatex">\((s,a) →r\)</span>, which is interpreted as: if the agent is in state s and applied action <span class="arithmatex">\(a\)</span> it obtains a reward <span class="arithmatex">\(r\)</span>. This reward can be actual or expected. The general setting that we deal with is the probabilistic one with the form of <span class="arithmatex">\(p(r|s,a)\)</span>, which provides us with the probability of the agent obtaining reward r given it was in state s and applied action <span class="arithmatex">\(a\)</span>. Another conditional probability that we deal with takes the form of <span class="arithmatex">\(p(s’|s,a)\)</span>, which is the probability of transitioning to state <span class="arithmatex">\(s’\)</span> given the agent was in state s and applied action <span class="arithmatex">\(a\)</span>. This is called the transition probability. Both the transition and reward probabilities can be inferred from a more general probability that specifies the dynamics of the environment. This joint conditional probability takes the form of <span class="arithmatex">\(p(s’,r|s,a)\)</span>, which is interpreted as the joint probability of transitioning to state s’ and obtaining reward r given that the agent was in state s and applied action <span class="arithmatex">\(a\)</span>. We will deal mostly with these dynamics in the second lesson of this unit. Bear in mind that obtaining the dynamics is difficult or intractable in most cases except for the simplest environment. Nevertheless, the dynamics are very useful theoretically for understanding the basic ideas of RL.</p>
<p>The reward function is strongly linked to the task that the agent is trying to achieve, this is the minimal information provided for the agent to indicate to it whether it is on the right track or not. We need to be careful not to devise a complicated reward function. This is usually unnecessary, and when doing so, we may be directly solving the problem for the agent. Instead, we would want the agent to solve the problem by utilising its simple reward signal and interacting with its environment to gain experience and sharpen and improve its decision-making policy. Improving its decision-making involves two things: evaluating its current policy and changing it in a way that will improve its performance. This is where the sum of rewards an agent can collect while achieving a task plays a major role. In RL, we link the policy to maximising the discounted sum of the rewards an agent can obtain while moving towards achieving a task. The reward can be negative, and in this case, the agent will be trying to minimise the sum of negative rewards that it is collecting before terminating. The termination happens when the agent achieves the required task, has taken a pre-specified number of steps or consumed pre-set computational resources. An example of a good simple reward is giving a robot a reward of 1 when it reaches a goal location and 0 otherwise. Another example is giving a robot a negative reward of -1 for each step it takes before reaching the goal location. Both of these rewards have their advantages, and we will study them in our exercises. The advantage of the first type is its simplicity and inforced sparsity, but it can take longer to explore the environment. The advantage of the second type is that it provides intermediate information that the agent can immediately utilise to improve its policy before reaching the goal location. The second type is specifically useful for online learning and when we want to alleviate the agent from having to change its starting position to cover all possible starts. The first type is useful for studying the capabilities of a learning algorithm with minimum information.</p>
<p>The sum of rewards from a specific state to the end of the task is called the return. Because we do not know how long the agent may take to achieve the task and to fairly maximise the rewards and allow for variability, we discount the rewards so that more recent rewards have more effect than later rewards. Nevertheless, our aim is to maximise the rewards in the long run. To be more explicit, we call the sum of discounted rewards from the current state to the end of the task the return that the agent will obtain in the future- the whole idea is to predict these rewards and be able to collect as much as possible.</p>
<p>The task we give the agent can be a continuous, infinite interaction with the environment. It can also take the form of a task with a specific goal or termination state, and when it is reached, the task is naturally aborted and repeated or reattempted. The former is called a continuous task, while the latter is called an episodic task. Episodic tasks have a start and end, while continuous tasks have a start but never end. The horizon (the number of steps) of episodic tasks is finite, while the horizon for continuous tasks is infinite. It turns out that discounting is necessary for theoretical guarantees for continuous tasks, which must be strictly &lt; 1, while for episodic tasks, it can be set to 1. We will deal mostly with episodic tasks.</p>
<p>The function that specifies each state's return when the agent <em>follows a specific policy</em> is called the <strong>value function</strong> and is denoted as <span class="arithmatex">\(v(s)\)</span>. On the other hand, we call the function that specifies the return of the current state given that the agent takes a specific action <span class="arithmatex">\(a\)</span> and then just follows a specific policy, the action-value function and is denoted as <span class="arithmatex">\(q(s,a)\)</span>. These two functions provide a great utility for us in guiding our search for an optimal policy. The action-value function can be directly utilised to provide a policy that greedily chooses the action with the maximum value; when we do so, we call the algorithms that follow this pattern a value-function algorithm. Alternatively, we can maximise the policy directly without having to maximise the expected return first. These types of algorithms that do so are called policy-gradient algorithms and depend on approximation.</p>
<p>We will largely deal with two types of algorithms: tabular algorithms, which use a tabular representation of the value function v and the action-value function q. These will be covered in the first three units. In the subsequent units, we cover the second type of algorithms that deal with function approximation, where representing the state and actions in a table is intractable or impractical. In these algorithms, we generalise the tabular algorithms that we cover in the first three units to be able to use the models that we covered in machine learning, such as linear regression models or neural networks</p>
<h2 id="tabular-representation-and-example">Tabular Representation and Example</h2>
<p>In the first three units, you will learn about the main ideas of reinforcement learning that use lookup tables. These tables identify a certain action that will be taken in a certain state and are called a policy. Our main concern is to design algorithms that <em>learn</em> a suitable policy.</p>
<p><strong>A Policy Lookup Table For Commuting to Work</strong></p>
<table>
<thead>
<tr>
<th>State</th>
<th>Action</th>
</tr>
</thead>
<tbody>
<tr>
<td>have energy and have time</td>
<td>walk</td>
</tr>
<tr>
<td>have energy and no   time</td>
<td>cycle</td>
</tr>
<tr>
<td>no  energy  and have time</td>
<td>take a bus</td>
</tr>
<tr>
<td>no  energy  and no   time</td>
<td>take a taxi</td>
</tr>
</tbody>
</table>
<p>More formally, in the first three units, we will study important reinforcement learning algorithms that use <em>tabular policy representation</em>. These algorithms learn a lookup table that identifies a suitable action that can be taken for a certain state. Our main concern is to design practical and efficient algorithms that can <em>learn</em> an optimal policy either via direct interaction with an environment, via an environment model that captures the dynamics of the environment, or both. An optimal policy is a policy that maximises the sum of discounted rewards obtained by following this policy.</p>
<p>The main underlying framework we assume is a Markov Decision Process (MDP). In a nutshell, this framework assumes that the probability of moving to the next state is only dependent on the current state and not on past states.</p>
<p>We start by covering non-associated problems, such as K-armed Bandit. These problems have no states. This will help us focus on the action space as it will be isolated from the state's effect. We then study how to solve MDP problems using Dynamic Programming (DP). DP assumes that we have a model of the environment that the agent is acting on. By model, we mean the dynamics or probabilities of landing in a state and obtaining a specific reward given an action and a previous state. This kind of conditional probability provides a comprehensive framework to reach an optimal policy, but it is hard to obtain in the real world. We then reside in sample methods, particularly Monte Carlo. This method allows us to gather samples of an agent running, making environmental decisions, and collecting rewards. We use averages to obtain an estimate of the discounted returns of an episode. We conclude our units by developing suitable core classes that allow us to study and demonstrate the advantages and disadvantages of these and other methods.</p>
<p>A policy can be deterministic or stochastic, below we show both for an associative problem. A deterministic policy is a special case of a stochastic policy with all of its actions' probabilities being 0 except one action.</p>
<!-- <table><tr><th>Deterministic Associative Policy</th><th> Stochastic Associative Policy</th><th> Non-Associative Policy</th></tr>
<tr><td> -->

<table>
<thead>
<tr>
<th>State</th>
<th>Action</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="arithmatex">\(S_1\)</span></td>
<td><span class="arithmatex">\(A_1\)</span></td>
</tr>
<tr>
<td><span class="arithmatex">\(S_2\)</span></td>
<td><span class="arithmatex">\(A_2\)</span></td>
</tr>
<tr>
<td><span class="arithmatex">\(S_3\)</span></td>
<td><span class="arithmatex">\(A_2\)</span></td>
</tr>
</tbody>
</table>
<!-- </td><td> -->

<table>
<thead>
<tr>
<th>State</th>
<th>Action</th>
<th>Action's Probability</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="arithmatex">\(S_1\)</span></td>
<td><span class="arithmatex">\(A_1\)</span></td>
<td>.8</td>
</tr>
<tr>
<td><span class="arithmatex">\(S_1\)</span></td>
<td><span class="arithmatex">\(A_2\)</span></td>
<td>.2</td>
</tr>
<tr>
<td><span class="arithmatex">\(S_2\)</span></td>
<td><span class="arithmatex">\(A_1\)</span></td>
<td>.6</td>
</tr>
<tr>
<td><span class="arithmatex">\(S_2\)</span></td>
<td><span class="arithmatex">\(A_2\)</span></td>
<td>.4</td>
</tr>
<tr>
<td><span class="arithmatex">\(S_3\)</span></td>
<td><span class="arithmatex">\(A_1\)</span></td>
<td>0.</td>
</tr>
<tr>
<td><span class="arithmatex">\(S_3\)</span></td>
<td><span class="arithmatex">\(A_2\)</span></td>
<td>1.</td>
</tr>
</tbody>
</table>
<!-- </td><td> -->

<table>
<thead>
<tr>
<th>Action</th>
<th>Action's Probability</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="arithmatex">\(A_1\)</span></td>
<td>.7</td>
</tr>
<tr>
<td><span class="arithmatex">\(A_2\)</span></td>
<td>.2</td>
</tr>
<tr>
<td><span class="arithmatex">\(A_3\)</span></td>
<td>.1</td>
</tr>
</tbody>
</table>
<!-- </td></tr></table> -->












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright Abdulrahman Altahhan
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.sections"], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.f1b6f286.min.js"></script>
      
        <script src="../../javascript/tablecontentsoverride.js"></script>
      
        <script src="../../javascript/config.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>