
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
        <meta name="author" content="Abdulrahman Altahhan,  2025.">
      
      
      
        <link rel="prev" href="../lesson1/lesson1.html">
      
      
        <link rel="next" href="../lesson3/lesson3.html">
      
      
      <link rel="icon" href="../../../img/favicon.jpg">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.4">
    
    
      
        <title>2. K-Arm Bandit - Reinforcement Learning for Autonomous Agents..</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.8608ea7d.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../css/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#lesson-2-understanding-q-via-k-armed-bandit" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
        <aside class="md-banner">
          <div class="md-banner__inner md-grid md-typeset">
            
            
    <div id="versionIndicator"><b>Version:</b> 04.06.21.a</div>
    <img id="customlogo" src="../../../img/logo.svg" alt="University of Leeds logo.">

          </div>
          
        </aside>
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../index.html" title="Reinforcement Learning for Autonomous Agents.." class="md-header__button md-logo" aria-label="Reinforcement Learning for Autonomous Agents.." data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Reinforcement Learning for Autonomous Agents..
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              2. K-Arm Bandit
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../index.html" class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../lesson1/lesson1.html" class="md-tabs__link">
          
  
  Unit 1

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit2/lesson5/lesson5.html" class="md-tabs__link">
          
  
  Unit 2

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit3/lesson8/lesson8.html" class="md-tabs__link">
          
  
  Unit 3

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit4/lesson12/lesson12.html" class="md-tabs__link">
          
  
  Unit 4

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit5/lesson15/lesson15.html" class="md-tabs__link">
          
  
  Unit 5

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit6/lesson18/lesson18.html" class="md-tabs__link">
          
  
  Unit 6.1

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../index.html" title="Reinforcement Learning for Autonomous Agents.." class="md-nav__button md-logo" aria-label="Reinforcement Learning for Autonomous Agents.." data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Reinforcement Learning for Autonomous Agents..
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../index.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Unit 1
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Unit 1
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lesson1/lesson1.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1. Tabular Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    2. K-Arm Bandit
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="lesson2.html" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    2. K-Arm Bandit
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#motivating-example" class="md-nav__link">
    <span class="md-ellipsis">
      Motivating Example
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Motivating Example">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#scenario-a" class="md-nav__link">
    <span class="md-ellipsis">
      Scenario a
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scenario-b" class="md-nav__link">
    <span class="md-ellipsis">
      Scenario b
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scenario-c" class="md-nav__link">
    <span class="md-ellipsis">
      Scenario c
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#stationary-probability" class="md-nav__link">
    <span class="md-ellipsis">
      Stationary probability
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#stationary-and-non-stationary-reward" class="md-nav__link">
    <span class="md-ellipsis">
      Stationary and non-stationary reward
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#averaging-the-rewards-for-greedy-and-greedy-policies" class="md-nav__link">
    <span class="md-ellipsis">
      Averaging the Rewards for greedy and ε-greedy policies
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#10-armed-bandet-testbed-estimating-q-via-samples-average" class="md-nav__link">
    <span class="md-ellipsis">
      10-armed Bandet Testbed: Estimating Q via Samples Average
    </span>
  </a>
  
    <nav class="md-nav" aria-label="10-armed Bandet Testbed: Estimating Q via Samples Average">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#generate-the-experiencesampling" class="md-nav__link">
    <span class="md-ellipsis">
      Generate the experience(sampling)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#learning-the-bandit-q-action-values" class="md-nav__link">
    <span class="md-ellipsis">
      Learning the bandit Q action-values
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multiple-runs-aka-trials" class="md-nav__link">
    <span class="md-ellipsis">
      Multiple runs (aka trials)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#action-selection-policy-comparison" class="md-nav__link">
    <span class="md-ellipsis">
      Action Selection (Policy) Comparison:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#incremental-implementation" class="md-nav__link">
    <span class="md-ellipsis">
      Incremental Implementation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#non-stationary-problems" class="md-nav__link">
    <span class="md-ellipsis">
      Non-stationary Problems
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Non-stationary Problems">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#comparison" class="md-nav__link">
    <span class="md-ellipsis">
      Comparison
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#q_bandit-in-a-better-form" class="md-nav__link">
    <span class="md-ellipsis">
      Q_bandit in a better form
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Q_bandit in a better form">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#policies-exploration-vs-exploitation" class="md-nav__link">
    <span class="md-ellipsis">
      Policies: Exploration vs. Exploitation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimistic-initial-values" class="md-nav__link">
    <span class="md-ellipsis">
      Optimistic Initial Values
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conclusion" class="md-nav__link">
    <span class="md-ellipsis">
      Conclusion
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#your-turn" class="md-nav__link">
    <span class="md-ellipsis">
      Your Turn
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lesson3/lesson3.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    3. MDP
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lesson4/lesson4.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    4. ROS
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 2
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Unit 2
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit2/lesson5/lesson5.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    5. Dynamic Programming
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit2/lesson6/lesson6.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    6. Monte Carlo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit2/lesson7/lesson7.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    7. Mobile Robots
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 3
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Unit 3
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson8/lesson8.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    8. Temporal Difference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson9/lesson9.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    9. n-Step Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson10/lesson10.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    10. Planning in RL(optional)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson11/lesson11.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    11. Localisation and SLAM
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 4
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Unit 4
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit4/lesson12/lesson12.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    12. Function Approximation Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit4/lesson13/lesson13.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    13. Linear Approximation for Prediction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit4/lesson14/lesson14.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    14. Linear Approximation for Control
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 5
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Unit 5
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit5/lesson15/lesson15.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    15. Linear Approximation with Eligibility Traces(prediction and control)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit5/lesson16/lesson16.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    16. Nonlinear Approximation for Control
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit5/lesson17/lesson17.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    17. Application on Robot Navigation
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 6.1
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Unit 6.1
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit6/lesson18/lesson18.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    18. Application on Games(optional)
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#motivating-example" class="md-nav__link">
    <span class="md-ellipsis">
      Motivating Example
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Motivating Example">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#scenario-a" class="md-nav__link">
    <span class="md-ellipsis">
      Scenario a
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scenario-b" class="md-nav__link">
    <span class="md-ellipsis">
      Scenario b
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scenario-c" class="md-nav__link">
    <span class="md-ellipsis">
      Scenario c
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#stationary-probability" class="md-nav__link">
    <span class="md-ellipsis">
      Stationary probability
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#stationary-and-non-stationary-reward" class="md-nav__link">
    <span class="md-ellipsis">
      Stationary and non-stationary reward
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#averaging-the-rewards-for-greedy-and-greedy-policies" class="md-nav__link">
    <span class="md-ellipsis">
      Averaging the Rewards for greedy and ε-greedy policies
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#10-armed-bandet-testbed-estimating-q-via-samples-average" class="md-nav__link">
    <span class="md-ellipsis">
      10-armed Bandet Testbed: Estimating Q via Samples Average
    </span>
  </a>
  
    <nav class="md-nav" aria-label="10-armed Bandet Testbed: Estimating Q via Samples Average">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#generate-the-experiencesampling" class="md-nav__link">
    <span class="md-ellipsis">
      Generate the experience(sampling)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#learning-the-bandit-q-action-values" class="md-nav__link">
    <span class="md-ellipsis">
      Learning the bandit Q action-values
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multiple-runs-aka-trials" class="md-nav__link">
    <span class="md-ellipsis">
      Multiple runs (aka trials)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#action-selection-policy-comparison" class="md-nav__link">
    <span class="md-ellipsis">
      Action Selection (Policy) Comparison:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#incremental-implementation" class="md-nav__link">
    <span class="md-ellipsis">
      Incremental Implementation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#non-stationary-problems" class="md-nav__link">
    <span class="md-ellipsis">
      Non-stationary Problems
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Non-stationary Problems">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#comparison" class="md-nav__link">
    <span class="md-ellipsis">
      Comparison
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#q_bandit-in-a-better-form" class="md-nav__link">
    <span class="md-ellipsis">
      Q_bandit in a better form
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Q_bandit in a better form">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#policies-exploration-vs-exploitation" class="md-nav__link">
    <span class="md-ellipsis">
      Policies: Exploration vs. Exploitation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimistic-initial-values" class="md-nav__link">
    <span class="md-ellipsis">
      Optimistic Initial Values
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conclusion" class="md-nav__link">
    <span class="md-ellipsis">
      Conclusion
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#your-turn" class="md-nav__link">
    <span class="md-ellipsis">
      Your Turn
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="lesson-2-understanding-q-via-k-armed-bandit">Lesson 2- Understanding Q via K-armed Bandit</h1>
<p><img alt="image-3.png" src="image-3.png" /></p>
<p><img alt="image-2.png" src="image-2.png" /></p>
<p>In this lesson you will learn about the k-armed bandit problem and its applications in RL. This problem is useful in understanding the basics of RL, in particular it demonstrates how an algorithm can <em>learn</em> an action-value function that we normally denote as Q in RL. </p>
<p><strong>Learning outcomes</strong></p>
<ol>
<li>understand the role the action-value function plays in RL and its relationship with a policy</li>
<li>appreciate the difference between stationary and non-stationary problems</li>
<li>understand how to devise a samples averaging solution to approximate an action-value function</li>
<li>appreciate the different policies types and the role a policy plays in RL algorithms</li>
</ol>
<p><strong>Reading</strong>:
The accompanying reading of this lesson are <strong>chapters 1 and 2</strong> of our text book available online <a href="http://incompleteideas.net/book/RLbook2020.pdf">here</a>. Please note that we explain the ideas of this topic from a practical perspective and not from a theoretical perspective which is already covered in the textbook.</p>
<p>In this lesson we develop the basic ideas of dealing with actions and policies which are the distinctive elements that set RL apart from other machine learning sub-disciplines. We study a simple but effective problem of the k-armed bandit. This problem is manifested in other scenarios such as when a medical specialist wants to decide which treatment to give for a patient from a set of medicine, some of which he/she are trying for the first time(exploring). </p>
<p>Our toy problem is similar to a usual bandit but it is assumed that there is a set of k actions that the agent can choose from. We want to reach an effective policy that allows the agent to maximise its returns (wins). The bandit is assumed to have a Gaussian distribution around a mean reward that is different for each action (armed). Each time an armed is pulled (in our RL terminology we say an action is taken) the bandit will return a reward (positive or negative) by drawing from its Gaussian reward's distribution. The agent's task is to guess which action of these has the highest mean and pull it all the time to maximise its wins. Note that the distributions are fixed and not changing, although we can relax this assumption later.</p>
<p>Ok let us get started! </p>
<p>Below we import libraries that will be necessary for our implementation. For RL sampling and random number generation is of at most importance we will use them quite extensively. You have come across these in earlier module and if in doubt you can consult the library help available online.</p>
<div class="highlight"><pre><span></span><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="kn">import</span> <span class="n">arange</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">numpy.random</span><span class="w"> </span><span class="kn">import</span> <span class="n">rand</span><span class="p">,</span><span class="n">randn</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="n">randint</span><span class="p">,</span> <span class="n">choice</span><span class="p">,</span> <span class="n">uniform</span><span class="p">,</span> <span class="n">normal</span><span class="p">,</span> <span class="n">multivariate_normal</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tqdm</span><span class="w"> </span><span class="kn">import</span> <span class="n">trange</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
</code></pre></div>
<h2 id="motivating-example">Motivating Example</h2>
<p>Let us assume that we have an armeded bandit with two livers.</p>
<h3 id="scenario-a">Scenario a</h3>
<ul>
<li>We have a deterministic reward function that has a reward of -5 for action <span class="arithmatex">\(a_1\)</span> (pulling bandit 1)</li>
<li>We have a deterministic reward function that has a reward of 00 for action <span class="arithmatex">\(a_2\)</span> (pulling bandit 2)</li>
</ul>
<p>What is the optimal policy for this bandit?</p>
<h3 id="scenario-b">Scenario b</h3>
<ul>
<li>We have a nondeterministic reward function that has a reward of either -5 or 15 with equal probabilities of .5 for action <span class="arithmatex">\(a_1\)</span> (pulling bandit 1)</li>
<li>
<p>We have a nondeterministic reward function that has a reward of either 00 or 10 with equal probabilities of .5 for action <span class="arithmatex">\(a_2\)</span> (pulling bandit 2)</p>
</li>
<li>
<p>What is the net overall reward for actions <span class="arithmatex">\(a_1\)</span> and <span class="arithmatex">\(a_2\)</span>?</p>
</li>
<li>What is the optimal policy for this bandit?</li>
<li>How many optimal policies we have for this bandit?</li>
</ul>
<h3 id="scenario-c">Scenario c</h3>
<ul>
<li>We have a nondeterministic reward function that has a reward of either -5 or 15 with probabilities .4 and .6 for action <span class="arithmatex">\(a_1\)</span> (pulling bandit 1)</li>
<li>
<p>We have a nondeterministic reward function that has a reward of either 00 or 10 with probabilities .5 and .5 for action <span class="arithmatex">\(a_2\)</span> (pulling bandit 2)</p>
</li>
<li>
<p>What is the net overall reward for actions <span class="arithmatex">\(a_1\)</span> and <span class="arithmatex">\(a_2\)</span>?</p>
</li>
<li>What is the optimal policy for this bandit?</li>
<li>How many optimal policies we have for this bandit?</li>
<li>Can find a way to represent this </li>
</ul>
<p>In the code below, we use rand() and arange() functions. rand() is a random number generator function. Each time it is run, it will give a different number in the range [0,1]. arange(), on the other hand, is useful to give us a set of uniformly distributed numbers.</p>
<div class="highlight"><pre><span></span><code><span class="n">rand</span><span class="p">()</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>0.6801911659775486
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mf">.1</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="c1"># obtaining the value of an action a with binary reward</span>

<span class="k">def</span><span class="w"> </span><span class="nf">Q</span><span class="p">(</span><span class="n">pr</span><span class="o">=</span><span class="mf">.5</span><span class="p">,</span> <span class="n">r1</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">r2</span><span class="o">=-</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">R</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">N</span> <span class="o">=</span> <span class="mi">100000</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="n">R</span> <span class="o">+=</span> <span class="n">r1</span> <span class="k">if</span> <span class="n">rand</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="n">pr</span> <span class="k">else</span> <span class="n">r2</span>

    <span class="k">return</span> <span class="n">R</span><span class="o">/</span><span class="n">N</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">Pr</span> <span class="o">=</span> <span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mf">.1</span><span class="p">)</span>
<span class="n">Qs1</span> <span class="o">=</span> <span class="p">[</span><span class="n">Q</span><span class="p">(</span><span class="n">pr</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">)</span> <span class="k">for</span> <span class="n">pr</span> <span class="ow">in</span> <span class="n">Pr</span><span class="p">]</span>
<span class="n">Qs2</span> <span class="o">=</span> <span class="p">[</span><span class="n">Q</span><span class="p">(</span><span class="n">pr</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span>  <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">pr</span> <span class="ow">in</span> <span class="n">Pr</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Reawrd Probability&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Σ Reawrd Expecation&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Pr</span><span class="p">,</span> <span class="n">Qs1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Q1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Pr</span><span class="p">,</span> <span class="n">Qs2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Q2&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_18_0.png" /></p>
<p>As can be seen, the two bandits functions intersect with each other at the .5 probability, meaning they are equivalent for this probability. For other probabilities Bandit 1 is superior for pr&gt;.5 (and hence an optimal policy will be to select this bandit always), while Bandit 2 is superior for pr&lt;.5 (and hence an optimal policy will be to select this bandit always). But bear in mind that we do not know the underlying probability before hand, and we would need to try out both to obtain an estimation for their corresponding value functions to be able to come up with a suitable policy.</p>
<p>Ok, now we move into covering the different concepts of the multi armeded bandit in more details.</p>
<p>We start by developing simple functions for 
1. returning an action from a stationary policy and 
2. returning a simple fixed reward from a stationary distribution</p>
<h2 id="stationary-probability">Stationary probability</h2>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">stationary</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="p">[</span><span class="mf">.5</span><span class="p">,</span> <span class="mf">.2</span><span class="p">,</span> <span class="mf">.3</span><span class="p">]):</span>
    <span class="k">return</span> <span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">p</span><span class="p">)),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">stationary</span><span class="p">()</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>0
</code></pre></div>
<p>As we can see we passed a distribution for a set of 3 actions and the function chose from these 3 actions according to the distribution. Each time you run the above code it may give a different action. If you repeat the process you will find out that the action choices are distributed according to the passed distribution.</p>
<div class="highlight"><pre><span></span><code><span class="n">n</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">counts</span><span class="p">[</span><span class="n">stationary</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="p">[</span><span class="mf">.5</span><span class="p">,</span> <span class="mf">.2</span><span class="p">,</span> <span class="mf">.3</span><span class="p">])]</span><span class="o">+=</span><span class="mi">1</span>

<span class="nb">print</span><span class="p">((</span><span class="n">counts</span><span class="o">/</span><span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>[0.5 0.2 0.3]
</code></pre></div>
<h2 id="stationary-and-non-stationary-reward">Stationary and non-stationary reward</h2>
<p>Once an action is sampled, we can return its reward from a set of rewards. 
We can start by assuming that each action has a fixed reward. Below we show how to obtain the reward.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">reward</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">]):</span>
    <span class="k">return</span> <span class="n">r</span><span class="p">[</span><span class="n">a</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="n">reward</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">reward</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">reward</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>10 20 30
</code></pre></div>
<p>Such a reward will be very easy to guess for an observer by just recording each action occurrence once. To address this, we can assume that each action's reward can vary around a mean and a Gaussian, we get lots of variations that makes it difficult for an observer to know the expected reward directly.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">reward</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">20</span><span class="p">,</span> <span class="o">-</span><span class="mi">30</span><span class="p">,</span> <span class="mi">40</span><span class="p">]):</span>
    <span class="k">return</span> <span class="n">r</span><span class="p">[</span><span class="n">a</span><span class="p">]</span><span class="o">+</span><span class="n">r</span><span class="p">[</span><span class="n">a</span><span class="p">]</span><span class="o">*</span><span class="n">randn</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">reward</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">reward</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">reward</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>-10.097974534627859 -51.81451370896566 11.677690812495104
</code></pre></div>
<p>On average we still expect to obtain more reward by choosing the third action(2) and if we play enough (by sampling from the rewards by taking a specific action) we will obtain the mean of the reward. Of course in a real game this would have an immense loss on the player that it is not a practical worry.</p>
<div class="highlight"><pre><span></span><code><span class="n">rewards</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">100000</span>
<span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">rewards</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reward</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="p">(</span><span class="n">rewards</span><span class="o">/</span><span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>array([-19., -29.,  40.])
</code></pre></div>
<p>Note that we still call the above distribution as stationary since it is not changing. So the first case was a point based believe and the second was a stationary distribution. Often, we deal with non-stationary distribution where the distribution itself changes while the agent is interacting with the environment. For example, if the mean of the reward itself is changing then this becomes a non-stationary distribution and the problem becomes non-stationary. Non-stationary problems are common in RL and we will deal with them often. They occur as an artifact of the learning process itself where the agent estimation of the action-value function changes with time which in turn changes the agent policy. This will become clearer in this and other lessons.</p>
<h2 id="averaging-the-rewards-for-greedy-and-greedy-policies">Averaging the Rewards for greedy and ε-greedy policies</h2>
<p>Greedy policy is a simple policy that always picks the action with the highest action-value.
On the other hand, an ε-greedy policy is similar to a greedy policy, however it allows the agent to pick random exploratory actions from time to time. The percentage of those exploratory actions is designated by ε (epsilon). Usually, we set ε to .1 (10%) or .05 (5%). A third type of a greedy policy is a dynamic ε-greedy policy which anneals or decays the exploration factor ε with time. In practice, ε-greedy policy usually works fine and better than more sophisticated policies that strike balance between exploration and exploitation (taking the greedy action is called exploitation while taking other actions is called exploration). Regardless of how, we need to allow for some form of exploratory actions otherwise it would not be possible for the agent to improve its policy.</p>
<p>Below we show an implementation bandit function that uses an ε-greedy policy. nA denotes the number of actions (number of armeds to be pulled). Because we are only dealing with actions the Q function has the form of Q(a). The armed bandit is non-associative problem, meaning we do not deal with states. Later we will deal with associative problems where Q(s,a) has two inputs the state and the action. </p>
<p>Below we create a function that takes a bandit (in the form of a set of rewards each of which corresponds with an action) and generates a value Q that quantifies the value/benefit that we obtain by taking each action. This is a simple improvement over the code that we have just written earlier to obtain the expected reward of an action. This time we choose actions <em>randomly instead of uniformly</em> and hence we need to keep a count of each action. At the end we just divide the sum of obtained rewards over the action's count to obtain the average which is a good estimator of the expected reward.</p>
<div class="highlight"><pre><span></span><code><span class="n">rand</span><span class="p">()</span> <span class="c1">#[0,1)</span>
<span class="n">Actions</span> <span class="o">=</span>    <span class="p">[</span><span class="mi">0</span><span class="p">,</span>  <span class="mi">1</span><span class="p">,</span>    <span class="mi">2</span><span class="p">]</span>
<span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span>  <span class="mi">10</span><span class="p">,</span>   <span class="mi">10</span><span class="p">])</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">100</span><span class="p">,</span><span class="mi">50</span><span class="p">,</span>  <span class="mi">75</span><span class="p">])</span>
<span class="n">Q</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>
<span class="c1"># print(Q/C)</span>

<span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argwhere</span><span class="p">(</span><span class="n">Q</span><span class="o">==</span><span class="n">Q</span><span class="o">.</span><span class="n">max</span><span class="p">())</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>1
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">Q_bandit_fixed</span><span class="p">(</span><span class="n">bandit</span><span class="p">,</span> <span class="n">ε</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">r</span>  <span class="o">=</span> <span class="n">bandit</span>      <span class="c1"># the bandit is assumed to be a set of rewards for each action</span>
    <span class="n">nA</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>      <span class="c1"># number of actions</span>
    <span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nA</span><span class="p">)</span> <span class="c1"># action-values</span>
    <span class="n">C</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">nA</span><span class="p">)</span>  <span class="c1"># action-counts</span>
    <span class="n">avgR</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">T</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
        <span class="c1"># ε-greedy action selection (when ε=0 this turns into a greedy selection)</span>
        <span class="k">if</span> <span class="n">rand</span><span class="p">()</span><span class="o">&lt;=</span> <span class="n">ε</span><span class="p">:</span> <span class="n">a</span> <span class="o">=</span> <span class="n">randint</span><span class="p">(</span><span class="n">nA</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>          <span class="n">a</span> <span class="o">=</span> <span class="p">(</span><span class="n">Q</span><span class="o">/</span><span class="n">C</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>

        <span class="n">Q</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="n">r</span><span class="p">[</span><span class="n">a</span><span class="p">]</span>
        <span class="n">C</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">avgR</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">t</span><span class="o">*</span><span class="n">avgR</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="n">r</span><span class="p">[</span><span class="n">a</span><span class="p">])</span><span class="o">/</span><span class="p">(</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>


    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">avgR</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;average reward ε=</span><span class="si">%.2f</span><span class="s1">&#39;</span><span class="o">%</span><span class="n">ε</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">Q</span><span class="o">/</span><span class="n">C</span>
</code></pre></div>
<p>Let us see how the Q_bandit_fixed will learn to choose the best action that yields the maximum returns.</p>
<div class="highlight"><pre><span></span><code><span class="n">Q_bandit_fixed</span><span class="p">(</span><span class="n">bandit</span><span class="o">=</span><span class="p">[</span><span class="mf">.1</span><span class="p">,</span> <span class="mf">.2</span><span class="p">,</span> <span class="mf">.7</span><span class="p">])</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>array([0.09772727, 0.19874214, 0.699125  ])
</code></pre></div>
<p><img alt="png" src="output_37_1.png" /></p>
<p>As we can see it has improved to more than 0.6.</p>
<p>Let us see how the completely greedy policy would do on average:</p>
<div class="highlight"><pre><span></span><code><span class="n">Q_bandit_fixed</span><span class="p">(</span><span class="n">bandit</span><span class="o">=</span><span class="p">[</span><span class="mf">.1</span><span class="p">,</span> <span class="mf">.2</span><span class="p">,</span> <span class="mf">.7</span><span class="p">],</span> <span class="n">ε</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>array([0.0999001, 0.       , 0.       ])
</code></pre></div>
<p><img alt="png" src="output_40_1.png" /></p>
<p>As we can see it could not improve beyond 0.1</p>
<p>The main restriction in Q_bandit_fixed( ) function is that we assume that the reward function is fixed (hence the name Q_bandit_fixed). I.e., each action will receive a specific reward that does not change. This made the above solution a bit excessive since we could have just summed the rewards and then took their max. Nevertheless, this is useful as a scaffolding for our next step.</p>
<p>In the next section we develop a more general armed-bandit function that allows for the reward to vary according to some <em>unknown</em> distribution. The Q_banditAvg function will learn the distribution and find the best action that will allow it to obtain a maximal reward on average, similar to what we have done here.</p>
<h2 id="10-armed-bandet-testbed-estimating-q-via-samples-average">10-armed Bandet Testbed: Estimating Q via Samples Average</h2>
<p>Remember that Q represents the average/expected reward of an action from the start up until time step <span class="arithmatex">\(t\)</span> exclusive. Later we will develop this idea to encompass what we call the expected return of an action.
q* (qˣ in the code) represents the actual action-values for the armeds which are a set of reward that has been offset by a normal distribution randn(). This guarantees that on average the rewards of an action a is q*[a] but it will make it not easy for an observer to know exactly what the expected reward is.</p>
<h3 id="generate-the-experiencesampling">Generate the experience(sampling)</h3>
<p>generate an experience (rewards)</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">bandit</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">qˣ</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">qˣ</span><span class="p">[</span><span class="n">a</span><span class="p">]</span> <span class="o">+</span> <span class="n">randn</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">bandit</span><span class="p">(</span><span class="n">a</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">qˣ</span><span class="o">=</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)))</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>4.144352710269642
</code></pre></div>
<p>This kind of function get us a multivariate normal distribution of size k. To see how we will generate a sample bandit with all of its possible data and plot it.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">generate_a_bandit_data</span><span class="p">(</span><span class="n">qˣ</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
    <span class="n">nA</span> <span class="o">=</span> <span class="n">qˣ</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">multivariate_normal</span><span class="p">(</span><span class="n">qˣ</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">nA</span><span class="p">),</span> <span class="n">T</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">violinplot</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">generate_a_bandit_data</span><span class="p">(</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">T</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</code></pre></div>
<p><img alt="png" src="output_48_0.png" /></p>
<h3 id="learning-the-bandit-q-action-values">Learning the bandit Q action-values</h3>
<p>Now we turn our attention to learning the Q function for an unknown reward distribution. Each action has its own Gaussian distribution around a mean but we could use other distributions. The set of means are themselves drawn from a normal distribution of mean 0 and variance of 1.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># learn the Q value for bandit and use it to select the action that will win the most reward</span>
<span class="k">def</span><span class="w"> </span><span class="nf">Q_banditAvg</span><span class="p">(</span><span class="n">qˣ</span><span class="p">,</span> <span class="n">ε</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>    

    <span class="c1"># |A| and max(q*)</span>
    <span class="n">nA</span>   <span class="o">=</span> <span class="n">qˣ</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>            <span class="c1"># number of actions, usually 10</span>
    <span class="n">amax</span> <span class="o">=</span> <span class="n">qˣ</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>            <span class="c1"># the optimal action for this bandit</span>

    <span class="c1"># stats.</span>
    <span class="n">r</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">T</span><span class="p">)</span>                  <span class="c1"># reward at time step t</span>
    <span class="n">a</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>       <span class="c1"># chosen action at time step t, needs to be int as it will be used as index</span>
    <span class="n">oA</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">T</span><span class="p">)</span>                  <span class="c1"># whether an optimal action is selected at time step t</span>

    <span class="c1"># estimates</span>
    <span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nA</span><span class="p">)</span>                  <span class="c1"># action-values all initialised to 0</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">nA</span><span class="p">)</span>                   <span class="c1"># actions selection count</span>


    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
        <span class="c1"># action selection is what prevents us from vectorising the solution which must reside in a for loop</span>
        <span class="k">if</span> <span class="n">rand</span><span class="p">()</span><span class="o">&lt;=</span> <span class="n">ε</span><span class="p">:</span> <span class="n">a</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">randint</span><span class="p">(</span><span class="n">nA</span><span class="p">)</span>       <span class="c1"># explore</span>
        <span class="k">else</span><span class="p">:</span>          <span class="n">a</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">Q</span><span class="o">/</span><span class="n">N</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>    <span class="c1"># exploit</span>

        <span class="c1"># update the stats.</span>
        <span class="n">r</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>  <span class="o">=</span> <span class="n">bandit</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="n">t</span><span class="p">],</span> <span class="n">qˣ</span><span class="p">)</span>
        <span class="n">oA</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span>        <span class="n">a</span><span class="p">[</span><span class="n">t</span><span class="p">]</span><span class="o">==</span><span class="n">amax</span>

        <span class="c1"># update Q (action-values estimate)</span>
        <span class="n">N</span><span class="p">[</span><span class="n">a</span><span class="p">[</span><span class="n">t</span><span class="p">]]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">Q</span><span class="p">[</span><span class="n">a</span><span class="p">[</span><span class="n">t</span><span class="p">]]</span> <span class="o">+=</span> <span class="n">r</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> 

    <span class="k">return</span> <span class="n">r</span><span class="p">,</span> <span class="n">oA</span>
</code></pre></div>
<p>Let us now run this function and plot one 10-armed bandits</p>
<div class="highlight"><pre><span></span><code><span class="n">R</span><span class="p">,</span> <span class="n">oA</span> <span class="o">=</span> <span class="n">Q_banditAvg</span><span class="p">(</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="p">,</span> <span class="n">ε</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">R</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Steps&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Average rewrads&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">oA</span><span class="p">,</span><span class="s1">&#39;.&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Steps&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;%Optimal action&#39;</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>Text(0, 0.5, &#39;%Optimal action&#39;)
</code></pre></div>
<p><img alt="png" src="output_53_1.png" /></p>
<p>Note how the % of optimal actions for one trial (run) takes either 1 or 0. This figure to the left seems not be conveying useful information. However when we average this percentage over several runs we will see a clear learning pattern. This is quite common theme in RL. We often would want to average a set of runs/experiments due to the stochasticity of the process that we deal with.</p>
<h3 id="multiple-runs-aka-trials">Multiple runs (aka trials)</h3>
<p>We need to average multiple runs to obtain a reliable unbiased results that reflect the expected performance of the learning algorithm.</p>
<p>In the Q_bandits_runs() we pass on any extra keyword arguments to the Q_bandit() function. This trick will save us to redefine arguments and it will be utilised extensively in our treatment of RL algorithms. </p>
<p>If you are not familiar with it please refer to the many online tutorials such as <a href="https://realpython.com/python-kwargs-and-args/">this</a>. Before you do that we provide a quick cap here. You can think of and read ' **kw ' to be equivalent to ' etc ' in plain English.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">func1</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span> <span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="p">);</span> <span class="n">func2</span><span class="p">()</span> 
<span class="k">def</span><span class="w"> </span><span class="nf">func2</span><span class="p">(</span><span class="n">d</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span> <span class="nb">print</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
<span class="n">func1</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>10
20
</code></pre></div>
<p>The problem with the above is that we cannot change d via func1, to do so we can use the arbitrary **kw trick. Note that kw is just a dictionary of keys and values like any python dictionary.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">func1</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">):</span> <span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="p">);</span> <span class="n">func2</span><span class="p">(</span><span class="o">**</span><span class="n">kw</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">func2</span><span class="p">(</span><span class="n">d</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>       <span class="nb">print</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
<span class="n">func1</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>10
30
</code></pre></div>
<p>As you can see we passed d inside func1 although it is not originally in the set of keywords arguments of func1. Also if we later changed the prototype of func2() we do not need to change func1(). For example:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">func2</span><span class="p">(</span><span class="n">d</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="mi">200</span><span class="p">):</span> <span class="nb">print</span><span class="p">(</span><span class="n">d</span><span class="p">);</span> <span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
<span class="n">func1</span><span class="p">(</span><span class="n">c</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="mi">66</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>10
30
66
</code></pre></div>
<p>One more thing: we use trange(runs) instead of range(runs) because it gives us a bar to indicate how much left for the process to finish. This is quite useful when we do extensive runs.</p>
<div class="highlight"><pre><span></span><code><span class="k">for</span> <span class="n">run</span> <span class="ow">in</span> <span class="n">trange</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">.1</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>100%|███████████████████████████████████████████| 10/10 [00:01&lt;00:00,  9.55it/s]
</code></pre></div>
<p>Ok now we are ready to the Q_bandits_runs() definition.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">Q_bandits_runs</span><span class="p">(</span><span class="n">Q_bandit</span><span class="p">,</span> <span class="n">runs</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>  <span class="n">k</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>  <span class="n">label</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">):</span>
    <span class="c1">#np.random.seed(20) # this allows us to generate the same experience for consistency, you can comment it out</span>

    <span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">runs</span><span class="p">,</span> <span class="n">T</span><span class="p">))</span>  <span class="c1"># rewards over runs and time steps</span>
    <span class="n">oA</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">runs</span><span class="p">,</span> <span class="n">T</span><span class="p">))</span>  <span class="c1"># optimal actions over runs and steps</span>

    <span class="k">for</span> <span class="n">run</span> <span class="ow">in</span> <span class="n">trange</span><span class="p">(</span><span class="n">runs</span><span class="p">):</span>

        <span class="c1"># generate the k-armed bandit actual q* values by calling normal(0, 1, k) </span>
        <span class="n">R</span><span class="p">[</span><span class="n">run</span><span class="p">],</span> <span class="n">oA</span><span class="p">[</span><span class="n">run</span><span class="p">]</span> <span class="o">=</span> <span class="n">Q_bandit</span><span class="p">(</span><span class="n">qˣ</span><span class="o">=</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="n">T</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">)</span>

    <span class="n">subplotQ_bandits_runs</span><span class="p">(</span><span class="n">R</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Average rewrads&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">])</span>

    <span class="n">subplotQ_bandits_runs</span><span class="p">(</span><span class="n">oA</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;%Optimal action&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">1.2</span><span class="p">,</span><span class="mf">.2</span><span class="p">),</span> <span class="p">[</span><span class="s1">&#39;0%&#39;</span><span class="p">,</span> <span class="s1">&#39;20%&#39;</span><span class="p">,</span> <span class="s1">&#39;40%&#39;</span><span class="p">,</span> <span class="s1">&#39;60%&#39;</span><span class="p">,</span> <span class="s1">&#39;80%&#39;</span><span class="p">,</span> <span class="s1">&#39;100%&#39;</span><span class="p">])</span>
</code></pre></div>
<p>Where subplotQ_bandits_runs function handles the learning curve plots for us and is given below. Note that in the above we obtain different set of 10-bandit distributions and conduct an experimental run on them. Because all of them are normally standard distribution their sums of rewards (values) converges to the same quantity around 1.5.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">subplotQ_bandits_runs</span><span class="p">(</span><span class="n">experience</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">.6</span><span class="o">*</span><span class="n">experience</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">i</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">experience</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">experience</span><span class="p">[</span><span class="n">x</span><span class="p">]),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Steps&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">ylabel</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;right&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;top&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>
<h3 id="action-selection-policy-comparison">Action Selection (Policy) Comparison:</h3>
<p>Now we are ready to compare between policies with different exploration rates ε. Note that ε kw(keyword argument) has been passed on to the Q_bandit() function from the Q_bandits_runs() function.</p>
<div class="highlight"><pre><span></span><code><span class="n">Q_bandits_runs</span><span class="p">(</span><span class="n">ε</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span>  <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ε=.1&#39;</span><span class="p">,</span>  <span class="n">Q_bandit</span><span class="o">=</span><span class="n">Q_banditAvg</span><span class="p">,</span> <span class="n">runs</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>100%|██████████████████████████████████████| 1000/1000 [00:04&lt;00:00, 238.61it/s]
</code></pre></div>
<p><img alt="png" src="output_68_1.png" /></p>
<div class="highlight"><pre><span></span><code><span class="n">Q_bandits_runs</span><span class="p">(</span><span class="n">ε</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span>  <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ε=.1&#39;</span><span class="p">,</span>  <span class="n">Q_bandit</span><span class="o">=</span><span class="n">Q_banditAvg</span><span class="p">)</span>
<span class="n">Q_bandits_runs</span><span class="p">(</span><span class="n">ε</span><span class="o">=</span><span class="mf">.01</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ε=.01&#39;</span><span class="p">,</span> <span class="n">Q_bandit</span><span class="o">=</span><span class="n">Q_banditAvg</span><span class="p">)</span>
<span class="n">Q_bandits_runs</span><span class="p">(</span><span class="n">ε</span><span class="o">=</span><span class="mf">.0</span><span class="p">,</span>  <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ε=.0&#39;</span> <span class="p">,</span> <span class="n">Q_bandit</span><span class="o">=</span><span class="n">Q_banditAvg</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>100%|██████████████████████████████████████| 2000/2000 [00:07&lt;00:00, 257.52it/s]
100%|██████████████████████████████████████| 2000/2000 [00:07&lt;00:00, 274.10it/s]
100%|██████████████████████████████████████| 2000/2000 [00:07&lt;00:00, 279.78it/s]
</code></pre></div>
<p><img alt="png" src="output_69_1.png" /></p>
<p>As we can see the ε=.1 exploration rate seems to give us a sweet spot. Try ε=.2 and see the effect.
This empirically indicates that indeed we need to allow the agent to explore in order to come up with a viable optimal or close to optimal policy.</p>
<h2 id="incremental-implementation">Incremental Implementation</h2>
<p>If we look at the sum</p>
<div class="arithmatex">\[
% \begin{aligned}
Q_{t+1}  = \frac{1}{t}\sum_{i=1}^{t}R_i = \frac{1}{t}\left(\sum_{i=1}^{t-1}R_i + R_t\right) 
\]</div>
<div class="arithmatex">\[
        = \frac{1}{t}\left((t-1)\frac{\sum_{i=1}^{t-1}R_i}{t-1} + R_t\right) 
\]</div>
<div class="arithmatex">\[
        = \frac{1}{t}\left(\left(t-1\right)Q_t + R_t\right) 
\]</div>
<div class="arithmatex">\[
Q_{t+1} = Q_t + \frac{1}{t}\left(R_t - Q_t\right) 
\]</div>
<!-- % \end{aligned} -->
<!-- $$ -->
<p>We can see that we can write the estimate in an incremental form that allows us to update our estimate <span class="arithmatex">\(Q_t\)</span> instead of recalculate the sum in each time step. This is very handy when it comes to efficiently implement an algorithm to give us the sum. Further, it  turns out that it also has other advantages. To realise this, note that the <span class="arithmatex">\(\frac{1}{t}\)</span> diminishes when <span class="arithmatex">\(t\)</span> grows, which is natural for averages. But if we want the latest rewards to have a bigger impact (weights) then we can simply replace this fraction by a constance<span class="arithmatex">\(\alpha\)</span> to obtain the following <strong>incremental update</strong></p>
<div class="arithmatex">\[
    Q_{t+1} = Q_t + \alpha\left(R_t - Q_t\right)
\]</div>
<p>Note that incremental updates plays a very important role in RL and we will be constantly seeking them due to their efficiency in online application.</p>
<p>Below we redefine our Q_bandit function to be incremental. Note how we adjust each Q[a] with the difference between its reward estimate and the actual reward estimate then we divide by the action count N[a]</p>
<div class="arithmatex">\[ Q[a] += (R[a,t]- Q[a])/N[a] \]</div>
<p>We also tidy up a bit so that the setup code is not repeated later when we change the update.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">bandit_init</span><span class="p">(</span><span class="n">qˣ</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">q0</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>

    <span class="c1"># |A| and max(q*)</span>
    <span class="n">nA</span>   <span class="o">=</span> <span class="n">qˣ</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>                <span class="c1"># number of actions, usually 10</span>
    <span class="n">amax</span> <span class="o">=</span> <span class="n">qˣ</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>                <span class="c1"># the optimal action for this bandit</span>

    <span class="c1"># stats.</span>
    <span class="n">r</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">T</span><span class="p">)</span>                  <span class="c1"># reward at time step t</span>
    <span class="n">a</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>       <span class="c1"># chosen action at time step t, needs to be int as it will be used as index</span>

    <span class="c1"># estimates</span>
    <span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">nA</span><span class="p">)</span><span class="o">*</span><span class="n">q0</span>                <span class="c1"># action-values all initialised to q0</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nA</span><span class="p">)</span>                  <span class="c1"># 👀 earlier we cheated a bit by initialising N to ones to avoid div by 0</span>

    <span class="k">return</span> <span class="n">nA</span><span class="p">,</span><span class="n">amax</span><span class="p">,</span>  <span class="n">r</span><span class="p">,</span><span class="n">a</span><span class="p">,</span>  <span class="n">Q</span><span class="p">,</span><span class="n">N</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">Q_banditN</span><span class="p">(</span><span class="n">qˣ</span><span class="p">,</span> <span class="n">ε</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">q0</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="n">nA</span><span class="p">,</span><span class="n">amax</span><span class="p">,</span>  <span class="n">r</span><span class="p">,</span><span class="n">a</span><span class="p">,</span>  <span class="n">Q</span><span class="p">,</span><span class="n">N</span> <span class="o">=</span> <span class="n">bandit_init</span><span class="p">(</span><span class="n">qˣ</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">q0</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
        <span class="c1"># choose a[t]</span>
        <span class="k">if</span> <span class="n">rand</span><span class="p">()</span><span class="o">&lt;=</span> <span class="n">ε</span><span class="p">:</span> <span class="n">a</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">randint</span><span class="p">(</span><span class="n">nA</span><span class="p">)</span>     <span class="c1"># explore</span>
        <span class="k">else</span><span class="p">:</span>          <span class="n">a</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>      <span class="c1"># exploit, 👀 note that we do not use Q/N as before</span>

        <span class="c1"># get the reward from bandit</span>
        <span class="n">r</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>  <span class="o">=</span> <span class="n">bandit</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="n">t</span><span class="p">],</span> <span class="n">qˣ</span><span class="p">)</span>

        <span class="c1"># update Q (action-values estimate)</span>
        <span class="n">N</span><span class="p">[</span><span class="n">a</span><span class="p">[</span><span class="n">t</span><span class="p">]]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">Q</span><span class="p">[</span><span class="n">a</span><span class="p">[</span><span class="n">t</span><span class="p">]]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">-</span> <span class="n">Q</span><span class="p">[</span><span class="n">a</span><span class="p">[</span><span class="n">t</span><span class="p">]])</span><span class="o">/</span><span class="n">N</span><span class="p">[</span><span class="n">a</span><span class="p">[</span><span class="n">t</span><span class="p">]]</span>   

    <span class="k">return</span> <span class="n">r</span><span class="p">,</span> <span class="n">a</span><span class="o">==</span><span class="n">amax</span>
</code></pre></div>
<p>Now we can call again our Q_bandits_runs which will use the newly defined Q_bandit</p>
<div class="highlight"><pre><span></span><code><span class="n">Q_bandits_runs</span><span class="p">(</span><span class="n">ε</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span>  <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ε =.1&#39;</span><span class="p">,</span>  <span class="n">Q_bandit</span><span class="o">=</span><span class="n">Q_banditN</span><span class="p">,</span> <span class="n">runs</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>100%|████████████████████████████████████████| 200/200 [00:00&lt;00:00, 238.21it/s]
</code></pre></div>
<p><img alt="png" src="output_76_1.png" /></p>
<div class="highlight"><pre><span></span><code><span class="c1">#Q_bandits_runs(ε=.2)</span>
<span class="n">Q_bandits_runs</span><span class="p">(</span><span class="n">ε</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span>  <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ε =.1&#39;</span><span class="p">,</span>  <span class="n">Q_bandit</span><span class="o">=</span><span class="n">Q_banditN</span><span class="p">)</span>
<span class="n">Q_bandits_runs</span><span class="p">(</span><span class="n">ε</span><span class="o">=</span><span class="mf">.01</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ε =.01&#39;</span><span class="p">,</span> <span class="n">Q_bandit</span><span class="o">=</span><span class="n">Q_banditN</span><span class="p">)</span>
<span class="n">Q_bandits_runs</span><span class="p">(</span><span class="n">ε</span><span class="o">=</span><span class="mf">.0</span><span class="p">,</span>  <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ε =.0&#39;</span><span class="p">,</span>  <span class="n">Q_bandit</span><span class="o">=</span><span class="n">Q_banditN</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>100%|██████████████████████████████████████| 2000/2000 [00:07&lt;00:00, 255.15it/s]
100%|██████████████████████████████████████| 2000/2000 [00:07&lt;00:00, 264.25it/s]
100%|██████████████████████████████████████| 2000/2000 [00:07&lt;00:00, 264.78it/s]
</code></pre></div>
<p><img alt="png" src="output_77_1.png" /></p>
<h2 id="non-stationary-problems">Non-stationary Problems</h2>
<p>The limitation of the above implementation is that it requires actions counts and when the underlying reward distribution changes (non-stationary reward distribution) it does not respond well to take these changes into account. A better approach when we are faced with such problems is to use a fixed size step &lt;1 instead of dividing by the actions count. This way, because the step size is small the estimate gets updated when the underlying reward distribution changes. Of course this means that the estimates will keep changing even when the underlying distribution is not changing, however in practice this is not a problem when the step size is small enough. This effectively gives more weights to recent updates which gives a good changes-responsiveness property for this and similar methods that use a fixed size learning step <span class="arithmatex">\(\alpha\)</span>.</p>
<p>Note that the majority of RL problem are actually non-stationary. This is because, as we shall see later, when we gradually move towards an optimal policy by changing the Q action-values, the underlying reward distribution changes in response to taking actions that are optimal according to the current estimation. This is also the case here but in a subtle way.</p>
<div class="highlight"><pre><span></span><code><span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argwhere</span><span class="p">(</span><span class="n">Q</span><span class="o">==</span><span class="n">Q</span><span class="o">.</span><span class="n">max</span><span class="p">())</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>1
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">Q_banditα</span><span class="p">(</span><span class="n">qˣ</span><span class="p">,</span>  <span class="n">α</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">ε</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">q0</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>

    <span class="n">nA</span><span class="p">,</span> <span class="n">amax</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">bandit_init</span><span class="p">(</span><span class="n">qˣ</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">q0</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
        <span class="c1"># choose a[t]</span>
        <span class="k">if</span> <span class="n">rand</span><span class="p">()</span><span class="o">&lt;=</span> <span class="n">ε</span><span class="p">:</span> <span class="n">a</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">randint</span><span class="p">(</span><span class="n">nA</span><span class="p">)</span>     <span class="c1"># explore</span>
        <span class="k">else</span><span class="p">:</span>          <span class="n">a</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>      <span class="c1"># exploit, 👀 note that we do not use Q/N as before</span>

        <span class="c1"># get the reward from bandit</span>
        <span class="n">r</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>  <span class="o">=</span> <span class="n">bandit</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="n">t</span><span class="p">],</span> <span class="n">qˣ</span><span class="p">)</span>

        <span class="c1"># update Q (action-values estimate)</span>
        <span class="n">Q</span><span class="p">[</span><span class="n">a</span><span class="p">[</span><span class="n">t</span><span class="p">]]</span> <span class="o">+=</span> <span class="n">α</span><span class="o">*</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">-</span> <span class="n">Q</span><span class="p">[</span><span class="n">a</span><span class="p">[</span><span class="n">t</span><span class="p">]])</span>  <span class="c1"># 👀 constant step-size α: yields exponential recency-weighted average</span>
                                       <span class="c1">#    similar form of update will be used Throughout the unit</span>

    <span class="k">return</span> <span class="n">r</span><span class="p">,</span> <span class="n">a</span><span class="o">==</span><span class="n">amax</span>
</code></pre></div>
<p>The above is a neat way to learn the action-value function and it preferred due the reasons that we mentioned earlier.</p>
<h3 id="comparison">Comparison</h3>
<p>Let us now compare different exploration rates for this learning function. </p>
<div class="highlight"><pre><span></span><code><span class="n">Q_bandits_runs</span><span class="p">(</span><span class="n">ε</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span>  <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ε=.1&#39;</span><span class="p">,</span>  <span class="n">Q_bandit</span><span class="o">=</span><span class="n">Q_banditα</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">runs</span><span class="o">=</span><span class="mi">2000</span><span class="p">)</span>
<span class="n">Q_bandits_runs</span><span class="p">(</span><span class="n">ε</span><span class="o">=</span><span class="mf">.01</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ε=.01&#39;</span><span class="p">,</span> <span class="n">Q_bandit</span><span class="o">=</span><span class="n">Q_banditα</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">runs</span><span class="o">=</span><span class="mi">2000</span><span class="p">)</span>
<span class="n">Q_bandits_runs</span><span class="p">(</span><span class="n">ε</span><span class="o">=</span><span class="mf">.0</span><span class="p">,</span>  <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ε=.0&#39;</span><span class="p">,</span>  <span class="n">Q_bandit</span><span class="o">=</span><span class="n">Q_banditα</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span> <span class="n">runs</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>100%|███████████████████████████████████████| 2000/2000 [00:29&lt;00:00, 68.58it/s]
100%|███████████████████████████████████████| 2000/2000 [00:25&lt;00:00, 79.46it/s]
100%|█████████████████████████████████████████| 500/500 [00:06&lt;00:00, 76.24it/s]
</code></pre></div>
<p><img alt="png" src="output_83_1.png" /></p>
<h2 id="q_bandit-in-a-better-form">Q_bandit in a better form</h2>
<p>Let us finally define some useful policy functions and redefine and further simplify the Q_bandit function.</p>
<h3 id="policies-exploration-vs-exploitation">Policies: Exploration vs. Exploitation</h3>
<p>Getting the right balance between exploration and exploitation is a constant dilemma in RL.
One simple strategy as we saw earlier is to explore constantly occasionally ε% of the time! which we called ε-greedy. Another strategy is to insure that when we have multiple actions that are greedy we chose ebtween them equally and not bias one over the other. This is what we do in the greedyStoch policy below.</p>
<p>We start by showing how to randomly choose between two max Q value actions.</p>
<div class="highlight"><pre><span></span><code><span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">.2</span> <span class="p">,</span> <span class="mf">.4</span><span class="p">,</span> <span class="mf">.4</span><span class="p">])</span>
<span class="n">actions</span><span class="o">=</span><span class="p">[]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">30</span><span class="p">):</span>
    <span class="n">actions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argwhere</span><span class="p">(</span><span class="n">Q</span><span class="o">==</span><span class="n">Q</span><span class="o">.</span><span class="n">max</span><span class="p">())</span><span class="o">.</span><span class="n">ravel</span><span class="p">()))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>[2, 1, 1, 1, 2, 2, 2, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2]
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="c1"># returns one of the max Q actions; there is an element of stochasticity in this policy</span>
<span class="k">def</span><span class="w"> </span><span class="nf">greedyStoch</span><span class="p">(</span><span class="n">Q</span><span class="p">):</span>   
    <span class="k">return</span> <span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argwhere</span><span class="p">(</span><span class="n">Q</span><span class="o">==</span><span class="n">Q</span><span class="o">.</span><span class="n">max</span><span class="p">())</span><span class="o">.</span><span class="n">ravel</span><span class="p">())</span>


<span class="c1"># returns the first max Q action most of the time (1-ε)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">εgreedy</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">ε</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">Q</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span> <span class="k">if</span> <span class="n">rand</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">ε</span> <span class="k">else</span> <span class="n">randint</span><span class="p">(</span><span class="n">Q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>


<span class="k">def</span><span class="w"> </span><span class="nf">εgreedyStoch</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">ε</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">greedyStoch</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span> <span class="k">if</span> <span class="n">rand</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">ε</span> <span class="k">else</span> <span class="n">randint</span><span class="p">(</span><span class="n">Q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">Q_banditα</span><span class="p">(</span><span class="n">qˣ</span><span class="p">,</span>  <span class="n">α</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">ε</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">T</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">q0</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="n">εgreedy</span><span class="p">):</span>
    <span class="n">nA</span><span class="p">,</span> <span class="n">amax</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">bandit_init</span><span class="p">(</span><span class="n">qˣ</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">q0</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
        <span class="c1"># using a specific policy</span>
        <span class="n">a</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">policy</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">ε</span><span class="p">)</span>               

        <span class="c1"># get the reward from bandit</span>
        <span class="n">r</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>  <span class="o">=</span> <span class="n">bandit</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="n">t</span><span class="p">],</span> <span class="n">qˣ</span><span class="p">)</span>

        <span class="c1"># update Q (action-values estimate)</span>
        <span class="n">Q</span><span class="p">[</span><span class="n">a</span><span class="p">[</span><span class="n">t</span><span class="p">]]</span> <span class="o">+=</span> <span class="n">α</span><span class="o">*</span><span class="p">(</span><span class="n">r</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">-</span> <span class="n">Q</span><span class="p">[</span><span class="n">a</span><span class="p">[</span><span class="n">t</span><span class="p">]])</span>  


    <span class="k">return</span> <span class="n">r</span><span class="p">,</span> <span class="n">a</span><span class="o">==</span><span class="n">amax</span>
</code></pre></div>
<p>Let us compare different <em>learning rates</em> α to see how our Q_bandits() function reacts to them. </p>
<div class="highlight"><pre><span></span><code><span class="n">Q_bandits_runs</span><span class="p">(</span><span class="n">α</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span>  <span class="n">label</span><span class="o">=</span><span class="s1">&#39;α=.1&#39;</span><span class="p">,</span>  <span class="n">Q_bandit</span><span class="o">=</span><span class="n">Q_banditα</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>100%|██████████████████████████████████████| 2000/2000 [00:05&lt;00:00, 347.86it/s]
</code></pre></div>
<p><img alt="png" src="output_92_1.png" /></p>
<div class="highlight"><pre><span></span><code><span class="n">Q_bandits_runs</span><span class="p">(</span><span class="n">α</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span>  <span class="n">label</span><span class="o">=</span><span class="s1">&#39;α=.1&#39;</span><span class="p">,</span>  <span class="n">Q_bandit</span><span class="o">=</span><span class="n">Q_banditα</span><span class="p">)</span>
<span class="n">Q_bandits_runs</span><span class="p">(</span><span class="n">α</span><span class="o">=</span><span class="mf">.01</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;α=.01&#39;</span><span class="p">,</span> <span class="n">Q_bandit</span><span class="o">=</span><span class="n">Q_banditα</span><span class="p">)</span>
<span class="n">Q_bandits_runs</span><span class="p">(</span><span class="n">α</span><span class="o">=</span><span class="mf">.5</span><span class="p">,</span>  <span class="n">label</span><span class="o">=</span><span class="s1">&#39;α=.5&#39;</span><span class="p">,</span>  <span class="n">Q_bandit</span><span class="o">=</span><span class="n">Q_banditα</span><span class="p">)</span>
<span class="n">Q_bandits_runs</span><span class="p">(</span><span class="n">α</span><span class="o">=</span><span class="mf">.0</span><span class="p">,</span>  <span class="n">label</span><span class="o">=</span><span class="s1">&#39;α=.0&#39;</span><span class="p">,</span>  <span class="n">Q_bandit</span><span class="o">=</span><span class="n">Q_banditα</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>100%|██████████████████████████████████████| 2000/2000 [00:05&lt;00:00, 335.52it/s]
100%|██████████████████████████████████████| 2000/2000 [00:06&lt;00:00, 330.52it/s]
100%|██████████████████████████████████████| 2000/2000 [00:05&lt;00:00, 342.88it/s]
100%|██████████████████████████████████████| 2000/2000 [00:06&lt;00:00, 331.20it/s]
</code></pre></div>
<p><img alt="png" src="output_93_1.png" /></p>
<h2 id="optimistic-initial-values">Optimistic Initial Values</h2>
<p>It turns out that we can infuse exploration in the RL solution by optimistically initialising the Q values.
This encourages the agent to explore due to its disappointment when its initial Q values are not matching the reward values that are coming from the ground (interacting with the environment). This intrinsic exploration motive to explore more actions at the start, vanishes with time when the Q values become more realistic. </p>
<p>This is a good and effective strategy for exploration. But of course it has its limitations, for example it does not necessarily work for if there a constant or renewed need for exploration. This could happen either when the task or the environment are changing. Below, we show the effect of optimistically initiating the Q values on the 10-armed bandit testbed. We can clearly see that without exploration i.e. when ε=0 and Q=5 initial values outperformed the exploratory policy ε=.1 with Q=0 initial values.</p>
<p>Ok, we will apply the same principle to stochastically return one of the max Q actions which is coded in greedyStoch() policy. This type of policy will prove useful later when we deal with control.</p>
<div class="highlight"><pre><span></span><code><span class="n">Q_bandits_runs</span><span class="p">(</span><span class="n">ε</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">q0</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ε=.1  Realistic Q=0&#39;</span><span class="p">,</span>  <span class="n">Q_bandit</span><span class="o">=</span><span class="n">Q_banditα</span><span class="p">)</span>
<span class="n">Q_bandits_runs</span><span class="p">(</span><span class="n">ε</span><span class="o">=</span><span class="mi">0</span> <span class="p">,</span> <span class="n">q0</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ε=0   Optimistic Q=5&#39;</span><span class="p">,</span> <span class="n">Q_bandit</span><span class="o">=</span><span class="n">Q_banditα</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>100%|██████████████████████████████████████| 2000/2000 [00:05&lt;00:00, 334.63it/s]
100%|██████████████████████████████████████| 2000/2000 [00:04&lt;00:00, 404.58it/s]
</code></pre></div>
<p><img alt="png" src="output_97_1.png" /></p>
<div class="highlight"><pre><span></span><code><span class="n">Q_bandits_runs</span><span class="p">(</span><span class="n">ε</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">q0</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ε=.1  Realistic Q=0&#39;</span><span class="p">,</span>  <span class="n">Q_bandit</span><span class="o">=</span><span class="n">Q_banditα</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="n">εgreedyStoch</span><span class="p">)</span>
<span class="n">Q_bandits_runs</span><span class="p">(</span><span class="n">ε</span><span class="o">=</span><span class="mi">0</span> <span class="p">,</span> <span class="n">q0</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ε=0   Optimistic Q=5&#39;</span><span class="p">,</span> <span class="n">Q_bandit</span><span class="o">=</span><span class="n">Q_banditα</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="n">εgreedyStoch</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>100%|███████████████████████████████████████| 2000/2000 [01:01&lt;00:00, 32.73it/s]
100%|███████████████████████████████████████| 2000/2000 [01:06&lt;00:00, 30.04it/s]
</code></pre></div>
<p><img alt="png" src="output_98_1.png" /></p>
<p>As we can see above the optimistic initialization has actually beaten the constant exploration rate and it constitutes a very useful trick for us to encourage the agent to explore while still acting greedily. Of course we can combine both strategies and we will leave this for you as a task. We will use this trick in our coverage of RL in later lessons.</p>
<h2 id="conclusion">Conclusion</h2>
<p>In this lesson you have learned about the importance of the action value function Q and stationary and non-stationary reward distribution and how we can devise a general algorithms to address them and we concluded by showing an incremental learning algorithm to tackle the k-armed bandit problem. You have seen different exploration strategy and we extensively compared between exploration rates and learning rates for our different algorithms.</p>
<h2 id="your-turn">Your Turn</h2>
<ol>
<li>Define a softmax policy as per its definition in eq. 2.11, then compare the Q_bandits_runs on ε-greedy and softmax policies.</li>
<li>Combine the optimistic initialisation and exploration rates and see how the bandit_Q function react to them.</li>
</ol>







  
  






                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright Abdulrahman Altahhan
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.sections"], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.f1b6f286.min.js"></script>
      
        <script src="../../javascript/tablecontentsoverride.js"></script>
      
        <script src="../../javascript/config.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>