
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
        <meta name="author" content="Abdulrahman Altahhan, 2025.">
      
      
      
        <link rel="prev" href="../lesson2/lesson2.html">
      
      
        <link rel="next" href="../lesson4/lesson4.html">
      
      
      <link rel="icon" href="../../img/favicon.jpg">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.4">
    
    
      
        <title>3. MDP - Reinforcement Learning for Autonomous Agents..</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.8608ea7d.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../css/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#lesson-3-markov-decision-processes-and-value-functions" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
        <aside class="md-banner">
          <div class="md-banner__inner md-grid md-typeset">
            
            
    <div id="versionIndicator"><b>Version:</b> 04.06.21.a</div>
    <img id="customlogo" src="../../../img/logo.svg" alt="University of Leeds logo.">

          </div>
          
        </aside>
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../index.html" title="Reinforcement Learning for Autonomous Agents.." class="md-header__button md-logo" aria-label="Reinforcement Learning for Autonomous Agents.." data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Reinforcement Learning for Autonomous Agents..
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              3. MDP
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../index.html" class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../lesson1/lesson1.html" class="md-tabs__link">
          
  
  Unit 1

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit2/lesson5/lesson5.html" class="md-tabs__link">
          
  
  Unit 2

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit3/lesson8/lesson8.html" class="md-tabs__link">
          
  
  Unit 3

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit4/lesson12/lesson12.html" class="md-tabs__link">
          
  
  Unit 4

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit5/lesson15/lesson15.html" class="md-tabs__link">
          
  
  Unit 5

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit6/lesson18/lesson18.html" class="md-tabs__link">
          
  
  Unit 6.1

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../index.html" title="Reinforcement Learning for Autonomous Agents.." class="md-nav__button md-logo" aria-label="Reinforcement Learning for Autonomous Agents.." data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Reinforcement Learning for Autonomous Agents..
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../index.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Unit 1
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Unit 1
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lesson1/lesson1.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1. Tabular Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lesson2/lesson2.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2. K-Arm Bandit
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    3. MDP
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="lesson3.html" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    3. MDP
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#markov-decision-processes-mdp" class="md-nav__link">
    <span class="md-ellipsis">
      Markov Decision Processes (MDP)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-return-g_t-of-a-time-step-t" class="md-nav__link">
    <span class="md-ellipsis">
      The Return \(G_t\) of a time step \(t\)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The Return \(G_t\) of a time step \(t\)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#g_t-monotonicity-for-mdp-rewards" class="md-nav__link">
    <span class="md-ellipsis">
      \(G_t\) Monotonicity for MDP Rewards
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#g_t-monotonicity-for-sparse-mdp-rewards" class="md-nav__link">
    <span class="md-ellipsis">
      \(G_t\) Monotonicity for Sparse MDP Rewards
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-expected-return-function-v" class="md-nav__link">
    <span class="md-ellipsis">
      The Expected Return Function V
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bellman-equation-for-v-and-q" class="md-nav__link">
    <span class="md-ellipsis">
      Bellman Equation for V and Q
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bellman-optimality-equations-for-v-and-q" class="md-nav__link">
    <span class="md-ellipsis">
      Bellman Optimality Equations for V and Q
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#grid-world-environments" class="md-nav__link">
    <span class="md-ellipsis">
      Grid World Environments
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#your-turn" class="md-nav__link">
    <span class="md-ellipsis">
      Your turn
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conclusion" class="md-nav__link">
    <span class="md-ellipsis">
      Conclusion
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lesson4/lesson4.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    4. ROS
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 2
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Unit 2
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit2/lesson5/lesson5.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    5. Dynamic Programming
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit2/lesson6/lesson6.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    6. Monte Carlo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit2/lesson7/lesson7.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    7. Mobile Robots
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 3
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Unit 3
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson8/lesson8.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    8. Temporal Difference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson9/lesson9.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    9. n-Step Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson10/lesson10.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    10. Planning in RL(optional)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson11/lesson11.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    11. Localisation and SLAM
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 4
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Unit 4
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit4/lesson12/lesson12.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    12. Function Approximation Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit4/lesson13/lesson13.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    13. Linear Approximation for Prediction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit4/lesson14/lesson14.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    14. Linear Approximation for Control
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 5
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Unit 5
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit5/lesson15/lesson15.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    15. Linear Approximation with Eligibility Traces(prediction and control)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit5/lesson16/lesson16.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    16. Nonlinear Approximation for Control
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit5/lesson17/lesson17.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    17. Application on Robot Navigation
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 6.1
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Unit 6.1
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit6/lesson18/lesson18.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    18. Application on Games(optional)
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#markov-decision-processes-mdp" class="md-nav__link">
    <span class="md-ellipsis">
      Markov Decision Processes (MDP)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-return-g_t-of-a-time-step-t" class="md-nav__link">
    <span class="md-ellipsis">
      The Return \(G_t\) of a time step \(t\)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The Return \(G_t\) of a time step \(t\)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#g_t-monotonicity-for-mdp-rewards" class="md-nav__link">
    <span class="md-ellipsis">
      \(G_t\) Monotonicity for MDP Rewards
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#g_t-monotonicity-for-sparse-mdp-rewards" class="md-nav__link">
    <span class="md-ellipsis">
      \(G_t\) Monotonicity for Sparse MDP Rewards
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-expected-return-function-v" class="md-nav__link">
    <span class="md-ellipsis">
      The Expected Return Function V
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bellman-equation-for-v-and-q" class="md-nav__link">
    <span class="md-ellipsis">
      Bellman Equation for V and Q
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bellman-optimality-equations-for-v-and-q" class="md-nav__link">
    <span class="md-ellipsis">
      Bellman Optimality Equations for V and Q
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#grid-world-environments" class="md-nav__link">
    <span class="md-ellipsis">
      Grid World Environments
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#your-turn" class="md-nav__link">
    <span class="md-ellipsis">
      Your turn
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conclusion" class="md-nav__link">
    <span class="md-ellipsis">
      Conclusion
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<script>
  window.MathJax = {
    tex: {
      tags: "ams",  // Enables equation numbering
    //   displayMath: [['$$', '$$'], ['\\[', '\\]']]
    }
  };
</script>

<h1 id="lesson-3-markov-decision-processes-and-value-functions">Lesson 3- Markov Decision Processes and value functions</h1>
<p>In this lesson we cover various grid world environments that are tackled as examples in the accompanying text book available online <a href="http://incompleteideas.net/book/RLbook2020.pdf">here</a>. Please note that we explain the ideas of this topic from a practical perspective and not from a theoretical perspective which is already covered in the textbook.</p>
<p><strong>Learning outcomes</strong></p>
<ol>
<li>understand MDP and its elements</li>
<li>understand the return for a time step <span class="arithmatex">\(t\)</span></li>
<li>understand the expected return of a state <span class="arithmatex">\(s\)</span></li>
<li>understand the Bellman optimality equations</li>
<li>become familiar with the different types of grid world problems that we will tackle in later units</li>
<li>become familiar with the way we assign a reward to an environment</li>
<li>be able to execute actions in a grid world and observe the result</li>
<li>be able to to visualise a policy and its action-value function</li>
</ol>
<p><strong>Reading</strong>:
The accompanying reading of this lesson is <strong>chapter 3</strong> from our text book by Sutton and Barto available online <a href="http://incompleteideas.net/book/RLbook2020.pdf">here</a>. </p>
<p>To help you, Abdulrahman recorded a set of videos that covers important concepts of RL.</p>
<h2 id="markov-decision-processes-mdp">Markov Decision Processes (MDP)</h2>
<iframe src="https://leeds365-my.sharepoint.com/personal/scsaalt_leeds_ac_uk/_layouts/15/embed.aspx?UniqueId=f93d3c1e-261f-42bd-93cd-92f67e120d99&embed=%7B%22ust%22%3Atrue%7D&referrer=StreamWebApp&referrerScenario=EmbedDialog.Create" width="470" height="200" frameborder="0" scrolling="no" allowfullscreen title="Markov Decision Processes (MDP)" enablejsapi=1></iframe>

<iframe src="https://leeds365-my.sharepoint.com/personal/scsaalt_leeds_ac_uk/_layouts/15/embed.aspx?UniqueId=79d3a9ea-5401-4f3c-bcf2-5907255ef8da&embed=%7B%22ust%22%3Atrue%7D&referrer=StreamWebApp&referrerScenario=EmbedDialog.Create" width="470" height="200"frameborder="0" scrolling="no" allowfullscreen title="2. Dynamics.mkv"></iframe>

<h2 id="the-return-g_t-of-a-time-step-t">The Return <span class="arithmatex">\(G_t\)</span> of a time step <span class="arithmatex">\(t\)</span></h2>
<p>We start by realising the </p>
<div class="arithmatex">\[
\begin{align*}
    G_t = R_{t+1} + &amp;\gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + ... + \gamma^{T-t-1} R_{T} \\
    G_{t+1} =       &amp; R_{t+2} + \gamma R_{t+3} + \gamma^2 R_{t+4} + ... + \gamma^{T-t-2} R_{T}
\end{align*}
\]</div>
<p>Hence by multiplying <span class="arithmatex">\(G_{t+1}\)</span> by <span class="arithmatex">\(\gamma\)</span> and adding R_{t+1} we get</p>
<div class="arithmatex">\[
\begin{equation}
    G_t = R_{t+1} + \gamma G_{t+1}
\end{equation}
\]</div>
<p><strong>The above equation is the most important equation that we will build all of our incremental updates in RL on it.</strong></p>
<p>In the video below we talk more about this important concept.</p>
<iframe src="https://leeds365-my.sharepoint.com/personal/scsaalt_leeds_ac_uk/_layouts/15/embed.aspx?UniqueId=e5a9acea-f258-4952-8e05-46f5ffb0c576&embed=%7B%22ust%22%3Atrue%2C%22hv%22%3A%22CopyEmbedCode%22%7D&referrer=StreamWebApp&referrerScenario=EmbedDialog.Create" width="470" height="200" frameborder="0" scrolling="no" allowfullscreen title="3. Returns 1"></iframe>

<iframe src="https://leeds365-my.sharepoint.com/personal/scsaalt_leeds_ac_uk/_layouts/15/embed.aspx?UniqueId=8a1a8b63-58be-45ce-86b1-eedb4bc133c4&embed=%7B%22ust%22%3Atrue%2C%22hv%22%3A%22CopyEmbedCode%22%7D&referrer=StreamWebApp&referrerScenario=EmbedDialog.Create" width="470" height="200"frameborder="0" scrolling="no" allowfullscreen title="3. Returns 2"></iframe>

<h3 id="g_t-monotonicity-for-mdp-rewards"><span class="arithmatex">\(G_t\)</span> Monotonicity for MDP Rewards</h3>
<p>Let us see how the return develops for an MDP with a reward of 1 or -1 for each time step.
To calculate <span class="arithmatex">\(G_t\)</span> we will go backwards, i.e. we will need to calculate <span class="arithmatex">\(G_{t+1}\)</span> to be able to calculate <span class="arithmatex">\(G_t\)</span> due to the incremental form of <span class="arithmatex">\(G_t\)</span> where we have that <span class="arithmatex">\(G_t = R_{t+1} + \gamma G_{t+1}\)</span>.</p>
<ul>
<li>Mathematically, we can prove that <span class="arithmatex">\(G_t\)</span> is monotonically decreasing iff(if and only if) <span class="arithmatex">\(\frac{R_{t}}{1 - \gamma} &gt;  G_{t}\)</span> <span class="arithmatex">\(\forall t\)</span> and <span class="arithmatex">\(G_T=R_T &gt; 0\)</span>. <ul>
<li>Furthermore, when <span class="arithmatex">\(R_t=1\)</span> <span class="arithmatex">\(\forall t\)</span> and <span class="arithmatex">\(\gamma=.9\)</span> then <span class="arithmatex">\(G_t\)</span> converges in the limit to 10, i.e. 10 will be an upper bound for <span class="arithmatex">\(G_t\)</span>. </li>
<li>Similarly, when <span class="arithmatex">\(R_t=1\)</span> <span class="arithmatex">\(\forall t\)</span> and <span class="arithmatex">\(\gamma=.09\)</span> then <span class="arithmatex">\(G_t\)</span> converges in the limit to 100</li>
<li>More generally, when <span class="arithmatex">\(1-\gamma = 1/\beta\)</span> then <span class="arithmatex">\(R_t \beta &gt; G_t\)</span> </li>
</ul>
</li>
<li>On the other hand, we can prove that <span class="arithmatex">\(G_t\)</span> is monotonically increasing iff <span class="arithmatex">\(\frac{R_{t}}{1 - \gamma} &lt;  G_{t}\)</span>.<ul>
<li>Furthermore, when <span class="arithmatex">\(R_t=-1\)</span> <span class="arithmatex">\(\forall t\)</span> and <span class="arithmatex">\(\gamma=.9\)</span> then <span class="arithmatex">\(G_t\)</span> converges to -10, i.e. -10 is its lower bound. </li>
<li>More generally, when <span class="arithmatex">\(1-\gamma = 1/\beta\)</span> then <span class="arithmatex">\(R_t \beta &lt; G_t\)</span> </li>
</ul>
</li>
</ul>
<p>Below we prove the former and leave the latter for you as homework.</p>
<p><span class="arithmatex">\(G_t = R_{t+1} + \gamma G_{t+1}\)</span></p>
<p>We start by assuming that <span class="arithmatex">\(G_t\)</span> is strictly monotonically decreasing (we dropped the word strictly in th above for readability)</p>
<p><span class="arithmatex">\(G_t &gt; G_{t+1} &gt; 0\)</span> <span class="arithmatex">\(\forall t\)</span> (which entails that <span class="arithmatex">\(G_T=R_T &gt; 0\)</span> when the horizon is finite, i.e. ends at <span class="arithmatex">\(t=T\)</span>) we substitute by the incremental form of <span class="arithmatex">\(G_t\)</span></p>
<p><span class="arithmatex">\(G_t &gt; G_{t+1} &gt; 0\)</span> <span class="arithmatex">\(\forall t \implies R_{t+1} + \gamma G_{t+1} &gt;G_{t+1} \implies\)</span><br />
<span class="arithmatex">\(R_{t+1} &gt;  G_{t+1} - \gamma G_{t+1} \implies\)</span>
<span class="arithmatex">\(R_{t+1} &gt;  (1 - \gamma) G_{t+1} \implies\)</span></p>
<p><span class="arithmatex">\(\frac{R_{t+1}}{1 - \gamma} &gt; G_{t+1}\)</span> ( <span class="arithmatex">\(\gamma \ne 1\)</span>)</p>
<p>The inequality <span class="arithmatex">\(\frac{R_{t+1}}{1 - \gamma} &gt;  G_{t+1}\)</span> (which also can be written as <span class="arithmatex">\(\frac{R_{t}}{1 - \gamma} &gt;  G_{t}\)</span>) must be satisfied whenever <span class="arithmatex">\(G_t\)</span> is monotonically decreasing, i.e. it is a necessary condition. We can show that this inequality is also a sufficient condition to prove that <span class="arithmatex">\(G_t\)</span> is monotonically decreasing by following the same logic backwards. Similar things can be proven for the non-strictly monotonically decreasing case i.e. when <span class="arithmatex">\(G_t\ge G_{t+1} \ge 0\)</span> <span class="arithmatex">\(\forall t\)</span>.</p>
<p>Now when <span class="arithmatex">\(R_{t+1}=1\)</span> and <span class="arithmatex">\(\gamma=.9\)</span>, then by substituting these values in the inequality, we get that
<span class="arithmatex">\(\frac{1}{1 - .9} &gt;  G_{t+1} \implies\)</span> <span class="arithmatex">\(10 &gt; G_{t+1}\)</span> </p>
<p>We provided you with a simple code to in the worksheet below to demosntrate how the G changes with time steps.</p>
<p><div class="highlight"><pre><span></span><code><span class="n">G</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">R</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">γ</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="n">T</span> <span class="o">=</span> <span class="mi">100</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">t</span><span class="o">&gt;</span> <span class="mi">70</span><span class="p">:</span> <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;G_&#39;</span><span class="p">,</span><span class="n">t</span><span class="p">,</span><span class="s1">&#39;=&#39;</span><span class="p">,</span><span class="nb">round</span><span class="p">(</span><span class="n">G</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
    <span class="n">G</span> <span class="o">=</span> <span class="n">R</span> <span class="o">+</span> <span class="n">γ</span><span class="o">*</span><span class="n">G</span> 
</code></pre></div>
    G_ 100 = 0
    G_ 99 = 1.0
    G_ 98 = 1.9
    G_ 97 = 2.71
    G_ 96 = 3.439
    ...</p>
<h3 id="g_t-monotonicity-for-sparse-mdp-rewards"><span class="arithmatex">\(G_t\)</span> Monotonicity for Sparse MDP Rewards</h3>
<p>For sparse positive end-of-episode rewards, the above strict inequality is not satisfied since <span class="arithmatex">\(R_t=0\)</span> <span class="arithmatex">\(\forall t&lt;T\)</span> and <span class="arithmatex">\(R_T&gt;0\)</span>.
1. In this case, we can show that <span class="arithmatex">\(G_t \le G_{t+1}\)</span> i.e. <span class="arithmatex">\(G_t\)</span> it is a monotonically increasing function.
    1. Furthermore, when <span class="arithmatex">\(\gamma&lt;1\)</span> then <span class="arithmatex">\(G_t\)</span> is strictly increasing, i.e.  <span class="arithmatex">\(G_t &lt; G_{t+1}\)</span>
1. Furthermore, <span class="arithmatex">\(G_{t} = \gamma^{T-t-1} R_{T}\)</span>.
    1. when <span class="arithmatex">\(R_T=1\)</span> then <span class="arithmatex">\(G_{t} = \gamma^{T-t-1}\)</span> 
    1. when <span class="arithmatex">\(R_T=-1\)</span> then <span class="arithmatex">\(G_{t} = -\gamma^{T-t-1}\)</span></p>
<ul>
<li>
<p>To prove the monotonicity we start with our incremental form for the return: 
    <span class="arithmatex">\(G_t = R_{t+1} + \gamma G_{t+1}\)</span>:</p>
<p>Since we have that <span class="arithmatex">\(R_{t+1} = 0\)</span> <span class="arithmatex">\(\forall t&lt;T\)</span> then</p>
<p><span class="arithmatex">\(G_t = \gamma G_{t+1}\)</span> <span class="arithmatex">\(\forall t&lt;T\)</span>, therefore, since <span class="arithmatex">\(\gamma \le 1\)</span> then <span class="arithmatex">\(G_t \le G_{t+1}\)</span> <span class="arithmatex">\(\forall t&lt;T\)</span>.</p>
</li>
<li>
<p>To prove that  <span class="arithmatex">\(G_{t} = \gamma^{T-t-1} R_{T}\)</span> we can also utilise the incremental form and perform a deduction, but it is easier to start with the general form of a return, we have:</p>
<p><span class="arithmatex">\(G_t = R_{t+1} + \gamma R_{t+2}  + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + ... + \gamma^{T-t-1} R_{T}\)</span></p>
<p>Since we have that <span class="arithmatex">\(R_{t+1} = 0\)</span> <span class="arithmatex">\(\forall t&lt;T\)</span> then</p>
<p><span class="arithmatex">\(G_t = \gamma^{T-t-1} R_{T}\)</span></p>
</li>
</ul>
<p>This gives us guidance on the type of behaviour that we expect our agent to develop when we follow one of these reward regimes (sparse or non-sparse). </p>
<p>The above suggests that for sparse end-of-episode rewards, decisions near the terminal state(s) have far more important effects on the learning process than earlier decisions. While for non-sparse positive rewards MDPs, earlier states have higher returns and hence more importance than near terminal states. </p>
<p>If we want our agent to place more importance on earlier states, and near-starting state decisions, then we will need to utilise non-sparse (positive or negative) rewards. Positive rewards encourage repeating certain actions that maintain the stream of positive rewards for the agent. An example will be the pole balancing problem. Negative rewards, encourage the agent to speed up towards ending the episode so that it can minimise the number of negative rewards received.</p>
<p>When we want our agent to place more importance for the decisions near the terminal states, then a sparse reward is more convenient. Sparse rewards are also more suitable for offline learning as they simplify the learning and analysis of the agent's behaviour. Non-sparse rewards suit online learning on the other hand, because they give a quick indication of the agent behaviour suitability and hence speed up the early population of the value function. </p>
<p><div class="highlight"><pre><span></span><code><span class="n">G</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">γ</span> <span class="o">=</span> <span class="mf">0.9</span> <span class="c1"># change to 1 to see the effect</span>
<span class="n">T</span> <span class="o">=</span> <span class="mi">100</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span> <span class="c1"># note that if we use a forward loop, our calculations will be all wrong although the code runs</span>
    <span class="k">if</span> <span class="n">t</span><span class="o">&gt;</span><span class="mi">70</span><span class="p">:</span> <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;G_&#39;</span><span class="p">,</span><span class="n">t</span><span class="p">,</span><span class="s1">&#39;=&#39;</span><span class="p">,</span><span class="nb">round</span><span class="p">(</span><span class="n">G</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="nb">round</span><span class="p">(</span><span class="n">γ</span><span class="o">**</span><span class="p">(</span><span class="n">T</span><span class="o">-</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span><span class="mi">2</span><span class="p">)</span> <span class="k">if</span> <span class="n">t</span><span class="o">&lt;</span><span class="n">T</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">R</span><span class="o">=</span><span class="mi">1</span> <span class="k">if</span> <span class="n">t</span><span class="o">==</span><span class="n">T</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="n">G</span> <span class="o">=</span> <span class="n">R</span> <span class="o">+</span> <span class="n">γ</span><span class="o">*</span><span class="n">G</span>
</code></pre></div>
    G_ 100 = 0 0
    G_ 99 = 1.0 1.0
    G_ 98 = 0.9 0.9
    G_ 97 = 0.81 0.81
    ...</p>
<h2 id="the-expected-return-function-v">The Expected Return Function V</h2>
<p>Once we move form an actul return that comes froma an actual experience at time step <span class="arithmatex">\(t\)</span> to try to estimate this return, we move to an expectaiton <em>function</em>. This function, traditionally called the value function v, is an important function. But now isntead of tying the value of the return to a particular experience at a step t which would be less useful in generalising the lessons an agent can learn from interacting with the environment, it makes more sense to ty this up to a certain state <span class="arithmatex">\(s\)</span>. This will allow the agent to learn a useful expectation of the return(discounted sum of rewards) for a particualr state when the agent follows a policy <span class="arithmatex">\(\pi\)</span>. I.e. we are now saying that a we will get an expected value of the return for a particular state under a policy <span class="arithmatex">\(\pi\)</span>. So we moved from subscripting by a step <span class="arithmatex">\(t\)</span> into passing a state <span class="arithmatex">\(s\)</span> to the function and subscripting by a policy.</p>
<div class="arithmatex">\[
\begin{equation}
    v_{\pi}(s) = \mathbb{E}_{\pi}(G_t)   \label{eq:v}  %\tag{1}
\end{equation}
\]</div>
<p>Equation <span class="arithmatex">\(\eqref{eq:v}\)</span> gives the definition of v function.</p>
<p>In the following video we tackle this idea in more details.</p>
<p><iframe src="https://leeds365-my.sharepoint.com/personal/scsaalt_leeds_ac_uk/_layouts/15/embed.aspx?UniqueId=7b8178ed-68d1-4335-8ab7-3d81f214f362&embed=%7B%22ust%22%3Atrue%2C%22hv%22%3A%22CopyEmbedCode%22%7D&referrer=StreamWebApp&referrerScenario=EmbedDialog.Create" width="940" height="200"frameborder="0" scrolling="no" allowfullscreen title="4. Returns Expectation and Sampling.mkv"></iframe></p>
<h2 id="bellman-equation-for-v-and-q">Bellman Equation for V and Q</h2>
<iframe src="https://leeds365-my.sharepoint.com/personal/scsaalt_leeds_ac_uk/_layouts/15/embed.aspx?UniqueId=6d6d9455-7174-447a-8bcb-eceaa51a4af5&embed=%7B%22ust%22%3Atrue%2C%22hv%22%3A%22CopyEmbedCode%22%7D&referrer=StreamWebApp&referrerScenario=EmbedDialog.Create" width="470" height="200" frameborder="0" scrolling="no" allowfullscreen title="5. Bellman v.mkv"></iframe>

<iframe src="https://leeds365-my.sharepoint.com/personal/scsaalt_leeds_ac_uk/_layouts/15/embed.aspx?UniqueId=3a18cbb0-6960-42c1-bcf4-8e0893c09c89&embed=%7B%22ust%22%3Atrue%2C%22hv%22%3A%22CopyEmbedCode%22%7D&referrer=StreamWebApp&referrerScenario=EmbedDialog.Create" width="470" height="200"frameborder="0" scrolling="no" allowfullscreen title="6. Bellman q simple.mkv"></iframe>

<h2 id="bellman-optimality-equations-for-v-and-q">Bellman Optimality Equations for V and Q</h2>
<iframe src="https://leeds365-my.sharepoint.com/personal/scsaalt_leeds_ac_uk/_layouts/15/embed.aspx?UniqueId=b73eab99-7af2-4b9e-8909-19492615d273&embed=%7B%22ust%22%3Atrue%2C%22hv%22%3A%22CopyEmbedCode%22%7D&referrer=StreamWebApp&referrerScenario=EmbedDialog.Create" width="470" height="200" frameborder="0" scrolling="no" allowfullscreen title="7. Bellman Optimality 1.mkv"></iframe>

<iframe src="https://leeds365-my.sharepoint.com/personal/scsaalt_leeds_ac_uk/_layouts/15/embed.aspx?UniqueId=8d89893b-6e99-4380-a31e-93e2974cd04a&embed=%7B%22ust%22%3Atrue%2C%22hv%22%3A%22CopyEmbedCode%22%7D&referrer=StreamWebApp&referrerScenario=EmbedDialog.Create" width="470" height="200" frameborder="0" scrolling="no" allowfullscreen title="7. Bellman Optimality 2.mkv"></iframe>

<p>You can adjust the video settings in SharePoint (speed up to 1.2 and reduce the noise if necessary)</p>
<p><em>Exercise 1</em>: If you realise there is a missing symbol in the [video: Bellman Equation for v] last equations, do you know what it is and where it has originally come from?</p>
<p><em>Exercise 2</em>: Can you derive Bellman Optimality Equation for <span class="arithmatex">\(q(s,a)\)</span> from first principles?</p>
<p><a href="https://leeds365-my.sharepoint.com/:v:/g/personal/scsaalt_leeds_ac_uk/EVBv-P5S4_VKqFt_E0vikIUBdpV1BZX2V-IDM3ROXDDV4A?e=YQQchV">video:  Bellman Optimality for q from first principles</a></p>
<h2 id="grid-world-environments">Grid World Environments</h2>
<p>Ok, so now we are ready to tackle the practicals, please go ahead and download the worksheet and run and experiement with the provided code to build some grid world environments and visualise them and make a simple robot agent takes some steps/actions within these environments!.</p>
<p>You will need to download a python library (Grid.py) that we bespokley developed to help you run RL algorithms on toy problems and be abel to easily visualise them as needed, the code is optimised to run efficiently and you will be able to use these environmnets to test different RL algorithms extensively. Please place the library in the same directory of the worksheet. In general it would be a good idea to place all worksheets and libraries provided in one directory. This will make importing and runing code easier and more streamlined.</p>
<p>In a grid world, we have a set of cells that the agent can move between them inside a box. The agent can move left, right, up and down. We can also allow the agent to move diagonally, but this is uncommon. </p>
<p>We needed to be as efficient as possible, and hence we have chosen to represent each state by its count, where we count from the bottom left corner up to the right top corner, and we start with 0 up to nS-1 where nS is the number of states. This will allow us to streamline the process of accessing and storing a state and will be of at most efficiency. We also deal with actions similarly, i.e. each action of the nA actions is given an index 0..nA-1. For the usual grid, this means 0,1,2,3 for actions left, right, up and down. We represent a 2-d grid by a 1-d array, and so care must be taken on how the agent moves between cells. </p>
<p>Moving left or right seems easy because we can add or subtract from the <em>current</em> state. But when the agent is on the edge of the box, we cannot allow for an action that takes it out of the box. So if the agent is on the far right, we cannot allow it to go further to the right. To account for this issue, we have written a valid() function to validate an action. Moving up and down is similar, but we need to add and subtract a full row, which is how many columns we have in our grid. the valid() function checks for the current state and what would be the next state, and it knows that an agent will overstep the boundary as follows: if s%cols!=0, this means that the agent was not on the left edge and executing a right action (s+a)%cols==0 will take it to the left edge. This means it was on the right edge and wanted to move off this right edge. Other checks are similar. We have also accounted for moving diagonally so the agent will not overstep the boundaries.</p>
<p>We have also accounted for different reward schemes that we might want to use later in other lessons. These are formulated as an array of 4 elements [intermediate, goal1, goal2, cliff] the first reward represents the reward the agent obtains if it is on any intermediate cell. Intermediate cells are non-terminal cells. Goals or terminal states are those that a task would be completed if the agent steps into them. By setting the goals array, we can decide which cells are terminal/goals. As we can see, there are two goals, this will allow us to deal with all the classical problems we will tackle in our RL treatments, but we could have set up more. So, our reward array's second and third elements are for the goals. The last entry is for a cliff. A cliff cell is a special type of non-terminal cell where the agent will emulate falling off a cliff and usually is given a high negative reward and then will be hijacked and put in its start position when it steps into these types of cells. These types of cells are non-terminal in the sense that the agent did not achieve the task when it went to them, but they provoke a reset of the agent position with a large negative reward.</p>
<p>The most important function in our class is the step(a) function. An agent will take a specific action in the environment via this function. Via this function, we return the reward from our environment and a flag (done) indicating whether the task is accomplished. This makes our environment compatible with the classic setup of an OpenAI Gym Atari games, which we deal with towards the end of our RL treatment.</p>
<h2 id="your-turn">Your turn</h2>
<p>Go ahead and play around with some grid world environment by executing and experiementing with the code in the following worksheet.</p>
<!-- <a href="Grid.py" download> Grid world library</a> -->

<p><a href="../../workseets/worksheet3.ipynb">worksheet3</a></p>
<h2 id="conclusion">Conclusion</h2>
<p>In this lesson, we covered the basics of RL functions and concepts that we will utilise in other lessons. We also provided you with a environment that you can directly utilise to build simple grid world environments. Please note that you are not required to study the Gris.py file or understand how the grid is programmatically built, but you need to understand how it operates.!</p>







  
  






                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright Abdulrahman Altahhan
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.sections"], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.f1b6f286.min.js"></script>
      
        <script src="../../javascript/tablecontentsoverride.js"></script>
      
        <script src="../../javascript/config.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>