
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
        <meta name="author" content="Dr Abdulrahman Altahhan">
      
      
      
        <link rel="prev" href="../lesson2/lesson2.html">
      
      
        <link rel="next" href="../lesson4/lesson4.html">
      
      
      <link rel="icon" href="../../../docs/img/favicon.jpg">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.4">
    
    
      
        <title>3. MDP - Reinforcement Learning for Autonomous Agents-codespace</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.8608ea7d.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../css/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#lesson-3-markov-decision-processes-dynamics-and-bellman-equaitons" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
        <aside class="md-banner">
          <div class="md-banner__inner md-grid md-typeset">
            
            
    <div id="versionIndicator"><b>Version:</b> 04.06.21.a</div>
    <img id="customlogo" src="../../img/logo.svg" alt="University of Leeds logo.">

          </div>
          
        </aside>
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../index.html" title="Reinforcement Learning for Autonomous Agents-codespace" class="md-header__button md-logo" aria-label="Reinforcement Learning for Autonomous Agents-codespace" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Reinforcement Learning for Autonomous Agents-codespace
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              3. MDP
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../index.html" class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../lesson1/lesson1.html" class="md-tabs__link">
          
  
  Unit 1

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit2/lesson5/lesson5.html" class="md-tabs__link">
          
  
  Unit 2

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit3/lesson8/lesson8.html" class="md-tabs__link">
          
  
  Unit 3

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit4/lesson12/lesson12.html" class="md-tabs__link">
          
  
  Unit 4

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit5/lesson15/lesson15.html" class="md-tabs__link">
          
  
  Unit 5

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit6/lesson18/lesson18.html" class="md-tabs__link">
          
  
  Unit 6

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../index.html" title="Reinforcement Learning for Autonomous Agents-codespace" class="md-nav__button md-logo" aria-label="Reinforcement Learning for Autonomous Agents-codespace" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Reinforcement Learning for Autonomous Agents-codespace
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../index.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Unit 1
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Unit 1
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lesson1/lesson1.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1. Tabular Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lesson2/lesson2.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2. K-Arm Bandit
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    3. MDP
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="lesson3.html" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    3. MDP
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-markov-decision-process-mdp" class="md-nav__link">
    <span class="md-ellipsis">
      1. Markov Decision Process (MDP)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. Markov Decision Process (MDP)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#markov-property" class="md-nav__link">
    <span class="md-ellipsis">
      Markov Property
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stationarity" class="md-nav__link">
    <span class="md-ellipsis">
      Stationarity
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deterministic-vs-stochastic-dynamics" class="md-nav__link">
    <span class="md-ellipsis">
      Deterministic vs. Stochastic Dynamics
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reward-structure" class="md-nav__link">
    <span class="md-ellipsis">
      Reward Structure
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#irreducibility-and-aperiodicity" class="md-nav__link">
    <span class="md-ellipsis">
      Irreducibility and Aperiodicity
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-policy-and-its-stationarity" class="md-nav__link">
    <span class="md-ellipsis">
      2. Policy and its Stationarity
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. Policy and its Stationarity">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#stationary-policy" class="md-nav__link">
    <span class="md-ellipsis">
      Stationary Policy
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#non-stationary-policy" class="md-nav__link">
    <span class="md-ellipsis">
      Non-Stationary Policy
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-transition-and-reward-dynamics" class="md-nav__link">
    <span class="md-ellipsis">
      2. Transition and Reward Dynamics
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. Transition and Reward Dynamics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transition-dynamics" class="md-nav__link">
    <span class="md-ellipsis">
      Transition Dynamics
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Transition Dynamics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#key-properties-of-transition-dynamics" class="md-nav__link">
    <span class="md-ellipsis">
      Key Properties of Transition Dynamics:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reward-dynamics" class="md-nav__link">
    <span class="md-ellipsis">
      Reward Dynamics
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Reward Dynamics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#reward-function-properties" class="md-nav__link">
    <span class="md-ellipsis">
      Reward Function Properties:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-the-return-g_t-of-a-time-step-t" class="md-nav__link">
    <span class="md-ellipsis">
      2. The Return \(G_t\) of a time step \(t\)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. The Return \(G_t\) of a time step \(t\)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#g_t-monotonicity-for-mdp-rewards" class="md-nav__link">
    <span class="md-ellipsis">
      \(G_t\) Monotonicity for MDP Rewards
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#g_t-monotonicity-for-sparse-mdp-rewards" class="md-nav__link">
    <span class="md-ellipsis">
      \(G_t\) Monotonicity for Sparse MDP Rewards
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-the-expected-return-function-v" class="md-nav__link">
    <span class="md-ellipsis">
      2. The Expected Return Function V
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-the-bellman-equations" class="md-nav__link">
    <span class="md-ellipsis">
      2. The Bellman Equations
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. The Bellman Equations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-bellman-equation-for-the-value-function" class="md-nav__link">
    <span class="md-ellipsis">
      2.1 Bellman Equation for the Value Function
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-bellman-equation-for-the-q-function" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 Bellman Equation for the Q-Function
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23-bellman-optimality-equations" class="md-nav__link">
    <span class="md-ellipsis">
      2.3 Bellman Optimality Equations
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.3 Bellman Optimality Equations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bellman-optimality-equation-for-the-value-function" class="md-nav__link">
    <span class="md-ellipsis">
      Bellman Optimality Equation for the Value Function:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bellman-optimality-equation-for-the-q-function" class="md-nav__link">
    <span class="md-ellipsis">
      Bellman Optimality Equation for the Q-Function:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    <span class="md-ellipsis">
      Summary
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#your-turn" class="md-nav__link">
    <span class="md-ellipsis">
      Your turn
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lesson4/lesson4.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    4. ROS
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 2
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Unit 2
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit2/lesson5/lesson5.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    5. Dynamic Programming
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit2/lesson6/lesson6.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    6. Monte Carlo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit2/lesson7/lesson7.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    7. Mobile Robots
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 3
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Unit 3
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson8/lesson8.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    8. Temporal Difference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson9/lesson9.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    9. n-Step Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson10/lesson10.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    10. Planning in RL(optional)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson11/lesson11.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    11. Localisation and SLAM
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 4
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Unit 4
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit4/lesson12/lesson12.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    12. Function Approximation Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit4/lesson13/lesson13.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    13. Linear Approximation for Prediction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit4/lesson14/lesson14.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    14. Linear Approximation for Control
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 5
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Unit 5
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit5/lesson15/lesson15.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    15. Linear Approximation with Eligibility Traces(prediction and control)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit5/lesson16/lesson16.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    16. Nonlinear Approximation for Control
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit5/lesson17/lesson17.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    17. Application on Robot Navigation
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 6
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Unit 6
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit6/lesson18/lesson18.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    18. Application on Games(optional)
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-markov-decision-process-mdp" class="md-nav__link">
    <span class="md-ellipsis">
      1. Markov Decision Process (MDP)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. Markov Decision Process (MDP)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#markov-property" class="md-nav__link">
    <span class="md-ellipsis">
      Markov Property
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stationarity" class="md-nav__link">
    <span class="md-ellipsis">
      Stationarity
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deterministic-vs-stochastic-dynamics" class="md-nav__link">
    <span class="md-ellipsis">
      Deterministic vs. Stochastic Dynamics
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reward-structure" class="md-nav__link">
    <span class="md-ellipsis">
      Reward Structure
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#irreducibility-and-aperiodicity" class="md-nav__link">
    <span class="md-ellipsis">
      Irreducibility and Aperiodicity
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-policy-and-its-stationarity" class="md-nav__link">
    <span class="md-ellipsis">
      2. Policy and its Stationarity
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. Policy and its Stationarity">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#stationary-policy" class="md-nav__link">
    <span class="md-ellipsis">
      Stationary Policy
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#non-stationary-policy" class="md-nav__link">
    <span class="md-ellipsis">
      Non-Stationary Policy
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-transition-and-reward-dynamics" class="md-nav__link">
    <span class="md-ellipsis">
      2. Transition and Reward Dynamics
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. Transition and Reward Dynamics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transition-dynamics" class="md-nav__link">
    <span class="md-ellipsis">
      Transition Dynamics
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Transition Dynamics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#key-properties-of-transition-dynamics" class="md-nav__link">
    <span class="md-ellipsis">
      Key Properties of Transition Dynamics:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reward-dynamics" class="md-nav__link">
    <span class="md-ellipsis">
      Reward Dynamics
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Reward Dynamics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#reward-function-properties" class="md-nav__link">
    <span class="md-ellipsis">
      Reward Function Properties:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-the-return-g_t-of-a-time-step-t" class="md-nav__link">
    <span class="md-ellipsis">
      2. The Return \(G_t\) of a time step \(t\)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. The Return \(G_t\) of a time step \(t\)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#g_t-monotonicity-for-mdp-rewards" class="md-nav__link">
    <span class="md-ellipsis">
      \(G_t\) Monotonicity for MDP Rewards
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#g_t-monotonicity-for-sparse-mdp-rewards" class="md-nav__link">
    <span class="md-ellipsis">
      \(G_t\) Monotonicity for Sparse MDP Rewards
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-the-expected-return-function-v" class="md-nav__link">
    <span class="md-ellipsis">
      2. The Expected Return Function V
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-the-bellman-equations" class="md-nav__link">
    <span class="md-ellipsis">
      2. The Bellman Equations
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. The Bellman Equations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-bellman-equation-for-the-value-function" class="md-nav__link">
    <span class="md-ellipsis">
      2.1 Bellman Equation for the Value Function
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-bellman-equation-for-the-q-function" class="md-nav__link">
    <span class="md-ellipsis">
      2.2 Bellman Equation for the Q-Function
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23-bellman-optimality-equations" class="md-nav__link">
    <span class="md-ellipsis">
      2.3 Bellman Optimality Equations
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2.3 Bellman Optimality Equations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bellman-optimality-equation-for-the-value-function" class="md-nav__link">
    <span class="md-ellipsis">
      Bellman Optimality Equation for the Value Function:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bellman-optimality-equation-for-the-q-function" class="md-nav__link">
    <span class="md-ellipsis">
      Bellman Optimality Equation for the Q-Function:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    <span class="md-ellipsis">
      Summary
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#your-turn" class="md-nav__link">
    <span class="md-ellipsis">
      Your turn
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<script>
  window.MathJax = {
    tex: {
      tags: "ams",  // Enables equation numbering
    //   displayMath: [['$$', '$$'], ['\\[', '\\]']]
    }
  };
</script>

<h1 id="lesson-3-markov-decision-processes-dynamics-and-bellman-equaitons">Lesson 3- Markov Decision Processes, Dynamics and Bellman Equaitons</h1>
<p><strong>Learning outcomes</strong></p>
<ol>
<li>understand MDP and its elements</li>
<li>understand the return for a time step <span class="arithmatex">\(t\)</span></li>
<li>understand the expected return of a state <span class="arithmatex">\(s\)</span></li>
<li>understand the Bellman optimality equations</li>
<li>become familiar with the different types of grid world problems</li>
</ol>
<!-- 6. become familiar with the way we assign a reward to an environment
1. be able to execute actions in a grid world and observe the result
2. be able to to visualise a policy and its action-value function -->

<h2 id="1-markov-decision-process-mdp">1. Markov Decision Process (MDP)</h2>
<p>A <strong>Markov Decision Process (MDP)</strong> provides a mathematical framework to model decision-making problems where an agent interacts with an environment. It is characterized by a tuple <span class="arithmatex">\( (S, A, P, R, \gamma) \)</span> where:</p>
<ul>
<li><span class="arithmatex">\( S \)</span> is the set of states in the environment.</li>
<li><span class="arithmatex">\( A \)</span> is the set of actions available to the agent.</li>
<li><span class="arithmatex">\( P(s' | s, a) \)</span> is the <strong>transition function</strong>, representing the probability of transitioning from state <span class="arithmatex">\( s \)</span> to state <span class="arithmatex">\( s' \)</span> after taking action <span class="arithmatex">\( a \)</span>.</li>
<li><span class="arithmatex">\( R(s, a, s') \)</span> is the <strong>reward function</strong>, representing the immediate reward received when transitioning from state <span class="arithmatex">\( s \)</span> to state <span class="arithmatex">\( s' \)</span> after taking action <span class="arithmatex">\( a \)</span>.</li>
<li><span class="arithmatex">\( \gamma \)</span> is the <strong>discount factor</strong>, a value between 0 and 1 that determines the importance of future rewards relative to immediate rewards.</li>
</ul>
<p>The MDP framework is central to reinforcement learning (RL), as it allows the agent to plan and optimize its actions over time to maximize the expected return.</p>
<h3 id="markov-property">Markov Property</h3>
<p>The <strong>Markov Property</strong> asserts that the future state depends only on the current state and action, not on any previous states or actions. This is a core assumption in MDPs and ensures that the system has <strong>no memory</strong> of past actions or states.</p>
<p>Formally:</p>
<div class="arithmatex">\[
P(s_{t+1} | s_t, a_t, \dots, s_0, a_0) = P(s_{t+1} | s_t, a_t)
\]</div>
<h3 id="stationarity">Stationarity</h3>
<p>In many MDPs, the transition and reward functions are <strong>stationary</strong>, meaning that they do not change over time. This ensures that the transition probabilities and rewards are the same at every time step.</p>
<p>Formally:</p>
<div class="arithmatex">\[
P(s' | s, a) = P(s' | s, a) \quad \forall t
\]</div>
<h3 id="deterministic-vs-stochastic-dynamics">Deterministic vs. Stochastic Dynamics</h3>
<ul>
<li><strong>Deterministic Dynamics</strong>: If the transition function <span class="arithmatex">\( P(s' | s, a) \)</span> always produces the same next state, the system is deterministic.</li>
<li><strong>Stochastic Dynamics</strong>: If <span class="arithmatex">\( P(s' | s, a) \)</span> is probabilistic, the system is stochastic.</li>
</ul>
<h3 id="reward-structure">Reward Structure</h3>
<p>The <strong>reward function</strong> <span class="arithmatex">\( R(s, a, s') \)</span> can either be <strong>deterministic</strong> (fixed reward) or <strong>stochastic</strong> (random reward). The structure of the rewards influences the agent's behavior and learning.</p>
<h3 id="irreducibility-and-aperiodicity">Irreducibility and Aperiodicity</h3>
<p>For algorithms like <strong>Value Iteration</strong>, it is important that the MDP is <strong>irreducible</strong> (all states are reachable from any other state) and <strong>aperiodic</strong> (there are no cycles of fixed lengths that prevent convergence).</p>
<hr />
<h2 id="2-policy-and-its-stationarity">2. Policy and its Stationarity</h2>
<p>A <strong>policy</strong> in reinforcement learning is a strategy or function that defines the agent's actions at each state in an environment. Mathematically, a policy is often represented as <span class="arithmatex">\( \pi(a|s) \)</span>, where <span class="arithmatex">\( s \)</span> is a state and <span class="arithmatex">\( a \)</span> is an action. The policy <span class="arithmatex">\( \pi(a|s) \)</span> gives the probability of taking action <span class="arithmatex">\( a \)</span> when in state <span class="arithmatex">\( s \)</span>. </p>
<h3 id="stationary-policy">Stationary Policy</h3>
<p>A <strong>stationary policy</strong> is one where the action probabilities depend only on the current state and remain constant over time. Formally, a stationary policy satisfies:
[
\pi_t(a|s) = \pi(a|s) \quad \text{for all time steps} \, t
]
This means the policy does not change as the environment evolves. This is common in many reinforcement learning settings where the dynamics of the problem do not change over time.</p>
<h3 id="non-stationary-policy">Non-Stationary Policy</h3>
<p>In contrast, a <strong>non-stationary policy</strong> is one where the action probabilities can change with time:
[
\pi_t(a|s) \neq \pi_{t'}(a|s) \quad \text{for some} \, t \neq t'
]
This occurs when the policy is adapted or modified based on external factors, such as learning or changes in the environment. A non-stationary policy is useful in situations where the environment or the agent's understanding of it evolves over time.</p>
<h2 id="2-transition-and-reward-dynamics">2. Transition and Reward Dynamics</h2>
<h3 id="transition-dynamics">Transition Dynamics</h3>
<p>The <strong>transition dynamics</strong> describe how the environment behaves when the agent takes an action in a given state. The transition function <span class="arithmatex">\( P(s' | s, a) \)</span> specifies the probability of transitioning from state <span class="arithmatex">\( s \)</span> to state <span class="arithmatex">\( s' \)</span> when the agent takes action <span class="arithmatex">\( a \)</span>.</p>
<p>Formally, the transition function is expressed as:</p>
<div class="arithmatex">\[
P(s' | s, a) = \mathbb{P}(s_{t+1} = s' | s_t = s, a_t = a)
\]</div>
<p>Where:
- <span class="arithmatex">\( s_t \)</span> is the state at time step <span class="arithmatex">\( t \)</span>,
- <span class="arithmatex">\( a_t \)</span> is the action taken at time step <span class="arithmatex">\( t \)</span>,
- <span class="arithmatex">\( s_{t+1} \)</span> is the next state after taking action <span class="arithmatex">\( a_t \)</span> from <span class="arithmatex">\( s_t \)</span>.</p>
<h4 id="key-properties-of-transition-dynamics">Key Properties of Transition Dynamics:</h4>
<ul>
<li><strong>Stochastic Nature</strong>: Transition dynamics are typically <strong>stochastic</strong>, meaning that taking the same action in the same state may result in different next states with some probability.</li>
<li><strong>Markov Property</strong>: The system satisfies the <strong>Markov Property</strong>, meaning the next state depends only on the current state and action, not on the history of previous states or actions.</li>
</ul>
<p>Example:
In a grid world, if the agent is at state <span class="arithmatex">\( s = (2, 2) \)</span> and takes action <span class="arithmatex">\( a = \text{move left} \)</span>, the transition probability might be deterministic:</p>
<div class="arithmatex">\[
P(s' | (2, 2), \text{move left}) = 
\begin{cases} 
1 &amp; \text{if } s' = (1, 2) \\
0 &amp; \text{otherwise}
\end{cases}
\]</div>
<p>This means that the agent always moves from <span class="arithmatex">\( (2, 2) \)</span> to <span class="arithmatex">\( (1, 2) \)</span> when taking the action "move left".</p>
<iframe src="https://leeds365-my.sharepoint.com/personal/scsaalt_leeds_ac_uk/_layouts/15/embed.aspx?UniqueId=f93d3c1e-261f-42bd-93cd-92f67e120d99&embed=%7B%22ust%22%3Atrue%7D&referrer=StreamWebApp&referrerScenario=EmbedDialog.Create" width="470" height="200" frameborder="0" scrolling="no" allowfullscreen title="Markov Decision Processes (MDP)" enablejsapi=1></iframe>

<iframe src="https://leeds365-my.sharepoint.com/personal/scsaalt_leeds_ac_uk/_layouts/15/embed.aspx?UniqueId=79d3a9ea-5401-4f3c-bcf2-5907255ef8da&embed=%7B%22ust%22%3Atrue%7D&referrer=StreamWebApp&referrerScenario=EmbedDialog.Create" width="470" height="200"frameborder="0" scrolling="no" allowfullscreen title="2. Dynamics.mkv"></iframe>

<h3 id="reward-dynamics">Reward Dynamics</h3>
<p>The <strong>reward dynamics</strong> define the reward that the agent receives when it takes an action in a given state and transitions to a new state. The reward function <span class="arithmatex">\( R(s, a, s') \)</span> specifies the immediate reward when transitioning from state <span class="arithmatex">\( s \)</span> to state <span class="arithmatex">\( s' \)</span> after taking action <span class="arithmatex">\( a \)</span>.</p>
<p>Formally, the reward function is expressed as:</p>
<div class="arithmatex">\[
R(s, a, s') = \mathbb{E}[r_{t+1} | s_t = s, a_t = a, s_{t+1} = s']
\]</div>
<p>Where:
- <span class="arithmatex">\( r_{t+1} \)</span> is the reward received at time step <span class="arithmatex">\( t+1 \)</span>,
- <span class="arithmatex">\( s_t \)</span> and <span class="arithmatex">\( a_t \)</span> represent the state and action at time <span class="arithmatex">\( t \)</span>,
- <span class="arithmatex">\( s_{t+1} \)</span> is the resulting state at time <span class="arithmatex">\( t+1 \)</span>.</p>
<h4 id="reward-function-properties">Reward Function Properties:</h4>
<ul>
<li><strong>Immediate Reward</strong>: <span class="arithmatex">\( R(s, a, s') \)</span> gives the immediate reward for transitioning from state <span class="arithmatex">\( s \)</span> to state <span class="arithmatex">\( s' \)</span> after action <span class="arithmatex">\( a \)</span>.</li>
<li><strong>Stochastic Reward</strong>: Rewards can be <strong>stochastic</strong>, meaning the same action in the same state can yield different rewards.</li>
</ul>
<p>Example:
If the agent takes action <span class="arithmatex">\( a = \text{move right} \)</span> from state <span class="arithmatex">\( s = (1, 1) \)</span>, the reward function might be:</p>
<div class="arithmatex">\[
R((1, 1), \text{move right}, (2, 1)) = 10
\]</div>
<p>Indicating that moving to the goal state <span class="arithmatex">\( (2, 1) \)</span> yields a reward of 10.</p>
<p>Conversely, if the agent moves to a dangerous state:</p>
<div class="arithmatex">\[
R((1, 1), \text{move left}, (0, 1)) = -5
\]</div>
<p>The agent receives a penalty of -5.</p>
<h2 id="2-the-return-g_t-of-a-time-step-t">2. The Return <span class="arithmatex">\(G_t\)</span> of a time step <span class="arithmatex">\(t\)</span></h2>
<p>We start by realising the </p>
<div class="arithmatex">\[
\begin{align*}
    G_t = R_{t+1} + &amp;\gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + ... + \gamma^{T-t-1} R_{T} \\
    G_{t+1} =       &amp; R_{t+2} + \gamma R_{t+3} + \gamma^2 R_{t+4} + ... + \gamma^{T-t-2} R_{T}
\end{align*}
\]</div>
<p>Hence by multiplying <span class="arithmatex">\(G_{t+1}\)</span> by <span class="arithmatex">\(\gamma\)</span> and adding R_{t+1} we get</p>
<div class="arithmatex">\[
\begin{equation}
    G_t = R_{t+1} + \gamma G_{t+1}
\end{equation}
\]</div>
<p><strong>The above equation is the most important equation in RL that the Bellman Equations are built on it. In turn, we build all of our incremental updates in RL on Bellman optimality equation</strong></p>
<p>In the video below we talk more about this important concept.</p>
<iframe src="https://leeds365-my.sharepoint.com/personal/scsaalt_leeds_ac_uk/_layouts/15/embed.aspx?UniqueId=e5a9acea-f258-4952-8e05-46f5ffb0c576&embed=%7B%22ust%22%3Atrue%2C%22hv%22%3A%22CopyEmbedCode%22%7D&referrer=StreamWebApp&referrerScenario=EmbedDialog.Create" width="470" height="200" frameborder="0" scrolling="no" allowfullscreen title="3. Returns 1"></iframe>

<iframe src="https://leeds365-my.sharepoint.com/personal/scsaalt_leeds_ac_uk/_layouts/15/embed.aspx?UniqueId=8a1a8b63-58be-45ce-86b1-eedb4bc133c4&embed=%7B%22ust%22%3Atrue%2C%22hv%22%3A%22CopyEmbedCode%22%7D&referrer=StreamWebApp&referrerScenario=EmbedDialog.Create" width="470" height="200"frameborder="0" scrolling="no" allowfullscreen title="3. Returns 2"></iframe>

<h3 id="g_t-monotonicity-for-mdp-rewards"><span class="arithmatex">\(G_t\)</span> Monotonicity for MDP Rewards</h3>
<p>Let us see how the return develops for an MDP with a reward of 1 or -1 for each time step.
To calculate <span class="arithmatex">\(G_t\)</span> we will go backwards, i.e. we will need to calculate <span class="arithmatex">\(G_{t+1}\)</span> to be able to calculate <span class="arithmatex">\(G_t\)</span> due to the incremental form of <span class="arithmatex">\(G_t\)</span> where we have that <span class="arithmatex">\(G_t = R_{t+1} + \gamma G_{t+1}\)</span>.</p>
<ul>
<li>Mathematically, we can prove that <span class="arithmatex">\(G_t\)</span> is monotonically decreasing iff(if and only if) <span class="arithmatex">\(\frac{R_{t}}{1 - \gamma} &gt;  G_{t}\)</span> <span class="arithmatex">\(\forall t\)</span> and <span class="arithmatex">\(G_T=R_T &gt; 0\)</span>. <ul>
<li>Furthermore, when <span class="arithmatex">\(R_t=1\)</span> <span class="arithmatex">\(\forall t\)</span> and <span class="arithmatex">\(\gamma=.9\)</span> then <span class="arithmatex">\(G_t\)</span> converges in the limit to 10, i.e. 10 will be an upper bound for <span class="arithmatex">\(G_t\)</span>. </li>
<li>Similarly, when <span class="arithmatex">\(R_t=1\)</span> <span class="arithmatex">\(\forall t\)</span> and <span class="arithmatex">\(\gamma=.09\)</span> then <span class="arithmatex">\(G_t\)</span> converges in the limit to 100</li>
<li>More generally, when <span class="arithmatex">\(1-\gamma = 1/\beta\)</span> then <span class="arithmatex">\(R_t \beta &gt; G_t\)</span> </li>
</ul>
</li>
<li>On the other hand, we can prove that <span class="arithmatex">\(G_t\)</span> is monotonically increasing iff <span class="arithmatex">\(\frac{R_{t}}{1 - \gamma} &lt;  G_{t}\)</span>.<ul>
<li>Furthermore, when <span class="arithmatex">\(R_t=-1\)</span> <span class="arithmatex">\(\forall t\)</span> and <span class="arithmatex">\(\gamma=.9\)</span> then <span class="arithmatex">\(G_t\)</span> converges to -10, i.e. -10 is its lower bound. </li>
<li>More generally, when <span class="arithmatex">\(1-\gamma = 1/\beta\)</span> then <span class="arithmatex">\(R_t \beta &lt; G_t\)</span> </li>
</ul>
</li>
</ul>
<p>Below we prove the former and leave the latter for you as homework.</p>
<p><span class="arithmatex">\(G_t = R_{t+1} + \gamma G_{t+1}\)</span></p>
<p>We start by assuming that <span class="arithmatex">\(G_t\)</span> is strictly monotonically decreasing (we dropped the word strictly in th above for readability)</p>
<p><span class="arithmatex">\(G_t &gt; G_{t+1} &gt; 0\)</span> <span class="arithmatex">\(\forall t\)</span> (which entails that <span class="arithmatex">\(G_T=R_T &gt; 0\)</span> when the horizon is finite, i.e. ends at <span class="arithmatex">\(t=T\)</span>) we substitute by the incremental form of <span class="arithmatex">\(G_t\)</span></p>
<p><span class="arithmatex">\(G_t &gt; G_{t+1} &gt; 0\)</span> <span class="arithmatex">\(\forall t \implies R_{t+1} + \gamma G_{t+1} &gt;G_{t+1} \implies\)</span><br />
<span class="arithmatex">\(R_{t+1} &gt;  G_{t+1} - \gamma G_{t+1} \implies\)</span>
<span class="arithmatex">\(R_{t+1} &gt;  (1 - \gamma) G_{t+1} \implies\)</span></p>
<p><span class="arithmatex">\(\frac{R_{t+1}}{1 - \gamma} &gt; G_{t+1}\)</span> ( <span class="arithmatex">\(\gamma \ne 1\)</span>)</p>
<p>The inequality <span class="arithmatex">\(\frac{R_{t+1}}{1 - \gamma} &gt;  G_{t+1}\)</span> (which also can be written as <span class="arithmatex">\(\frac{R_{t}}{1 - \gamma} &gt;  G_{t}\)</span>) must be satisfied whenever <span class="arithmatex">\(G_t\)</span> is monotonically decreasing, i.e. it is a necessary condition. We can show that this inequality is also a sufficient condition to prove that <span class="arithmatex">\(G_t\)</span> is monotonically decreasing by following the same logic backwards. Similar things can be proven for the non-strictly monotonically decreasing case i.e. when <span class="arithmatex">\(G_t\ge G_{t+1} \ge 0\)</span> <span class="arithmatex">\(\forall t\)</span>.</p>
<p>Now when <span class="arithmatex">\(R_{t+1}=1\)</span> and <span class="arithmatex">\(\gamma=.9\)</span>, then by substituting these values in the inequality, we get that
<span class="arithmatex">\(\frac{1}{1 - .9} &gt;  G_{t+1} \implies\)</span> <span class="arithmatex">\(10 &gt; G_{t+1}\)</span> </p>
<h3 id="g_t-monotonicity-for-sparse-mdp-rewards"><span class="arithmatex">\(G_t\)</span> Monotonicity for Sparse MDP Rewards</h3>
<p>For sparse positive end-of-episode rewards, the above strict inequality is not satisfied since <span class="arithmatex">\(R_t=0\)</span> <span class="arithmatex">\(\forall t&lt;T\)</span> and <span class="arithmatex">\(R_T&gt;0\)</span>.
1. In this case, we can show that <span class="arithmatex">\(G_t \le G_{t+1}\)</span> i.e. <span class="arithmatex">\(G_t\)</span> it is a monotonically increasing function.
    1. Furthermore, when <span class="arithmatex">\(\gamma&lt;1\)</span> then <span class="arithmatex">\(G_t\)</span> is strictly increasing, i.e.  <span class="arithmatex">\(G_t &lt; G_{t+1}\)</span>
1. Furthermore, <span class="arithmatex">\(G_{t} = \gamma^{T-t-1} R_{T}\)</span>.
    1. when <span class="arithmatex">\(R_T=1\)</span> then <span class="arithmatex">\(G_{t} = \gamma^{T-t-1}\)</span> 
    1. when <span class="arithmatex">\(R_T=-1\)</span> then <span class="arithmatex">\(G_{t} = -\gamma^{T-t-1}\)</span></p>
<ul>
<li>
<p>To prove the monotonicity we start with our incremental form for the return: 
    <span class="arithmatex">\(G_t = R_{t+1} + \gamma G_{t+1}\)</span>:</p>
<p>Since we have that <span class="arithmatex">\(R_{t+1} = 0\)</span> <span class="arithmatex">\(\forall t&lt;T\)</span> then</p>
<p><span class="arithmatex">\(G_t = \gamma G_{t+1}\)</span> <span class="arithmatex">\(\forall t&lt;T\)</span>, therefore, since <span class="arithmatex">\(\gamma \le 1\)</span> then <span class="arithmatex">\(G_t \le G_{t+1}\)</span> <span class="arithmatex">\(\forall t&lt;T\)</span>.</p>
</li>
<li>
<p>To prove that  <span class="arithmatex">\(G_{t} = \gamma^{T-t-1} R_{T}\)</span> we can also utilise the incremental form and perform a deduction, but it is easier to start with the general form of a return, we have:</p>
<p><span class="arithmatex">\(G_t = R_{t+1} + \gamma R_{t+2}  + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + ... + \gamma^{T-t-1} R_{T}\)</span></p>
<p>Since we have that <span class="arithmatex">\(R_{t+1} = 0\)</span> <span class="arithmatex">\(\forall t&lt;T\)</span> then</p>
<p><span class="arithmatex">\(G_t = \gamma^{T-t-1} R_{T}\)</span></p>
</li>
</ul>
<p>This gives us guidance on the type of behaviour that we expect our agent to develop when we follow one of these reward regimes (sparse or non-sparse). </p>
<p>The above suggests that for sparse end-of-episode rewards, decisions near the terminal state(s) have far more important effects on the learning process than earlier decisions. While for non-sparse positive rewards MDPs, earlier states have higher returns and hence more importance than near terminal states. </p>
<p>If we want our agent to place more importance on earlier states, and near-starting state decisions, then we will need to utilise non-sparse (positive or negative) rewards. Positive rewards encourage repeating certain actions that maintain the stream of positive rewards for the agent. An example will be the pole balancing problem. Negative rewards, encourage the agent to speed up towards ending the episode so that it can minimise the number of negative rewards received.</p>
<p>When we want our agent to place more importance for the decisions near the terminal states, then a sparse reward is more convenient. Sparse rewards are also more suitable for offline learning as they simplify the learning and analysis of the agent's behaviour. Non-sparse rewards suit online learning on the other hand, because they give a quick indication of the agent behaviour suitability and hence speed up the early population of the value function. </p>
<h2 id="2-the-expected-return-function-v">2. The Expected Return Function V</h2>
<p>Once we move form an actul return that comes froma an actual experience at time step <span class="arithmatex">\(t\)</span> to try to estimate this return, we move to an expectaiton <em>function</em>. This function, traditionally called the value function v, is an important function. But now isntead of tying the value of the return to a particular experience at a step t which would be less useful in generalising the lessons an agent can learn from interacting with the environment, it makes more sense to ty this up to a certain state <span class="arithmatex">\(s\)</span>. This will allow the agent to learn a useful expectation of the return(discounted sum of rewards) for a particualr state when the agent follows a policy <span class="arithmatex">\(\pi\)</span>. I.e. we are now saying that a we will get an expected value of the return for a particular state under a policy <span class="arithmatex">\(\pi\)</span>. So we moved from subscripting by a step <span class="arithmatex">\(t\)</span> into passing a state <span class="arithmatex">\(s\)</span> to the function and subscripting by a policy.</p>
<div class="arithmatex">\[
\begin{equation}
    v_{\pi}(s) = \mathbb{E}_{\pi}(G_t)   \label{eq:v}  %\tag{1}
\end{equation}
\]</div>
<p>Equation <span class="arithmatex">\(\eqref{eq:v}\)</span> gives the definition of v function.</p>
<p>In the following video we tackle this idea in more details.</p>
<p><iframe src="https://leeds365-my.sharepoint.com/personal/scsaalt_leeds_ac_uk/_layouts/15/embed.aspx?UniqueId=7b8178ed-68d1-4335-8ab7-3d81f214f362&embed=%7B%22ust%22%3Atrue%2C%22hv%22%3A%22CopyEmbedCode%22%7D&referrer=StreamWebApp&referrerScenario=EmbedDialog.Create" width="475" height="200"frameborder="0" scrolling="no" allowfullscreen title="4. Returns Expectation and Sampling.mkv"></iframe></p>
<h2 id="2-the-bellman-equations">2. The Bellman Equations</h2>
<p>The <strong>Bellman equations</strong> provide recursive relationships between the value of a state (or state-action pair) and the values of neighboring states. These equations are fundamental in solving MDPs and are the basis for many reinforcement learning algorithms.</p>
<h3 id="21-bellman-equation-for-the-value-function">2.1 Bellman Equation for the Value Function</h3>
<p>The <strong>value function</strong> <span class="arithmatex">\( V_{\pi}(s) \)</span> represents the expected return starting from state <span class="arithmatex">\( s \)</span> and following policy <span class="arithmatex">\( \pi \)</span>. The Bellman equation for <span class="arithmatex">\( V_{\pi}(s) \)</span> is:</p>
<div class="arithmatex">\[
V_{\pi}(s) = \mathbb{E}_{\pi}\left[ R(s, a, s') + \gamma \sum_{s'} P(s' | s, a) V_{\pi}(s') \right]
\]</div>
<p>Where:
- <span class="arithmatex">\( V_{\pi}(s) \)</span> is the value of state <span class="arithmatex">\( s \)</span> under policy <span class="arithmatex">\( \pi \)</span>,
- <span class="arithmatex">\( R(s, a, s') \)</span> is the immediate reward for transitioning from <span class="arithmatex">\( s \)</span> to <span class="arithmatex">\( s' \)</span> after action <span class="arithmatex">\( a \)</span>,
- <span class="arithmatex">\( \gamma \)</span> is the discount factor, and
- <span class="arithmatex">\( P(s' | s, a) \)</span> is the transition probability.</p>
<h3 id="22-bellman-equation-for-the-q-function">2.2 Bellman Equation for the Q-Function</h3>
<p>The <strong>Q-function</strong> <span class="arithmatex">\( Q_{\pi}(s, a) \)</span> represents the expected return after taking action <span class="arithmatex">\( a \)</span> in state <span class="arithmatex">\( s \)</span> and then following policy <span class="arithmatex">\( \pi \)</span>. The Bellman equation for <span class="arithmatex">\( Q_{\pi}(s, a) \)</span> is:</p>
<div class="arithmatex">\[
Q_{\pi}(s, a) = \mathbb{E}\left[ R(s, a, s') + \gamma \sum_{s'} P(s' | s, a) V_{\pi}(s') \right]
\]</div>
<p>Where:
- <span class="arithmatex">\( Q_{\pi}(s, a) \)</span> is the action-value function,
- The terms <span class="arithmatex">\( R(s, a, s') \)</span>, <span class="arithmatex">\( \gamma \)</span>, and <span class="arithmatex">\( P(s' | s, a) \)</span> are the same as in the value function equation.</p>
<iframe src="https://leeds365-my.sharepoint.com/personal/scsaalt_leeds_ac_uk/_layouts/15/embed.aspx?UniqueId=6d6d9455-7174-447a-8bcb-eceaa51a4af5&embed=%7B%22ust%22%3Atrue%2C%22hv%22%3A%22CopyEmbedCode%22%7D&referrer=StreamWebApp&referrerScenario=EmbedDialog.Create" width="470" height="200" frameborder="0" scrolling="no" allowfullscreen title="5. Bellman v.mkv"></iframe>

<iframe src="https://leeds365-my.sharepoint.com/personal/scsaalt_leeds_ac_uk/_layouts/15/embed.aspx?UniqueId=3a18cbb0-6960-42c1-bcf4-8e0893c09c89&embed=%7B%22ust%22%3Atrue%2C%22hv%22%3A%22CopyEmbedCode%22%7D&referrer=StreamWebApp&referrerScenario=EmbedDialog.Create" width="470" height="200"frameborder="0" scrolling="no" allowfullscreen title="6. Bellman q simple.mkv"></iframe>

<h3 id="23-bellman-optimality-equations">2.3 Bellman Optimality Equations</h3>
<p>The <strong>Bellman optimality equations</strong> describe the relationship between the optimal value function <span class="arithmatex">\( V^*(s) \)</span> or the optimal Q-function <span class="arithmatex">\( Q^*(s, a) \)</span> and the transition and reward dynamics. These equations are used to compute the optimal policy that maximizes the expected return.</p>
<h4 id="bellman-optimality-equation-for-the-value-function">Bellman Optimality Equation for the Value Function:</h4>
<div class="arithmatex">\[
V^*(s) = \max_a \mathbb{E}\left[ R(s, a, s') + \gamma \sum_{s'} P(s' | s, a) V^*(s') \right]
\]</div>
<h4 id="bellman-optimality-equation-for-the-q-function">Bellman Optimality Equation for the Q-Function:</h4>
<div class="arithmatex">\[
Q^*(s, a) = \mathbb{E}\left[ R(s, a, s') + \gamma \sum_{s'} P(s' | s, a) \max_{a'} Q^*(s', a') \right]
\]</div>
<p>Where:
- <span class="arithmatex">\( V^*(s) \)</span> is the optimal value function,
- <span class="arithmatex">\( Q^*(s, a) \)</span> is the optimal Q-function,
- The <strong>max</strong> operator ensures that the agent chooses the action <span class="arithmatex">\( a \)</span> that maximizes the expected return.</p>
<iframe src="https://leeds365-my.sharepoint.com/personal/scsaalt_leeds_ac_uk/_layouts/15/embed.aspx?UniqueId=b73eab99-7af2-4b9e-8909-19492615d273&embed=%7B%22ust%22%3Atrue%2C%22hv%22%3A%22CopyEmbedCode%22%7D&referrer=StreamWebApp&referrerScenario=EmbedDialog.Create" width="470" height="200" frameborder="0" scrolling="no" allowfullscreen title="7. Bellman Optimality 1.mkv"></iframe>

<iframe src="https://leeds365-my.sharepoint.com/personal/scsaalt_leeds_ac_uk/_layouts/15/embed.aspx?UniqueId=8d89893b-6e99-4380-a31e-93e2974cd04a&embed=%7B%22ust%22%3Atrue%2C%22hv%22%3A%22CopyEmbedCode%22%7D&referrer=StreamWebApp&referrerScenario=EmbedDialog.Create" width="470" height="200" frameborder="0" scrolling="no" allowfullscreen title="7. Bellman Optimality 2.mkv"></iframe>

<p>You can adjust the video settings in SharePoint (speed up to 1.2 and reduce the noise if necessary)</p>
<p><em>Exercise 1</em>: If you realise there is a missing symbol in the [video: Bellman Equation for v] last equations, do you know what it is and where it has originally come from?</p>
<p><em>Exercise 2</em>: Can you derive Bellman Optimality Equation for <span class="arithmatex">\(q(s,a)\)</span> from first principles?</p>
<p><a href="https://leeds365-my.sharepoint.com/:v:/g/personal/scsaalt_leeds_ac_uk/EVBv-P5S4_VKqFt_E0vikIUBdpV1BZX2V-IDM3ROXDDV4A?e=YQQchV">video:  Bellman Optimality for q from first principles</a></p>
<h3 id="summary">Summary</h3>
<p>The <strong>Markov Decision Process (MDP)</strong> framework models decision-making problems where an agent interacts with an environment. It includes <strong>transition dynamics</strong> <span class="arithmatex">\( P(s' | s, a) \)</span> and <strong>reward dynamics</strong> <span class="arithmatex">\( R(s, a, s') \)</span>, which describe the behavior of the environment. The <strong>Bellman equations</strong> provide recursive relationships for computing the value of states or actions, while the <strong>Bellman optimality equations</strong> help find the optimal policy. Properties like the <strong>Markov Property</strong>, <strong>stationarity</strong>, and the stochastic nature of the dynamics are key factors in MDPs. Understanding these dynamics and equations is central to reinforcement learning algorithms designed to find optimal decision-making strategies.</p>
<p><strong>Further Reading</strong>:
For further info refer to chapter 3 of the Sutton and Barto <a href="http://incompleteideas.net/book/RLbook2020.pdf">book</a>. </p>
<h2 id="your-turn">Your turn</h2>
<p>Go ahead and play around with some grid world environment by executing and experiementing with the code in the following worksheet.</p>
<!-- <a href="Grid.py" download> Grid world library</a> -->

<p><a href="../../workseets/worksheet3.ipynb">worksheet3</a></p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright Abdulrahman Altahhan
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.sections"], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.f1b6f286.min.js"></script>
      
        <script src="../../javascript/tablecontentsoverride.js"></script>
      
        <script src="../../javascript/config.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>