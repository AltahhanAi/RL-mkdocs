
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
        <meta name="author" content="Dr Abdulrahman Altahhan">
      
      
      
        <link rel="prev" href="../../unit2/lesson7/lesson7.html">
      
      
        <link rel="next" href="../lesson9/lesson9.html">
      
      
      <link rel="icon" href="../../img/favicon.jpg">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.4">
    
    
      
        <title>8. Temporal Difference - Reinforcement Learning and Robotics</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.8608ea7d.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../css/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#introduction-to-bootstrapping" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
        <aside class="md-banner">
          <div class="md-banner__inner md-grid md-typeset">
            
            
    <div id="versionIndicator"><b>Version:</b> 04.06.21.a</div>
    <img id="customlogo" src="../../img/logo.svg" alt="University of Leeds logo.">

          </div>
          
        </aside>
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../index.html" title="Reinforcement Learning and Robotics" class="md-header__button md-logo" aria-label="Reinforcement Learning and Robotics" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Reinforcement Learning and Robotics
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              8. Temporal Difference
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../index.html" class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit1/lesson1/lesson1.html" class="md-tabs__link">
          
  
  Unit 1

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit2/lesson5/lesson5.html" class="md-tabs__link">
          
  
  Unit 2

        </a>
      </li>
    
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="lesson8.html" class="md-tabs__link">
          
  
  Unit 3

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit4/lesson12/lesson12.html" class="md-tabs__link">
          
  
  Unit 4

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit5/lesson15/lesson15.html" class="md-tabs__link">
          
  
  Unit 5

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit6/lesson18/lesson18.html" class="md-tabs__link">
          
  
  Unit 6

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../index.html" title="Reinforcement Learning and Robotics" class="md-nav__button md-logo" aria-label="Reinforcement Learning and Robotics" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Reinforcement Learning and Robotics
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../index.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 1
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Unit 1
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit1/lesson1/lesson1.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1. Tabular Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit1/lesson2/lesson2.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2. K-Arm Bandit
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit1/lesson3/lesson3.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    3. MDP
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit1/lesson4/lesson4.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    4. ROS
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 2
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Unit 2
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit2/lesson5/lesson5.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    5. Dynamic Programming
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit2/lesson6/lesson6.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    6. Monte Carlo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit2/lesson7/lesson7.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    7. Mobile Robots
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Unit 3
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Unit 3
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
      <a href="lesson8.html" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    8. Temporal Difference
    
  </span>
  

      </a>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lesson9/lesson9.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    9. n-Step Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lesson10/lesson10.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    10. Planning in RL(optional)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lesson11/lesson11.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    11. Localisation and SLAM
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 4
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Unit 4
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit4/lesson12/lesson12.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    12. Function Approximation Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit4/lesson13/lesson13.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    13. Linear Approximation for Prediction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit4/lesson14/lesson14.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    14. Linear Approximation for Control
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 5
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Unit 5
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit5/lesson15/lesson15.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    15. Linear Approximation with Eligibility Traces(prediction and control)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit5/lesson16/lesson16.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    16. Nonlinear Approximation for Control
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit5/lesson17/lesson17.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    17. Application on Robot Navigation
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 6
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Unit 6
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit6/lesson18/lesson18.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    18. Application on Games(optional)
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="introduction-to-bootstrapping">Introduction to Bootstrapping</h1>
<p>In this and subsequent units, we cover a set of RL algorithms that use bootstrapping, a powerful idea that allows us to create online updates that do not wait until the end of an episode to learn from the experience, live as it comes. We will continue on the tabular method, cover planning, and then move to function approximation methods. Along the way, we cover encoding techniques for state space traditionally used in RL, such as tile coding. On the function approximation, we will assume a linear model in this unit. We cover non-linear models from an application perspective in the subsequent unit. We are mainly concerned with regression not classification from a machine learning perceptive.</p>
<p>The settings are still the same as that of an MDP. However, we assume that the state space is large and may not be practical to represent each state as an entry in a table. The states might also not manifest themselves clearly, and only we can obtain some observations about them. These observations result in a set of numerical, categorical or boolean features which we can then numerically deal with them as we did in earlier modules.</p>
<p><strong>Unit 3: Learning Outcomes</strong><br />
By the end of this unit, you will be able to:  </p>
<ol>
<li><strong>Assess</strong> the role of bootstrapping in RL and its impact on learning efficiency.  </li>
<li><strong>Explain</strong> n-step methods and the trade-offs associated with different values of n.  </li>
<li><strong>Compare</strong> n-step backup action-value-based control methods with direct policy estimation methods.  </li>
<li><strong>Evaluate</strong> how Temporal Difference (TD) methods obtain biased but low-variance estimates through environment interaction.  </li>
<li><strong>Analyze</strong> how actor-critic methods achieve biased but low-variance estimation through interaction with the environment.  </li>
<li><strong>Discuss</strong> the trade-offs between online and offline RL algorithms.  </li>
<li><strong>Design</strong> planning methods that incorporate model learning into RL.  </li>
</ol>
<hr />
<h1 id="lesson-7-tabular-methods-temporal-difference-learning">Lesson 7-Tabular Methods: Temporal Difference Learning</h1>
<p><strong>Learning outcomes</strong></p>
<ol>
<li>understand the idea of bootstrapping and how it is being used in TD</li>
<li>understand the differences between MC and TD and appreciate their strengths and weaknesses</li>
<li>understand how to use the ideas of TD to extend it to a control method such as Sarsa and Q-learning</li>
</ol>
<p>In this lesson, we cover the Temporal Difference learning method. TD is one of the fundamental ideas in RL. It uses bootstrapping to improve its predictions. The idea behind bootstrapping is to use (own estimation) to improve (own estimation) with an indication from the ground truth in the form of a reward. This sound surprising since we are not using a direct ground truth to revert to when we are improving the prediction. However, it turns out that there are theoretical guarantees that the method will converge to a solution that is usually <em>close to optimal</em>. The one constant stream of ground truth the agent keeps receiving is the rewards in each state. One of the major strengths of TD is that it can be used online without having to wait till the end of the episode as we did in the Monte Carlo methods. This also makes it extremely efficient and allows it to converge faster <em>in practice *than MC. TD uses ideas similar to what we did in GPI: slightly improving the prediction and *not</em> waiting until everything is clear (at the end of an episode). This idea is similar to what we did in stochastic mini-batch updates in ML. We will call it eagerness to learn. I.e., to grab whatever information is available and whenever it becomes available but at the same time keep accumulating a stock of this information to help us improve and sharpen our prediction. We will then move into designing control algorithms that depend on TD, we will tackle old and new algorithms, including Sarsa, Expected Sarsa, Q-learning and double Q-learning, and we will test them extensively using the infrastructure that we developed in the previous lesson. Finally, we conclude by studying a policy gradient algorithm for control, namely actor-critic, that depends on TD and REINFORCE.</p>
<p><strong>Plan</strong>
As usual, in general there are two types of RL problems that we will attempt to design methods to deal with 
1. Prediction problem
For These problems we will design Policy Evaluation Methods that attempt to find the best estimate for the value-function given a policy.</p>
<ol>
<li>Control problems 
For These problems we will design Value Iteration methods which utilise the idea of Generalised Policy Iteration. They attempt to find the best policy, via estimating an action-value function for a current policy then moving to a better and improved policy by choosing a greedy action often. We will then move to Policy Gradient methods that directly estimate a useful policy for the agent by maximising its value function.</li>
</ol>
<p>Ok, so we start by implementing the TD algorithm. Due to the way we structured our code and classes, it is relatively simple and straightforward to define any online and offline methods. TD is an online method that will be called in <em>each step during an episode</em>. We, therefore, can turn off the storage because we do not need it, but leaving it will not hurt the grid problems we are tackling. It will consume some memory and a few extra milliseconds of processing. For more difficult problems, we need to utilise the memory to train anyway, as we shall see in the Application unit.</p>
<p>We also would need to pass a learning step as we did for the MC algorithm. A learning step dictates how much error percentage will be considered when we update the value function. Sometimes we could go all the way α=1 when the algorithm is tabular, and the problem is simple. For most of the problems and algorithms we tackle, however, this is not desirable, and we set α=.1 or less to ensure the algorithm performs well on the common states and is acceptable on less common states. MC, however, is particularly sensitive towards this α, and we often would need to set it to smaller values such as .01.</p>
<h2 id="temporal-difference-td-learning-prediction">Temporal-Difference (TD) Learning (prediction)</h2>
<p>Earlier, in Lesson 6, we saw that the constant-<span class="arithmatex">\(\alpha\)</span> Monte Carlo (MC) prediction method has an update rule of the form:</p>
<div class="arithmatex">\[
    V(S_t) \leftarrow V(S_t) + \alpha \left( G(S_t) - V(S_t) \right)
\]</div>
<p>The core idea behind several well-known incremental reinforcement learning (RL) algorithms is to replace <span class="arithmatex">\( G_t \)</span> with an estimate.  </p>
<p>Our goal in Temporal-Difference (TD) learning is to combine the advantages of Monte Carlo (MC) methods and Dynamic Programming (DP). Specifically, we want the model-free nature of MC, which does not require a model of the environment and learns directly from experience and interaction (unlike DP). The above update rule already satisfies this requirement.  </p>
<p>At the same time, we seek the fast convergence of DP while avoiding its high computational cost. Using <span class="arithmatex">\( R_{t+1} + \gamma V(S_{t+1}) \)</span> in place of <span class="arithmatex">\( G_t \)</span> satisfies this requirement—let's unpack this further.  </p>
<p>If we recall how DP, particularly policy evaluation, updates its estimates, we see that it relies on the Bellman equation:</p>
<div class="arithmatex">\[
V^\pi(s) = \mathbb{E}_\pi \left[ R_{t+1} + \gamma V^\pi(S_{t+1}) \mid S_t = s \right]
\]</div>
<p>TD learning leverages the Bellman equation and assumes that since the agent interacts with the environment according to its current policy, the experience is drawn according to the policy distribution <span class="arithmatex">\( \pi \)</span>. This means that the expectation <span class="arithmatex">\( \mathbb{E}_\pi \)</span> in the Bellman equation is naturally satisfied if we use this experience from <span class="arithmatex">\( \pi \)</span> to update the estimate of the value function <span class="arithmatex">\( V^\pi(s) \)</span>. Thus, the learned value function should follow and satisfy the Bellman equation. If it does not, it should be adjusted accordingly.  </p>
<p>(Recall that any expectation can be approximated by sampling and averaging from the underlying distribution, as per the law of large numbers. Instead of averaging, we use a step-size parameter <span class="arithmatex">\( \alpha &lt; 1 \)</span> to continuously adjust the estimates, similar to the constant-<span class="arithmatex">\(\alpha\)</span> MC method.)  </p>
<p>According to the Bellman equation, <span class="arithmatex">\( R_{t+1} + \gamma V(S_{t+1}) \)</span> serves as a good candidate estimate for <span class="arithmatex">\( G_t \)</span>, leveraging bootstrapping in the same way as DP. In other words, the state-value function <span class="arithmatex">\( V(S_t) \)</span> should remain close to <span class="arithmatex">\( R_{t+1} + \gamma V(S_{t+1}) \)</span>. If it deviates, it should be adjusted in that direction.  </p>
<p>Thus, TD replaces <span class="arithmatex">\( G_t \)</span> with <span class="arithmatex">\( R_{t+1} + \gamma V(S_{t+1}) \)</span> in the constant-<span class="arithmatex">\(\alpha\)</span> MC update rule:</p>
<div class="arithmatex">\[
    V(S_t) \leftarrow V(S_t) + \alpha \left(R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \right)
\]</div>
<p>This formulation allows TD learning to balance between MC and DP, providing a practical and efficient approach to value estimation in reinforcement learning. It adheres to the incremental nature of the constant-<span class="arithmatex">\(\alpha\)</span> MC method while also enabling an <em>online</em> learning approach rather than the <em>end-of-episode</em> learning used in MC. At the same time, it allows for better and faster convergence, similar to DP.  </p>
<p><strong>Key Components:</strong></p>
<ul>
<li><span class="arithmatex">\( \alpha \)</span> is the step-size (learning rate),</li>
<li><span class="arithmatex">\( R_{t+1} \)</span> is the reward received after taking action in <span class="arithmatex">\( S_t \)</span>,</li>
<li><span class="arithmatex">\( \gamma \)</span> is the discount factor,</li>
<li><span class="arithmatex">\( V(S_{t+1}) \)</span> is the estimate of the next state's value.</li>
</ul>
<p>Unlike Monte Carlo (MC) methods, which require complete episodes (including constant-<span class="arithmatex">\(\alpha\)</span> MC), TD methods update values truly <em>incrementally</em> after each time step, making them more efficient for continuous or long-horizon problems. The replacement of <span class="arithmatex">\( G_t \)</span> with an estimate introduces bias into TD (due to bootstrapping), but it also reduces variance.  </p>
<p>In machine learning, the <strong>bias-variance trade-off</strong> is a common phenomenon, and in RL, we generally prefer lower variance since it stabilizes learning and allows for a smoother experience. TD tends to be faster and more sample-efficient than MC, making the bias-variance trade-off well worth it. Additionally, its ability to <em>incrementally</em> update the estimate <em>as rewards are collected</em> makes it a powerful tool in reinforcement learning.  </p>
<p>Below we show the pseudocode for the TD algorithm.</p>
<p><span class="arithmatex">\(
\begin{array}{ll}
\textbf{Algorithm: } \text{TD(0) Prediction} \\
\textbf{Input: } \text{Policy } \pi, \text{ step-size } \alpha, \text{ discount factor } \gamma \\
\textbf{Initialize: }  V(s) \leftarrow 0, \forall s \in S \\
\textbf{Loop for each episode:} \\
\quad \text{Initialize } s \\
\quad \textbf{Loop for each step } t \textbf{ until episode ends:} \\
\quad \quad \text{Take action } a \sim \pi(s), \text{ observe } r, s' \\
\quad \quad V(s) \leftarrow V(s) + \alpha \left( r + \gamma V(s') - V(s) \right) \\
\quad \quad s \leftarrow s' \\
\textbf{Return: } V(s), \forall s \in S \\
\end{array}
\)</span></p>
<hr />
<h3 id="td-vs-mc-summary">TD vs. MC Summary</h3>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Temporal-Difference (TD)</th>
<th>Monte Carlo (MC)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Update</strong></td>
<td>After each time step</td>
<td>After full episode</td>
</tr>
<tr>
<td><strong>Exploration Requirement</strong></td>
<td>Can learn from incomplete episodes</td>
<td>Requires complete episodes</td>
</tr>
<tr>
<td><strong>Variance</strong></td>
<td>Lower variance due to bootstrapping</td>
<td>Higher variance since full returns are used</td>
</tr>
<tr>
<td><strong>Bias</strong></td>
<td>More biased as it relies on current estimates</td>
<td>Less biased since it uses true returns</td>
</tr>
<tr>
<td><strong>Sample Efficiency</strong></td>
<td>More efficient, updates per time step</td>
<td>Less efficient, updates once per episode</td>
</tr>
<tr>
<td><strong>Suitability</strong></td>
<td>Better for continuous/long tasks and even episodic</td>
<td>Works well for episodic tasks</td>
</tr>
</tbody>
</table>
<p>TD methods blend <strong>bootstrapping (like Dynamic Programming)</strong> and <strong>sampling (like MC)</strong>, making them a flexible and powerful approach for reinforcement learning.</p>
<h3 id="td-implementation">TD Implementation</h3>
<p>Below we provide you with the Python code to implement this algorithm.</p>
<p><div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">TD</span><span class="p">(</span><span class="n">MRP</span><span class="p">):</span>
    <span class="c1"># ----------------------------- 🌖 online learning ----------------------    </span>
    <span class="k">def</span><span class="w"> </span><span class="nf">online</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">rn</span><span class="p">,</span><span class="n">sn</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">α</span><span class="o">*</span><span class="p">(</span><span class="n">rn</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span> <span class="n">done</span><span class="p">)</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">γ</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">[</span><span class="n">sn</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">])</span>
</code></pre></div>
That's it, this is all what you need to implement TD!. Belwo we explain the different parts of the code so that you become familiar with it.</p>
<h4 id="close-to-the-update-rule">Close to the Update Rule:</h4>
<p>The code is designed to directly follow the TD update rule for value estimation:</p>
<div class="arithmatex">\[
V(S_t) \leftarrow V(S_t) + \alpha \left( R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \right)
\]</div>
<ul>
<li><strong><span class="arithmatex">\( r_n \)</span></strong> represents the immediate reward <span class="arithmatex">\( R_{t+1} \)</span>,</li>
<li><strong><span class="arithmatex">\( s_n \)</span></strong> is the next state <span class="arithmatex">\( S_{t+1} \)</span>,</li>
<li><strong><span class="arithmatex">\( done \)</span></strong> is a flag indicating whether the episode has ended or not.</li>
</ul>
<h4 id="handling-terminal-states">Handling Terminal States:</h4>
<p>Notice that <span class="arithmatex">\( V[s_{t+1}] \)</span> is multiplied by (1 - done).</p>
<p>This ensures that when the episode ends (i.e., when the agent reaches the goal or completes the task), the value of <span class="arithmatex">\( V[s_{t+1}] \)</span> is not used in the update. Instead, only the final reward <span class="arithmatex">\( r_{t+1} \)</span> will contribute to the update. This is crucial for handling terminal states, and it saves you from having to handle terminal states separately in the environment. Since in deed, the value of  penultimate terminal state, must equate the rewards for ending in a terminal state and no need to have <span class="arithmatex">\( V[s_{t+1}] \)</span> as it = 0 by definition.</p>
<ul>
<li>Without this multiplication, you might need to treat goal states uniquely (for example, by setting <span class="arithmatex">\( V[s_{t+1}] = 0 \)</span> when <span class="arithmatex">\( s_{t+1} \)</span> is the goal, or by checking the <strong>done</strong> flag in the environment).</li>
<li>By applying this multiplication, we explicitly ensure the correct behavior in terminal states, which is preferred for clarity and maintainability.</li>
</ul>
<h4 id="no-use-of-a-and-an">No Use of "a" and "an":</h4>
<p>The function <code>online()</code> does not use the arguments <strong>"a"</strong> and <strong>"an"</strong> because TD is focused on <strong>value prediction</strong>, not action selection. At this stage, we are estimating the value of states under a policy, without involving actions or control. Actions come into play once we move from prediction to control (which is the next step in RL).</p>
<h4 id="online-learning">Online Learning:</h4>
<ul>
<li>TD learning is an online algorithm, meaning it updates the value estimate after each time step, therefore its update must be implemented by <em>overriding</em> this function from the MRP class.</li>
<li>Unlike Monte Carlo (MC) methods and other offline methods, which require storing experiences for full episodes before updating, TD updates its estimate after each step, making it more <em>memory-efficient</em>. This is particularly useful in problems where episodes may be long or continuous. 
  <!-- Note that we did not need to store the episodes trajectories in a pure online method, hence these methods are usually more memory efficient that there offline counterpart! -->
</li>
</ul>
<h4 id="testing-on-the-random-walk-problem">Testing on the Random Walk Problem:</h4>
<p>The <em>random walk prediction problem</em> (often used as a simple example in reinforcement learning) is the default environment for the <em>Markov Reward Process (MRP)</em>, so there's no need to pass it explicitly when testing the algorithm.</p>
<p>This makes the TD algorithm particularly suited for <em>real-time learning</em> in environments where episodes may not end quickly or where you want to avoid waiting for an entire episode to complete before updating your values (although the reward nature plays also a role in that, end-of-episode reward or intermediate reward).</p>
<p><div class="highlight"><pre><span></span><code><span class="n">TDwalk</span> <span class="o">=</span> <span class="n">TD</span><span class="p">(</span><span class="n">episodes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">v0</span><span class="o">=</span><span class="mf">.5</span><span class="p">,</span> <span class="o">**</span><span class="n">demoV</span><span class="p">())</span>
<span class="n">TDwalk</span><span class="o">.</span><span class="n">interact</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;TD learning&#39;</span><span class="p">)</span>
</code></pre></div>
<img alt="png" src="output_13_1.png" /></p>
<p>Note how TD performed far better and converged faster in fewer episodes than MC</p>
<h3 id="offline-td">Offline TD</h3>
<p>In this section, we develop an offline TD algorithm. This is not a common algorithm as it usually defies the reason for using TD. That is, we usually use TD because it is an online algorithm. Nevertheless, studying this algorithm allows us to appreciate the strengths and weaknesses of TD and to compare its performance with other offline algorithms, such as MC.</p>
<p><span class="arithmatex">\(
\begin{array}{ll}
\textbf{Algorithm: }  \text{Offline Temporal-Difference Policy Evaluation} \\
\textbf{Input: } \text{Episodes generated under policy } \pi \\
\textbf{Initialize: } V(S) \leftarrow 0, \forall S \in \mathcal{S}, \alpha &gt; 0 \\
\textbf{Repeat until convergence: } &amp; \\
\quad \text{For each episode: } &amp; \\
\quad \quad \textbf{For each step } t \textbf{ from } 0 \textbf{ to } T-1: &amp; \\
\quad \quad \quad \delta_t \leftarrow R_{t+1} + \gamma V(S_{t+1}) - V(S_t) &amp; \\
\quad \quad \quad \text{Store } (S_t, \delta_t) \text{ for batch update} &amp; \\
\quad \text{End episode loop} &amp; \\
\quad \textbf{For each state } S_t \textbf{ in batch:} &amp; \\
\quad \quad V(S_t) \leftarrow V(S_t) + \alpha \sum \delta_t \text{ (update using accumulated } \delta_t \text{)} &amp; \\
\textbf{Return: } V(S), \forall S \in \mathcal{S} \\
\end{array}
\)</span></p>
<p>Below we provide you with the Python implementation of the offline TD.</p>
<p><div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">TDf</span><span class="p">(</span><span class="n">MRP</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">init</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">store</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="c1"># ----------------------------- 🌘 offline TD learning ----------------------------   </span>
    <span class="k">def</span><span class="w"> </span><span class="nf">offline</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">s</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
            <span class="n">sn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">s</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">rn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">done</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">done</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">α</span><span class="o">*</span><span class="p">(</span><span class="n">rn</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span> <span class="n">done</span><span class="p">)</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">γ</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">[</span><span class="n">sn</span><span class="p">]</span><span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">])</span>
</code></pre></div>
Note that we can do it the changes backwards, you can try both and see the difference.</p>
<p><div class="highlight"><pre><span></span><code><span class="n">TDwalk</span> <span class="o">=</span> <span class="n">TDf</span><span class="p">(</span><span class="n">α</span><span class="o">=</span><span class="mf">.05</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">v0</span><span class="o">=</span><span class="mf">.5</span><span class="p">,</span> <span class="o">**</span><span class="n">demoV</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;TD learning&#39;</span><span class="p">)</span>
</code></pre></div>
<img alt="png" src="output_20_0.png" /></p>
<!-- #### Overriding the Offline Function -->

<p>Note how we <em>overrode</em> the <em>offline</em> function in our <em>MRP</em> class that we covered in the previous lesson. </p>
<p>The first three lines inside the <code>for</code> loop are to make the update format of the online and offline methods identical.</p>
<p>We could have also made the algorithm go <em>backwards</em>, similar to MC. Each approach has its own advantages and disadvantages, although for TD learning, since it uses the temporal difference error, it usually makes little difference.</p>
<p>You can uncomment the backward loop and try it yourself.</p>
<h2 id="conducting-trialsseveral-runs-of-experiments">Conducting trials(several runs) of experiments</h2>
<p>Let us now use a useful handy class called 'Runs' that summarises several runs for us to reach a reliable and unbiased conclusions when we compare algorithms performances.</p>
<p>Note that the class allows us to run several experiments efficiently. The main assumption is that the algorithms are inherited from an MRP class which applies for the majority of the classes that we will deal with in our units.</p>
<p>Let us now see how we can use this new class to easily run experiments to study how an algorithm behaves. Below we show a function that compares TD with MC on different learning rates. You can read about this comparison and the associated figure in Example 6.2 of the book (hence the function's name). We will follow this trend of naming functions after their counterpart examples or figures in the book.</p>
<p><div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">TD_MC_randwalk</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">randwalk</span><span class="p">(),</span> <span class="n">alg1</span><span class="o">=</span><span class="n">TDf</span><span class="p">,</span> <span class="n">alg2</span><span class="o">=</span><span class="n">MC</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">.25</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Empirical RMS error, averaged over states&#39;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">α</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">.05</span><span class="p">,</span> <span class="mf">.1</span><span class="p">,</span> <span class="mf">.15</span><span class="p">]:</span>
        <span class="n">TDαs</span> <span class="o">=</span> <span class="n">Runs</span><span class="p">(</span><span class="n">algorithm</span><span class="o">=</span><span class="n">alg1</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">,</span> <span class="n">α</span><span class="o">=</span><span class="n">α</span><span class="p">,</span> <span class="n">v0</span><span class="o">=</span><span class="mf">.5</span><span class="p">),</span> <span class="n">runs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">plotE</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">interact</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;TD α= </span><span class="si">%.2f</span><span class="s1">&#39;</span><span class="o">%</span><span class="n">α</span><span class="p">,</span> <span class="n">frmt</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">α</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">.01</span><span class="p">,</span> <span class="mf">.02</span><span class="p">,</span> <span class="mf">.03</span><span class="p">,</span> <span class="mf">.04</span><span class="p">]:</span>
        <span class="n">MCs</span> <span class="o">=</span> <span class="n">Runs</span><span class="p">(</span><span class="n">algorithm</span><span class="o">=</span><span class="n">alg2</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">,</span> <span class="n">α</span><span class="o">=</span><span class="n">α</span><span class="p">,</span> <span class="n">v0</span><span class="o">=</span><span class="mf">.5</span><span class="p">),</span> <span class="n">runs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">plotE</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">interact</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;MC α= </span><span class="si">%.2f</span><span class="s1">&#39;</span><span class="o">%</span><span class="n">α</span><span class="p">,</span> <span class="n">frmt</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">example_6_2</span><span class="p">(</span><span class="o">**</span><span class="n">kw</span><span class="p">):</span> <span class="k">return</span> <span class="n">TD_MC_randwalk</span><span class="p">(</span><span class="o">**</span><span class="n">kw</span><span class="p">)</span>

<span class="n">example_6_2</span><span class="p">()</span>
</code></pre></div>
<img alt="png" src="output_27_1.png" /></p>
<p>We have already imported MC to compare its performance with our newly defined offline TD. Remember that MC is also offline algorithm.</p>
<h3 id="optimality-of-td-batch-td-vs-batch-mc">Optimality of TD: Batch TD vs. Batch MC</h3>
<p>In this section, we study the optimality of TD. We develop two algorithms, <strong>Batch TD</strong> and <strong>Batch MC</strong>. Both of these algorithms operate in a <strong>supervised learning fashion</strong>. We collect a set of episodes and then treat them as mini-batches. Afterward, we run a set of epochs that repeatedly present the so-far experience until the algorithm converges. </p>
<p>Inside each algorithm, we use both TD and MC updates, respectively to observe which value each algorithm converges to. By doing so, we have leveled up the strength of both algorithms. Both are offline and wait until the end of each episode to accommodate all past experiences after each episode. This allows us to focus on their performance purely in terms of <strong>convergence</strong>.</p>
<p>To achieve this, we inherit from the MRP_batch class, which allows us to conduct <em>batch TD learning</em>. The MRP_batch class makes all past experiences available, not just the default last episode experience. This provides us with the ability to leverage the entire history of experiences for each update, enabling us to perform more batch learning.</p>
<!-- ```python
class MRP_batch(MRP):
    def __init__(self, **kw):
        super().__init__(**kw)
        self.store = True # store the full experience
    # we will redfine the allocate to store the full experience instead of only latest episode
    def allocate(self): 
        self.r = np.zeros((self.max_t, self.episodes))
        self.s = np.ones ((self.max_t, self.episodes), dtype=np.uint32) *(self.env.nS+10)  
        self.a = np.zeros((self.max_t, self.episodes), dtype=np.uint32)  # actions and states are indices        
        self.done = np.zeros((self.max_t, self.episodes), dtype=bool) 
    def store_(self, s=None,a=None,rn=None,sn=None,an=None, done=None, t=0):
        # store one trajectory(sarsa) in the rigth episode buffer
        if s  is not None: self.s[t, self.ep] = s
        if a  is not None: self.a[t, self.ep] = a
        if rn is not None: self.r[t+1, self.ep] = rn
        if sn is not None: self.s[t+1, self.ep] = sn
        if an is not None: self.a[t+1, self.ep] = an
        if done is not None: self.done[t+1, self.ep] = done
    # returns the agent's trace from latest episode buffer
    def trace(self):
            return self.s[:self.t+1, self.ep]
``` -->

<p>Batch learning is usually not practical, but it is listed here to study the behavior of TD and gain insight into its target compared to MC. The key point is to demonstrate that TD, in practice, has a different goal than MC and is more efficient in converging to this target. As a result, TD typically reduces the error more effectively than MC does.</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">TD_batch</span><span class="p">(</span><span class="n">MRP_batch</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">α</span><span class="o">=</span><span class="mf">.001</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">α</span><span class="o">=</span><span class="n">α</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">)</span>
    <span class="c1"># ------------------------🌘 offline learning----------------------- </span>
    <span class="k">def</span><span class="w"> </span><span class="nf">offline</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># epochs</span>
        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">ΔV</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="o">*</span><span class="mi">0</span>
            <span class="c1"># each episode acts like a mini-batch in supervised learning</span>
            <span class="k">for</span> <span class="n">ep</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ep</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span> 
                <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Ts</span><span class="p">[</span><span class="n">ep</span><span class="p">]):</span>
                    <span class="n">s</span>  <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">s</span><span class="p">[</span><span class="n">t</span><span class="p">,</span> <span class="n">ep</span><span class="p">]</span>
                    <span class="n">sn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">s</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">ep</span><span class="p">]</span>
                    <span class="n">rn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">ep</span><span class="p">]</span>
                    <span class="n">done</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">done</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">ep</span><span class="p">]</span>

                    <span class="n">ΔV</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">+=</span> <span class="n">rn</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span> <span class="n">done</span><span class="p">)</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">γ</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">[</span><span class="n">sn</span><span class="p">]</span><span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">]</span>
            <span class="n">ΔV</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">α</span>
            <span class="c1"># exit the epochs loop if there is no more meaningful changes (method converged)</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">ΔV</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mf">1e-3</span><span class="p">:</span>  <span class="k">break</span> <span class="c1">#; print(&#39;exit&#39;)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">V</span> <span class="o">+=</span> <span class="n">ΔV</span>
</code></pre></div>
<p><div class="highlight"><pre><span></span><code><span class="n">TDwalk_batch</span> <span class="o">=</span> <span class="n">TD_batch</span><span class="p">(</span><span class="n">episodes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">v0</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">demoV</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<img alt="png" src="output_33_0.png" /></p>
<p>Note how the batch updates have much smoother and faster convergence per-episodes than a usual TD or MC. However, they have a much higher computational cost that makes them not suitable for practical problem.</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">MC_batch</span><span class="p">(</span><span class="n">MRP_batch</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">α</span><span class="o">=</span><span class="mf">.001</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">α</span><span class="o">=</span><span class="n">α</span><span class="p">,</span><span class="o">**</span><span class="n">kw</span><span class="p">)</span>

    <span class="c1"># -----------------------------------🌘 offline learning------------------------------------- </span>
    <span class="k">def</span><span class="w"> </span><span class="nf">offline</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># epochs</span>
        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">ΔV</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="o">*</span><span class="mi">0</span>
            <span class="c1"># each episode acts like a mini-batch in supervised learning</span>
            <span class="k">for</span> <span class="n">ep</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ep</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
                <span class="n">Gt</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Ts</span><span class="p">[</span><span class="n">ep</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
                    <span class="n">s</span>  <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">s</span><span class="p">[</span><span class="n">t</span><span class="p">,</span> <span class="n">ep</span><span class="p">]</span>
                    <span class="n">rn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">ep</span><span class="p">]</span>

                    <span class="n">Gt</span> <span class="o">=</span> <span class="n">rn</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">γ</span><span class="o">*</span><span class="n">Gt</span> 
                    <span class="n">ΔV</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">+=</span> <span class="n">Gt</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">]</span>

            <span class="n">ΔV</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">α</span>
            <span class="c1"># exit the epochs loop if there is no more meaningful changes (method converged)</span>
            <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">ΔV</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mf">1e-3</span><span class="p">:</span> <span class="k">break</span> <span class="c1">#;print(&#39;exit&#39;)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">V</span> <span class="o">+=</span> <span class="n">ΔV</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">MCwalk_batch</span> <span class="o">=</span> <span class="n">MC_batch</span><span class="p">(</span><span class="n">episodes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">v0</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">demoV</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_36_0.png" /></p>
<h3 id="batch-runs">Batch runs</h3>
<p>Now it is time to run experiments to specify which algorithm is better. We follow the experiments conducted in Figure 6.2 in the book. </p>
<p>Note that we initialize the value function to -1 this time to smoothen the resultant figure and remove any advantages the algorithms had when starting from 0.5 probabilities. This means that the algorithm would have to guess all the way from -1 to the probability of starting in a state <span class="arithmatex">\( s \)</span> and ending up in the right terminal state.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">figure_6_2</span><span class="p">():</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;right&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;top&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">.25</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Batch Training&#39;</span><span class="p">)</span>

    <span class="n">α</span><span class="o">=</span><span class="mf">.001</span>
    <span class="n">TDB</span> <span class="o">=</span> <span class="n">Runs</span><span class="p">(</span><span class="n">algorithm</span><span class="o">=</span><span class="n">TD_batch</span><span class="p">(</span><span class="n">v0</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">α</span><span class="o">=</span><span class="n">α</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">100</span><span class="p">),</span> <span class="n">runs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">plotE</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">interact</span><span class="p">(</span><span class="n">label</span><span class="o">=</span> <span class="s1">&#39;Batch TD, α= </span><span class="si">%.3f</span><span class="s1">&#39;</span><span class="o">%</span><span class="n">α</span><span class="p">)</span>
    <span class="n">MCB</span> <span class="o">=</span> <span class="n">Runs</span><span class="p">(</span><span class="n">algorithm</span><span class="o">=</span><span class="n">MC_batch</span><span class="p">(</span><span class="n">v0</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">α</span><span class="o">=</span><span class="n">α</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">100</span><span class="p">),</span> <span class="n">runs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">plotE</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">interact</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Batch MC, α= </span><span class="si">%.3f</span><span class="s1">&#39;</span><span class="o">%</span><span class="n">α</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">figure_6_2</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_40_1.png" /></p>
<p>The above figure clearly shows that <strong>TD</strong> converges faster than <strong>MC</strong>, primarily regardless of the randomness arising from experience variations. This demonstrates (though not proves) that TD meets our goal of combining the benefits of DP and MC. It converges more quickly, is more sample-efficient than MC, and is computationally more efficient than DP</p>
<h2 id="sarsa-and-q-learning-td-on-control">Sarsa and Q-learning -TD on Control</h2>
<p>In this section, we focus on TD updates to achieve control. We primarily cover two key algorithms:</p>
<ol>
<li><strong>Sarsa</strong>: An <strong>on-policy</strong> control algorithm, meaning the agent learns and follows the same policy.</li>
<li><strong>Q-learning</strong>: An <strong>off-policy</strong> control algorithm, where the agent follows an ε-greedy policy but learns about the optimal greedy policy.</li>
</ol>
<!-- ## From TD to Sarsa and Q-Learning -->

<p>The primary shift from standard TD learning to Sarsa and Q-learning is moving from value prediction to policy learning (improving Q). TD learning estimates the value of states or state-action pairs but does not explicitly account for the policy an agent follows. In contrast, reinforcement learning control methods like Sarsa and Q-learning focus on improving both the value estimates and the policy itself.</p>
<p>Assuming the agent follows an <strong>ε-greedy policy</strong> that depends on action-values (Q-values), we can extend TD ideas to control. Both Sarsa and Q-learning use an <strong>incremental update rule</strong>, but instead of updating the state-value function <span class="arithmatex">\(V\)</span>, we update the action-value function <span class="arithmatex">\(Q\)</span>. The general update rule is:</p>
<div class="arithmatex">\[
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left( G(S_t) - Q(S_t, A_t) \right)
\]</div>
<p>To estimate <span class="arithmatex">\(G_t\)</span>, we use a form of the Bellman equation. We can either:</p>
<ol>
<li>Use <span class="arithmatex">\(R_{t+1} + \gamma V(S_{t+1})\)</span>, requiring both <span class="arithmatex">\(Q\)</span> and <span class="arithmatex">\(V\)</span> tables in the resultant algorithm.</li>
<li>Use only the <span class="arithmatex">\(Q\)</span> table, which is the standard approach in Sarsa and Q-learning.</li>
</ol>
<h3 id="sarsa-vs-q-learning">Sarsa vs. Q-learning</h3>
<ul>
<li><strong>Sarsa (On-Policy)</strong>: Uses the Bellman equation and updates based on the next action <strong><span class="arithmatex">\(A_{t+1}\)</span></strong>:</li>
</ul>
<div class="arithmatex">\[
G(S_t) \approx R_{t+1} + \gamma Q(S_{t+1}, A_{t+1})
\]</div>
<p>Since <span class="arithmatex">\(A_{t+1}\)</span> is chosen by the current policy (e.g., ε-greedy), Sarsa updates based on actual agent behavior.</p>
<ul>
<li><strong>Q-learning (Off-Policy)</strong>: Uses the Bellman <strong>Optimality</strong> equation instead:</li>
</ul>
<div class="arithmatex">\[
G(S_t) \approx R_{t+1} + \gamma \max_a Q(S_{t+1}, a)
\]</div>
<p>This means Q-learning updates based on the best possible future action (regardless of whether it will be taken or not), rather than the one actually taken.</p>
<h3 id="how-does-sarsa-improve-the-policy">How Does Sarsa Improve the Policy?</h3>
<p>A key question is: <em>If Sarsa does not use the Bellman Optimality equation, how does it improve the policy?</em> </p>
<p>The answer lies in how <span class="arithmatex">\(A_{t+1}\)</span> is selected. Since actions are chosen based on an ε-greedy policy, <em>there is an implicit max operation in action selection</em>. That is, most of the time (<span class="arithmatex">\(1-\epsilon+\frac{\epsilon}{|\mathcal{A}|}\)</span>), the agent picks the action with the highest <span class="arithmatex">\(Q\)</span> value. This leads to <em>gradual policy improvement</em> while still allowing some exploration.</p>
<p>Because of this, Sarsa continuously refines the Q-values and improves the policy over time. The ε-greedy strategy ensures all actions are explored, preventing premature convergence to suboptimal solutions. Even after the Q-table stabilizes, the policy remains slightly suboptimal due to exploration. One can either decay ε over time or leave it small but nonzero to maintain some exploration.</p>
<!-- ### Policy Evaluation vs. Policy Improvement

- **TD methods** estimate state values under a fixed policy, focusing on prediction rather than optimization.
- **Sarsa** refines the action-value function \(Q(s, a)\) based on the **same policy it follows**, making it an **on-policy** algorithm.
- **Q-learning** learns the optimal action-value function **independent of the policy being followed**, making it **off-policy**. -->

<h3 id="balancing-exploration-and-exploitation">Balancing Exploration and Exploitation</h3>
<p>Both algorithms use ε-greedy exploration, so both can have a built-in exploraiton strategy, but:</p>
<ul>
<li>Sarsa updates <span class="arithmatex">\(Q(s, a)\)</span> use the actual actions taken by the agent, ensuring consistency between learning and acting.</li>
<li>Q-learning updates <span class="arithmatex">\(Q(s, a)\)</span> use the highest possible future value, leading to faster convergence but may suffer form over-estimation of the action-value function.</li>
</ul>
<p>The resultant behaviour of the algorithms is that:</p>
<ul>
<li>Sarsa learns smoother policies, as it updates based on the agent's behavior.</li>
<li>Q-learning learns optimal policies faster, but may be unstable in environments with significant randomness.</li>
</ul>
<p>Ultimately, <strong>Sarsa is more conservative and stable</strong>, while <strong>Q-learning converges faster to the optimal policy</strong> but may suffer from instability in highly stochastic environments.</p>
<p>Both algorithms use ε-greedy exploration, so both have a built-in exploration strategy, but:</p>
<ul>
<li><strong>Sarsa</strong> updates <span class="arithmatex">\(Q(s, a)\)</span> based on the actual actions taken by the agent, ensuring consistency between learning and acting.</li>
<li><strong>Q-learning</strong> updates <span class="arithmatex">\(Q(s, a)\)</span> using the highest possible future value, which accelerates convergence but may suffer from <strong>overestimation of the action-value function</strong>.</li>
</ul>
<p>While <strong>Sarsa is safer and more stable</strong>, <strong>Q-learning remains the preferred choice</strong> for most practical RL applications due to its off-policy nature, faster convergence, and ability to leverage experience replay. However, researchers continue to refine Q-learning to mitigate its overestimation issue, making it even more effective in modern reinforcement learning systems.</p>
<h3 id="sarsa-algorithm-in-pseudocode">Sarsa Algorithm in Pseudocode</h3>
<p><span class="arithmatex">\(
\begin{array}{ll}
\textbf{Algorithm: }  \text{Sarsa (On-policy TD Control)} \\
\textbf{Input: } \text{Environment dynamics and reward structure} \\
\textbf{Initialize: } Q(S, A) \leftarrow 0, \forall S \in \mathcal{S}, A \in \mathcal{A}, \alpha &gt; 0, \gamma \in [0, 1], \epsilon &gt; 0 \\
\textbf{For each episode: } &amp; \\
\quad \text{Initialize } S_0 \text{ and choose } A_0 \text{ based on policy derived from } Q \text{ (e.g., using $\epsilon$-greedy)} &amp; \\
\quad \text{For each step } t \text{ from 0 to T-1: } &amp; \\
\quad \quad R_{t+1} \leftarrow \text{Take action } A_t \text{, observe reward } R_{t+1} \text{, and next state } S_{t+1} &amp; \\
\quad \quad A_{t+1} \leftarrow \text{Choose } A_{t+1} \text{ based on policy derived from } Q \text{ (e.g., using $\epsilon$-greedy)} &amp; \\
\quad \quad Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) )&amp; \\
\quad \text{End step loop} &amp; \\
\textbf{Return: } Q(S, A), \forall S \in \mathcal{S}, A \in \mathcal{A} \\
\end{array}
\)</span></p>
<h3 id="implementation-of-sarsa">Implementation of Sarsa</h3>
<p><strong>Note: Using the previously shown TD algorithm directly is not suitable for control, we must adapt it so that it changes the Q tabel not the V table.</strong> In particular, we need to inherit from an MDP class to have the Q table available for us.</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Sarsa</span><span class="p">(</span><span class="n">MDP</span><span class="p">()):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">init</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_an</span> <span class="c1"># for Sarsa we want to decide the next action in time step t</span>
    <span class="c1"># ---------------------------------🌖 online learning ------------------------------------</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">online</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">rn</span><span class="p">,</span><span class="n">sn</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span><span class="n">an</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">α</span><span class="o">*</span><span class="p">(</span><span class="n">rn</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span> <span class="n">done</span><span class="p">)</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">γ</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">sn</span><span class="p">,</span><span class="n">an</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">])</span>
</code></pre></div>
<p>Note that we do not store the experience for this one-step online algorithm while we had to for MC, and this is again one of the advantages of online methods.</p>
<p>Let us now apply the Sarsa on a simple grid world environment. The goal is directly facing the start position. However, to make the problem more difficult for the algorithm we have deprioritised the right action and we place the order of the actions as follows: left, right, down and up. This simple change made the agent pick going left before going right and made the problem only a bit more difficult. Let us see how the Sarsa performs on it.</p>
<p>Similar to what we did earlier we will use the two dictionaries demoQ and demoR to make the calls more concise.</p>
<p><div class="highlight"><pre><span></span><code><span class="n">sarsa</span> <span class="o">=</span> <span class="n">Sarsa</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(),</span> <span class="n">α</span><span class="o">=</span><span class="mf">.8</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="o">**</span><span class="n">demoQ</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<img alt="png" src="output_52_0.png" /></p>
<p><div class="highlight"><pre><span></span><code><span class="n">mc</span> <span class="o">=</span> <span class="n">MCC</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(</span><span class="n">reward</span><span class="o">=</span><span class="s1">&#39;reward100&#39;</span><span class="p">),</span> <span class="n">α</span><span class="o">=</span><span class="mf">.3</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">demoQ</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<img alt="png" src="output_55_0.png" /></p>
<p>Note how Sarsa performed better and converged faster in fewer episodes than MCC although it did cover the full environment.</p>
<div class="highlight"><pre><span></span><code><span class="n">sarsa</span> <span class="o">=</span> <span class="n">Sarsa</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(</span><span class="n">reward</span><span class="o">=</span><span class="s1">&#39;reward100&#39;</span><span class="p">),</span> <span class="n">α</span><span class="o">=</span><span class="mf">.3</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">plotT</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">interact</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Sarsa&#39;</span><span class="p">)</span>
<span class="n">mcc</span>   <span class="o">=</span> <span class="n">MCC</span>  <span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(</span><span class="n">reward</span><span class="o">=</span><span class="s1">&#39;reward100&#39;</span><span class="p">),</span> <span class="n">α</span><span class="o">=</span><span class="mf">.3</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">plotT</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">interact</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;MCControl&#39;</span><span class="p">)</span>
</code></pre></div>
<p><img alt="png" src="output_57_1.png" /></p>
<p>Of course we change the seed the performance will change for both. Also if we change the learning rate α the performance will vary (change the seed to 0 and run). This is why it is important to conduct several runs in order to obtain the performance of the algorithms on average.</p>
<p><div class="highlight"><pre><span></span><code><span class="n">sarsa_large</span> <span class="o">=</span> <span class="n">Sarsa</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">maze_large</span><span class="p">(),</span> <span class="n">α</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span> <span class="p">,</span> <span class="o">**</span><span class="n">demoQ</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<img alt="png" src="output_60_0.png" /></p>
<h2 id="sarsa-on-windy-environment">Sarsa on windy environment</h2>
<p>In this section we show how Sarsa behaves on the windy environment that we have shown in lesson 2. The idea to show that TD is able of learning to deal with the upward wind in a manner that allows it to reach the goal effectively. This study can be seen in Example 6.5 in the book.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">Sarsa_windy</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">Sarsa</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">windy</span><span class="p">(</span><span class="n">reward</span><span class="o">=</span><span class="s1">&#39;reward1&#39;</span><span class="p">),</span> <span class="n">α</span><span class="o">=</span><span class="mf">.5</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">demoQ</span><span class="p">(),</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">170</span><span class="p">)</span><span class="o">.</span><span class="n">interact</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;TD on Windy&#39;</span><span class="p">)</span>

<span class="n">example_6_5</span> <span class="o">=</span> <span class="n">Sarsa_windy</span>

<span class="n">trainedV</span> <span class="o">=</span> <span class="n">example_6_5</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">trainedV</span><span class="o">.</span><span class="n">Ts</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(),</span> <span class="nb">range</span><span class="p">(</span><span class="n">trainedV</span><span class="o">.</span><span class="n">episodes</span><span class="p">),</span><span class="s1">&#39;-r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_67_0.png" /></p>
<h2 id="q-learning-off-policy-control">Q-learning off-policy control</h2>
<p>Now we move to the Q-learning algorithm. Q-learning is one of the most successful algorithms in RL. Although it is an <em>off-policy</em> (not offline) algorithm, it usually performs better than the Sarsa. Q-learning also allowed for a control algorithm's first proof of convergence due to its simple update rules. Below we show the pseudocode of Q-learning:</p>
<p><span class="arithmatex">\(
\begin{array}{ll}
\textbf{Algorithm: }  \text{Q-learning (Off-policy TD Control)} \\
\textbf{Input: } \text{Environment dynamics and reward structure} \\
\textbf{Initialize: } Q(S, A) \leftarrow 0, \forall S \in \mathcal{S}, A \in \mathcal{A}, \alpha &gt; 0, \gamma \in [0, 1], \epsilon &gt; 0 \\
\textbf{For each episode: } &amp; \\
\quad \text{Initialize } S_0 &amp; \\
\quad \text{For each step } t \text{ from 0 to T-1: } &amp; \\
\quad \quad \text{Choose } A_t \text{ based on policy derived from } Q \text{ (e.g., using $\epsilon$-greedy)} &amp; \\
\quad \quad R_{t+1}, S_{t+1} \leftarrow \text{Take action } A_t, \text{ observe reward and next state} &amp; \\
\quad \quad Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma \max_{a} Q(S_{t+1}, a) - Q(S_t, A_t)) &amp; \\
\quad \text{End step loop} &amp; \\
\textbf{Return: } Q(S, A), \forall S \in \mathcal{S}, A \in \mathcal{A} \\
\end{array}
\)</span></p>
<p>Note hwo we do not need to wait until we get the next action <span class="arithmatex">\(A_{t+1}\)</span> as in Sarsa. This key difference is reflected in the way we implement Sarsa and Q-learning. In Sarsa we needed to specify that we want to use the step_an function.</p>
<h3 id="implementation-of-q-learning">Implementation of Q-learning</h3>
<p><strong>Important</strong> Note that Q-learning does not require changing the step function because it does not require knowing the next action in advance (unlike Sarsa). Hence it uses a simple algorithmic schema that is almost identical to TD.</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Qlearn</span><span class="p">(</span><span class="n">MDP</span><span class="p">()):</span>
    <span class="c1">#---------------------------------🌖 online learning ---------------------------</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">online</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">rn</span><span class="p">,</span><span class="n">sn</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span><span class="n">_</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">α</span><span class="o">*</span><span class="p">(</span><span class="n">rn</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span> <span class="n">done</span><span class="p">)</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">γ</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">sn</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">])</span>
</code></pre></div>
<p>As you can see, we did not use the action <em>an</em> in Qlearning() because we take the max of the action and assume that it is the one that the agent will pick (although this might not be the case, and hence it is an <strong>off-policy</strong> learning algorithm because we are learning about a fully greedy policy while the agent is acting according to an εgreedy policy). Also note that we do not store the experience for this one-step online algorithm while we had to for MC, which is again one of the advantages of online methods.</p>
<p><div class="highlight"><pre><span></span><code><span class="n">qlearn</span> <span class="o">=</span> <span class="n">Qlearn</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(),</span> <span class="n">γ</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">α</span><span class="o">=</span><span class="mf">.8</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="o">**</span><span class="n">demoQ</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<img alt="png" src="output_71_0.png" /></p>
<h3 id="overestimation-in-q-learning">Overestimation in Q-learning</h3>
<p>Since Q-learning updates are based on <span class="arithmatex">\(\max_a Q(S_{t+1}, a)\)</span>, it always assumes the best possible action will be taken in the future, even if the agent’s policy does not necessarily follow this assumption. This optimistic evaluation can cause systematic overestimation of Q-values, especially in stochastic environments where rewards and transitions have variability.</p>
<h4 id="why-does-this-happen">Why does this happen?</h4>
<ul>
<li>If the Q-values are <em>noisy</em> (due to randomness in rewards or transitions), the <em>max operator tends to select overestimated values</em>.</li>
<li>This bias can <em>compound over time</em>, leading to inflated estimates and suboptimal policies.</li>
<li>In contrast, <em>Sarsa does not suffer from this issue</em>, as it updates based on the actual action chosen, avoiding the artificial boost from maximum Q-values.</li>
</ul>
<h4 id="consequences-of-overestimation">Consequences of Overestimation:</h4>
<ul>
<li>Instability in learning: The agent may favor suboptimal actions with overestimated values.</li>
<li>Poor policy performance in stochastic environments: The learned policy may not generalize well to different scenarios.</li>
<li>Slow or unstable convergence: The agent may take longer to find the optimal policy or oscillate between suboptimal actions.</li>
</ul>
<h4 id="mitigation-strategies">Mitigation Strategies:</h4>
<ul>
<li>Double Q-learning: Maintains two Q-value estimates and updates them separately to reduce overestimation bias.</li>
<li>Dueling Q-networks (in deep RL): Separates state-value estimation from action advantage estimation.</li>
<li>Clipped Q-learning (in some modern RL methods like Soft Actor-Critic): Limits extreme Q-value updates.</li>
</ul>
<p>Thus, while Q-learning learns optimal policies faster, it requires careful tuning or additional mechanisms to mitigate overestimation bias, particularly in stochastic environments.</p>
<hr />
<h3 id="why-researchers-and-practitioners-prefer-q-learning-over-sarsa">Why Researchers and Practitioners Prefer Q-learning Over Sarsa</h3>
<p>Despite the overestimation issue, Q-learning remains the more popular choice in practice due to several advantages:</p>
<ol>
<li>
<p>Off-policy nature:  </p>
<ul>
<li>Q-learning learns the optimal policy independently of the policy being followed during exploration. This allows it to <strong>use past experience (experience replay)</strong>, making it suitable for modern deep RL algorithms.</li>
<li>Sarsa, being on-policy, must continually update based on the current policy, making it <strong>less efficient for sample reuse</strong>.</li>
</ul>
</li>
<li>
<p>Faster convergence in deterministic environments:  </p>
<ul>
<li>Since Q-learning always uses the greedy update, it <em>quickly converges</em> to an optimal policy in deterministic environments where transitions and rewards are predictable.</li>
<li>Sarsa, on the other hand, <em>converges more cautiously</em>, as it updates based on the action actually taken.</li>
</ul>
</li>
<li>
<p>Better long-term performance in practical applications:  </p>
<ul>
<li>In domains like robotics, game-playing (e.g., Deep Q-Networks in Atari games), and control systems, Q-learning’s focus on optimality makes it more attractive.</li>
<li>Sarsa is more robust in highly stochastic environments but may settle for safer, suboptimal policies.</li>
</ul>
</li>
<li>
<p>Better compatibility with function approximation:  </p>
<ul>
<li>Deep RL methods, such as <em>DQN (Deep Q-Networks)</em>, are built on Q-learning due to its ability to learn from replayed experiences, improving sample efficiency.</li>
<li>Sarsa, being more sensitive to the policy being followed, does not benefit as much from experience replay.</li>
</ul>
</li>
</ol>
<hr />
<h3 id="convergence-guarantees">Convergence Guarantees</h3>
<p>Both Sarsa and Q-learning converge to the optimal action-value function under certain conditions:</p>
<ul>
<li>Sarsa (On-Policy Control) Convergence:  </li>
<li>Sarsa is guaranteed to converge to the optimal Q-values as long as every state-action pair is visited infinitely often and the learning rate satisfies the Robbins-Monro conditions (i.e., decreasing but not too fast).</li>
<li>Since it follows an*ε-greedy policy with exploration, it naturally satisfies this assumption over time.</li>
<li>
<p>However, because it updates based on the agent’s policy, it may settle into a near-optimal policy rather than the true optimal one if exploration is not sufficiently reduced.</p>
</li>
<li>
<p>Q-learning (Off-Policy Control) Convergence:  </p>
</li>
<li>Q-learning is proven to converge to the optimal Q-values under the same Robbins-Monro conditions.</li>
<li>However, in stochastic environments, overestimation bias can lead to suboptimal learning, and additional modifications (e.g., Double Q-learning) are needed to ensure better stability.</li>
<li>In deterministic settings, Q-learning usually converges faster than Sarsa.</li>
</ul>
<h3 id="practical-considerations">Practical Considerations:</h3>
<ul>
<li>If the environment is <em>highly stochastic</em>, <em>Sarsa</em> is often preferred because it <em>avoids overestimation and learns safer policies</em>.</li>
<li>If <em>efficiency and optimality</em> are the priorities (e.g., games, robotics, deep RL), <em>Q-learning</em> is usually the better choice due to its <em>off-policy nature and faster learning</em>.</li>
<li>Modern deep RL algorithms (e.g., <strong>DQN, DDQN</strong>) mitigate Q-learning’s weaknesses while retaining its advantages, making it the dominant choice in large-scale applications.</li>
</ul>
<hr />
<h3 id="sarsa-vs-q-learning_1">Sarsa vs Q-learning</h3>
<table>
<thead>
<tr>
<th>Feature</th>
<th><strong>Sarsa</strong> (On-policy)</th>
<th><strong>Q-learning</strong> (Off-policy)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Exploration Strategy</strong></td>
<td>Uses the action actually taken</td>
<td>Uses max Q-value over all actions</td>
</tr>
<tr>
<td><strong>Policy Type</strong></td>
<td>Learns about the <strong>same policy</strong> it follows</td>
<td>Learns about an <strong>optimal greedy policy</strong></td>
</tr>
<tr>
<td><strong>Convergence</strong></td>
<td>More stable in stochastic environments</td>
<td>Can be faster, but may suffer from overestimation</td>
</tr>
<tr>
<td><strong>Optimality</strong></td>
<td>May settle for a slightly suboptimal policy</td>
<td>More likely to find the optimal policy</td>
</tr>
<tr>
<td><strong>Experience Replay</strong></td>
<td>Less effective</td>
<td>Well-suited (used in DQN, etc.)</td>
</tr>
<tr>
<td><strong>Application</strong></td>
<td>Safer, more robust learning (e.g., safety-critical RL)</td>
<td>Used in deep RL (DQN), robotics, and large-scale control problems</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="sarsa-and-q-learning-on-a-cliff-edge">Sarsa and Q-Learning on a Cliff Edge!</h2>
<p>This section compares the performance of on-policy Sarsa and off-policy Q-learning algorithms to show how each act on a specific problem. The problem that we will tackle is a cliff-edge world. This is a grid world of 12x4, with a goal location on the far-right bottom corner and the start location on the far-left bottom corner. There are no obstacles. However, there is a cliff between the start and the goal locations on the bottom. If the agent trespasses on it, it falls off the cliff, receives a penalty of -100 and will be relocated back to the start location <em>without starting a new episode</em>. The agent receives a reward of -1 everywhere, including the goal location. We will use the sum of rewards metric to measure the performance of algorithms on this problem.</p>
<div class="highlight"><pre><span></span><code><span class="n">sarsa</span> <span class="o">=</span> <span class="n">Sarsa</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">cliffwalk</span><span class="p">(),</span> <span class="n">α</span><span class="o">=</span><span class="mf">.5</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">demoR</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_75_0.png" /></p>
<div class="highlight"><pre><span></span><code><span class="n">sarsa</span> <span class="o">=</span> <span class="n">Qlearn</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">cliffwalk</span><span class="p">(),</span> <span class="n">α</span><span class="o">=</span><span class="mf">.5</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">demoR</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_76_0.png" /></p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">Sarsa_Qlearn_cliffwalk</span><span class="p">(</span><span class="n">runs</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">α</span><span class="o">=</span><span class="mf">.5</span><span class="p">,</span> <span class="n">env</span><span class="o">=</span><span class="n">cliffwalk</span><span class="p">(),</span> <span class="n">alg1</span><span class="o">=</span><span class="n">Sarsa</span><span class="p">,</span> <span class="n">alg2</span><span class="o">=</span><span class="n">Qlearn</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;right&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;top&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>    
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([</span><span class="o">-</span><span class="mi">100</span><span class="p">,</span> <span class="o">-</span><span class="mi">75</span><span class="p">,</span> <span class="o">-</span><span class="mi">50</span><span class="p">,</span> <span class="o">-</span><span class="mi">25</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">100</span><span class="p">,</span> <span class="o">-</span><span class="mi">10</span><span class="p">)</span>


    <span class="n">SarsaCliff</span> <span class="o">=</span> <span class="n">Runs</span><span class="p">(</span><span class="n">algorithm</span><span class="o">=</span><span class="n">alg1</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">,</span> <span class="n">α</span><span class="o">=</span><span class="n">α</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">500</span><span class="p">),</span> <span class="n">runs</span><span class="o">=</span><span class="n">runs</span><span class="p">,</span> <span class="n">plotR</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">interact</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Sarsa&#39;</span><span class="p">)</span>
    <span class="n">QlearnCliff</span> <span class="o">=</span> <span class="n">Runs</span><span class="p">(</span><span class="n">algorithm</span><span class="o">=</span><span class="n">alg2</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">,</span> <span class="n">α</span><span class="o">=</span><span class="n">α</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">500</span><span class="p">),</span> <span class="n">runs</span><span class="o">=</span><span class="n">runs</span><span class="p">,</span> <span class="n">plotR</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">interact</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Q-learning&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">SarsaCliff</span><span class="p">,</span> <span class="n">QlearnCliff</span>

<span class="k">def</span><span class="w"> </span><span class="nf">example_6_6</span><span class="p">(</span><span class="o">**</span><span class="n">kw</span><span class="p">):</span> <span class="k">return</span> <span class="n">Sarsa_Qlearn_cliffwalk</span><span class="p">(</span><span class="o">**</span><span class="n">kw</span><span class="p">)</span>
</code></pre></div>
<p><div class="highlight"><pre><span></span><code><span class="n">SarsaCliff</span><span class="p">,</span> <span class="n">QlearnCliff</span> <span class="o">=</span> <span class="n">Sarsa_Qlearn_cliffwalk</span><span class="p">()</span>
</code></pre></div>
<img alt="png" src="output_78_1.png" /></p>
<h2 id="expected-sarsa">Expected Sarsa</h2>
<p>In this section, we cover the expected Sarsa algorithm. This algorithm is very similar to the Q-learning algorithm and has the same schematic structure (unlike Sarsa, it does not require obtaining the next action in advance). It takes all the probabilities of the different actions and forms an expectation of the next action.</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">XSarsa</span><span class="p">(</span><span class="n">MDP</span><span class="p">()):</span>

    <span class="c1"># ------------------------------------- 🌖 online learning --------------------------------------</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">online</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">rn</span><span class="p">,</span><span class="n">sn</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span><span class="n">_</span><span class="p">):</span>      
        <span class="c1"># obtain the ε-greedy policy probabilities, then obtain the expecation via a dot product for efficiency</span>
        <span class="n">π</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">π</span><span class="p">(</span><span class="n">sn</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">sn</span><span class="p">]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">π</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">α</span><span class="o">*</span><span class="p">(</span><span class="n">rn</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span> <span class="n">done</span><span class="p">)</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">γ</span><span class="o">*</span><span class="n">v</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">])</span>
</code></pre></div>
<p>Note that the policy is assumed to be ε-greedy, if you want to deal with other policies then a different implementation is required</p>
<div class="highlight"><pre><span></span><code><span class="n">xsarsa</span> <span class="o">=</span> <span class="n">XSarsa</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">cliffwalk</span><span class="p">(),</span> <span class="n">α</span><span class="o">=</span><span class="mf">.5</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">demoR</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_83_0.png" /></p>
<h2 id="double-q-learning">Double Q-learning</h2>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">DQlearn</span><span class="p">(</span><span class="n">MDP</span><span class="p">()):</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">init</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Q1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Q2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="c1"># we need to override the way we calculate the aciton-value function in our εgreedy policy</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">Q_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q1</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q2</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="k">if</span> <span class="n">s</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q2</span>

    <span class="c1"># ----------------------------- 🌖 online learning ----------------------</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">online</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">rn</span><span class="p">,</span><span class="n">sn</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span><span class="n">_</span><span class="p">):</span> 
        <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">p</span><span class="p">:</span>    <span class="bp">self</span><span class="o">.</span><span class="n">Q1</span><span class="p">[</span><span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">α</span><span class="o">*</span><span class="p">(</span><span class="n">rn</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span> <span class="n">done</span><span class="p">)</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">γ</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">Q2</span><span class="p">[</span><span class="n">sn</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q1</span><span class="p">[</span><span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>    <span class="bp">self</span><span class="o">.</span><span class="n">Q2</span><span class="p">[</span><span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">α</span><span class="o">*</span><span class="p">(</span><span class="n">rn</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span> <span class="n">done</span><span class="p">)</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">γ</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">Q1</span><span class="p">[</span><span class="n">sn</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q2</span><span class="p">[</span><span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">])</span>
</code></pre></div>
<h2 id="comparing-sarsa-expected-sarsa-q-learning-and-double-q-learning">Comparing Sarsa, Expected Sarsa, Q-learning and Double Q-learning</h2>
<p>Ok now we can compare all 4 algorithms on the different environments to see their performances. </p>
<h3 id="comparison-on-cliff-walking">Comparison on cliff walking</h3>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">XSarsaDQlearnCliff</span><span class="p">(</span><span class="n">runs</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">α</span><span class="o">=</span><span class="mf">.5</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;right&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;top&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>    
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([</span><span class="o">-</span><span class="mi">100</span><span class="p">,</span> <span class="o">-</span><span class="mi">75</span><span class="p">,</span> <span class="o">-</span><span class="mi">50</span><span class="p">,</span> <span class="o">-</span><span class="mi">25</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">100</span><span class="p">,</span> <span class="o">-</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">env</span> <span class="o">=</span> <span class="n">cliffwalk</span><span class="p">()</span>

    <span class="n">XSarsaCliff</span> <span class="o">=</span> <span class="n">Runs</span><span class="p">(</span><span class="n">algorithm</span><span class="o">=</span><span class="n">XSarsa</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">,</span> <span class="n">α</span><span class="o">=</span><span class="n">α</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">500</span><span class="p">),</span> <span class="n">runs</span><span class="o">=</span><span class="n">runs</span><span class="p">,</span> <span class="n">plotR</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">interact</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;XSarsa&#39;</span><span class="p">)</span>
    <span class="n">DQlearnCliff</span> <span class="o">=</span> <span class="n">Runs</span><span class="p">(</span><span class="n">algorithm</span><span class="o">=</span><span class="n">DQlearn</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">,</span> <span class="n">α</span><span class="o">=</span><span class="n">α</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">500</span><span class="p">),</span> <span class="n">runs</span><span class="o">=</span><span class="n">runs</span><span class="p">,</span> <span class="n">plotR</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">interact</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Double Q-learning&#39;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">XSarsaCliff</span><span class="p">,</span> <span class="n">DQlearnCliff</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">SarsaCliff</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Sarsa&#39;</span><span class="p">,</span> <span class="n">frmt</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="n">QlearnCliff</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Q-learning&#39;</span><span class="p">,</span> <span class="n">frmt</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
<span class="n">XSarsaCliff</span><span class="p">,</span> <span class="n">DQlearnCliff</span> <span class="o">=</span> <span class="n">XSarsaDQlearnCliff</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_90_1.png" /></p>
<h3 id="comparison-on-the-maze">Comparison on the Maze</h3>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">compareonMaze</span><span class="p">(</span><span class="n">runs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">α</span><span class="o">=</span><span class="mf">.5</span><span class="p">):</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;right&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;top&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

    <span class="n">env</span><span class="o">=</span><span class="n">Grid</span><span class="p">(</span><span class="n">gridsize</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">],</span> <span class="n">style</span><span class="o">=</span><span class="s1">&#39;maze&#39;</span><span class="p">,</span> <span class="n">s0</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="n">reward</span><span class="o">=</span><span class="s1">&#39;reward1&#39;</span><span class="p">)</span> <span class="c1"># this is bit bigger than the defualt maze</span>
    <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>

    <span class="n">SarsaMaze</span> <span class="o">=</span> <span class="n">Runs</span><span class="p">(</span><span class="n">algorithm</span><span class="o">=</span><span class="n">Sarsa</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">,</span> <span class="n">α</span><span class="o">=</span><span class="n">α</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">30</span><span class="p">),</span> <span class="n">runs</span><span class="o">=</span><span class="n">runs</span><span class="p">,</span> <span class="n">plotT</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">interact</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Sarsa&#39;</span><span class="p">)</span>
    <span class="n">XSarsaMaze</span> <span class="o">=</span> <span class="n">Runs</span><span class="p">(</span><span class="n">algorithm</span><span class="o">=</span><span class="n">XSarsa</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">,</span> <span class="n">α</span><span class="o">=</span><span class="n">α</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">30</span><span class="p">),</span> <span class="n">runs</span><span class="o">=</span><span class="n">runs</span><span class="p">,</span> <span class="n">plotT</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">interact</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;XSarsa&#39;</span><span class="p">)</span>

    <span class="n">QlearnMaze</span> <span class="o">=</span> <span class="n">Runs</span><span class="p">(</span><span class="n">algorithm</span><span class="o">=</span><span class="n">Qlearn</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">,</span> <span class="n">α</span><span class="o">=</span><span class="n">α</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">30</span><span class="p">),</span> <span class="n">runs</span><span class="o">=</span><span class="n">runs</span><span class="p">,</span> <span class="n">plotT</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">interact</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Q-learning&#39;</span><span class="p">)</span>
    <span class="n">DQlearnMaze</span> <span class="o">=</span> <span class="n">Runs</span><span class="p">(</span><span class="n">algorithm</span><span class="o">=</span><span class="n">DQlearn</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">,</span> <span class="n">α</span><span class="o">=</span><span class="n">α</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">30</span><span class="p">),</span> <span class="n">runs</span><span class="o">=</span><span class="n">runs</span><span class="p">,</span> <span class="n">plotT</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">interact</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Double Q-learning&#39;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">SarsaMaze</span><span class="p">,</span> <span class="n">XSarsaMaze</span><span class="p">,</span> <span class="n">QlearnMaze</span><span class="p">,</span> <span class="n">DQlearnMaze</span>
</code></pre></div>
<p><div class="highlight"><pre><span></span><code><span class="n">SarsaMaze</span><span class="p">,</span> <span class="n">XSarsaMaze</span><span class="p">,</span> <span class="n">QlearnMaze</span><span class="p">,</span> <span class="n">DQlearnMaze</span> <span class="o">=</span> <span class="n">compareonMaze</span><span class="p">(</span><span class="n">α</span><span class="o">=</span><span class="mf">.5</span><span class="p">)</span>
</code></pre></div>
<img alt="png" src="output_93_2.png" /></p>
<h2 id="actor-critic-td-for-policy-gradient-methods">Actor-Critic: TD for Policy Gradient Methods</h2>
<p>Earlier, we saw how REINFORCE could perform well in the grid environment. REINFORCE is a policy gradient method that attempts to directly estimate a policy instead of estimating an action-value function. This is done by using the value function as an objective function that we would want to <em>maximise</em> (instead of minimising an error function as in Sarsa or Q-learning).</p>
<p>Like Monte Carlo, REINFORCE is an offline method that needs to wait until the end of an episode to estimate the value function. The question, then, is there an algorithm similar to REINFORCE but online? The method should be derived similarly to Sarsa and Q-learning, which depends on the next step estimate of the value function.
The answer is yes, and the method is called Actor-critic, which does that exactly. The algorithm general unified update attempts to estimate its policy by directly <em>maximising the returns with respect to a baseline</em> (see section 13.4). When the algorithm replaces its returns with an estimate of the returns (section 13.5, the difference between the return estimate and the baseline becomes a TD error), the algorithm can be thought of as having two distinctive parts an actor and a critic. The actor maximises its <em>start-state-value function</em>, while the critic attempts to improve its <em>estimates</em> of the <em>state-value function</em> for all states. Both of them use the Temporal Difference (TD) error to improve their estimates, meaning they can work online. Like REINFORCE, the actor-critic uses a SoftMax policy to select an action according to the actor policy parameters. So, to maximise the value, the actor takes the derivative of the <span class="arithmatex">\(\nabla \log v(S_0)\)</span>. </p>
<p>Actor-critic is one of the oldest RL algorithms, and it avoids several issues that arise from the use of <span class="arithmatex">\(\epsilon\)</span>-greedy policy. The most obvious one is that the policy changes the <em>probability</em> of selecting an action gradually and continuously when the parameters change, unlike <span class="arithmatex">\(\epsilon\)</span>-greedy, which can change the <em>maximum value action</em> abruptly due to a small change in the parameters. This also allows it to provide better convergence guarantees.</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Actor_Critic</span><span class="p">(</span><span class="n">PG</span><span class="p">()):</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">step0</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">γt</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1"># powers of γ, must be reset at the start of each episode</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">online</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">rn</span><span class="p">,</span><span class="n">sn</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span><span class="n">an</span><span class="p">):</span> 
        <span class="n">π</span><span class="p">,</span> <span class="n">γ</span><span class="p">,</span> <span class="n">γt</span><span class="p">,</span> <span class="n">α</span><span class="p">,</span> <span class="n">τ</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">π</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">γ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">γt</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">α</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">τ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">t</span>
        <span class="n">δ</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span> <span class="n">done</span><span class="p">)</span><span class="o">*</span><span class="n">γ</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">[</span><span class="n">sn</span><span class="p">]</span> <span class="o">+</span> <span class="n">rn</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">]</span>  <span class="c1"># TD error is based on the critic estimate</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="o">+=</span> <span class="n">α</span><span class="o">*</span><span class="n">δ</span>                          <span class="c1"># critic</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="n">α</span><span class="o">*</span><span class="n">δ</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span> <span class="n">π</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">))</span><span class="o">*</span><span class="n">γt</span><span class="o">/</span><span class="n">τ</span>         <span class="c1"># actor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">γt</span> <span class="o">*=</span> <span class="n">γ</span>
</code></pre></div>
<h3 id="delayed-reward">Delayed Reward</h3>
<p>First let us establish the baseline performance.</p>
<p><div class="highlight"><pre><span></span><code><span class="n">ac</span> <span class="o">=</span> <span class="n">Actor_Critic</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(),</span> <span class="n">α</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">τ</span><span class="o">=</span><span class="mf">.3</span><span class="p">,</span> <span class="n">γ</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span> <span class="p">,</span> <span class="o">**</span><span class="n">demoQ</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<img alt="png" src="output_100_0.png" /></p>
<p>Note that we set α=1 which is unusual for an RL algorithm and the method just worked. This is a testimony to the resilience and strength of actor-critic methods. Note how reducing the exploration factor <span class="arithmatex">\(\tau=.3\)</span> led to a much faster convergence.</p>
<p>Note how we had to increase the number of episodes to converge when we set <span class="arithmatex">\(\alpha=.1\)</span> instead of <span class="arithmatex">\(\alpha=1\)</span>.</p>
<p><div class="highlight"><pre><span></span><code><span class="n">ac</span> <span class="o">=</span> <span class="n">Actor_Critic</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(),</span> <span class="n">α</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">τ</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">γ</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">demoQ</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<img alt="png" src="output_105_0.png" /></p>
<p>Note how reducing both <span class="arithmatex">\(\tau\)</span> and <span class="arithmatex">\(\alpha\)</span> helped reach convergence quickly but with a better exploration.</p>
<h3 id="intermediate-reward">Intermediate Reward</h3>
<p><div class="highlight"><pre><span></span><code><span class="n">ac</span> <span class="o">=</span> <span class="n">Actor_Critic</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(</span><span class="n">reward</span><span class="o">=</span><span class="s1">&#39;reward0&#39;</span><span class="p">),</span> <span class="n">α</span><span class="o">=</span><span class="mf">.7</span><span class="p">,</span> <span class="n">τ</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">γ</span><span class="o">=</span><span class="mf">.98</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span> <span class="p">,</span> <span class="o">**</span><span class="n">demoQ</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<img alt="png" src="output_116_0.png" /></p>
<p><div class="highlight"><pre><span></span><code><span class="n">ac</span> <span class="o">=</span> <span class="n">Actor_Critic</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">maze</span><span class="p">(</span><span class="n">reward</span><span class="o">=</span><span class="s1">&#39;reward0&#39;</span><span class="p">),</span> <span class="n">α</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">τ</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>  <span class="n">γ</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span> <span class="p">,</span> <span class="o">**</span><span class="n">demoQ</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<img alt="png" src="output_120_0.png" /></p>
<div class="highlight"><pre><span></span><code><span class="n">ac_large</span> <span class="o">=</span> <span class="n">Actor_Critic</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">maze_large</span><span class="p">(),</span> <span class="n">α</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">τ</span><span class="o">=</span><span class="mf">.3</span><span class="p">,</span> <span class="n">γ</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span> <span class="p">,</span> <span class="o">**</span><span class="n">demoQ</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_124_0.png" /></p>
<h2 id="model-selection-methods-comparisons-class">Model selection: methods comparisons class</h2>
<p>Ok, the question is, which one of these algorithms would perform best regardless of the learning rate α? To be able to know, we would need to compare the performances on a set of α values to see the full picture. To that end, we developed a useful comparison class. It allows us to compare algorithms with different hyperparameters similar to what we did in other machine learning modules. All that is required is to specify which hyperparameter we want to vary and then pass the values we want to test for in a dictionary.</p>
<p>We can compare different α values to specify which algorithm is dominant. This study can be seen in Figure 6.3 in the book. Here we do 10 runs because it takes longer to do more, but you are welcome to try to run it for 100 runs. Note that the asymptotic study will run for 1000. the idea here is to compare the performances of the above control algorithms and variants of Q-learning and Sarsa in a systematic manner. The domain is the cliff walking environment. We want to see which algorithms (Sarsa, expected Sarsa, Q-learning, double Q-learning) perform best regardless of the learning rate. Such comparison would give us a definitive answer on which algorithm is best for the given problem when we see a pattern of dominance for all learning rate values.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">figure_6_3</span><span class="p">(</span><span class="n">runs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">Interim</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">Asymptotic</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>  <span class="n">label</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">):</span> <span class="c1">#100</span>
    <span class="c1">#plt.ylim(-150, -10)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mf">.1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Interim and Asymptotic performance&#39;</span><span class="p">)</span>
    <span class="n">αs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">.1</span><span class="p">,</span><span class="mf">1.05</span><span class="p">,</span><span class="mf">.05</span><span class="p">)</span>


    <span class="n">algors</span> <span class="o">=</span> <span class="p">[</span> <span class="n">XSarsa</span><span class="p">,</span>   <span class="n">Sarsa</span><span class="p">,</span>   <span class="n">Qlearn</span><span class="p">]</span><span class="c1">#,      DQlearn]</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;XSarsa&#39;</span><span class="p">,</span> <span class="s1">&#39;Sarsa&#39;</span><span class="p">,</span> <span class="s1">&#39;Qlearning&#39;</span><span class="p">]</span><span class="c1">#, &#39;Double Q learning&#39;]</span>
    <span class="n">frmts</span>  <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">,</span>      <span class="s1">&#39;^&#39;</span><span class="p">,</span>     <span class="s1">&#39;s&#39;</span><span class="p">]</span><span class="c1">#,         &#39;d&#39;]</span>

    <span class="n">env</span> <span class="o">=</span> <span class="n">cliffwalk</span><span class="p">()</span>
    <span class="n">Interim_</span><span class="p">,</span> <span class="n">Asymptotic_</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="c1"># Interim perfromance......</span>
    <span class="k">if</span> <span class="n">Interim</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">g</span><span class="p">,</span> <span class="n">algo</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">algors</span><span class="p">):</span>
            <span class="n">compare</span> <span class="o">=</span> <span class="n">Compare</span><span class="p">(</span><span class="n">algorithm</span><span class="o">=</span><span class="n">algo</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="n">episodes</span><span class="p">),</span> <span class="n">runs</span><span class="o">=</span><span class="n">runs</span><span class="p">,</span> <span class="n">hyper</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;α&#39;</span><span class="p">:</span><span class="n">αs</span><span class="p">},</span>
                             <span class="n">plotR</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">compare</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="n">labels</span><span class="p">[</span><span class="n">g</span><span class="p">]</span><span class="o">+</span><span class="s1">&#39; Interim&#39;</span><span class="o">+</span><span class="n">label</span><span class="p">,</span> <span class="n">frmt</span><span class="o">=</span><span class="n">frmts</span><span class="p">[</span><span class="n">g</span><span class="p">]</span><span class="o">+</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
            <span class="n">Interim_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">compare</span><span class="p">)</span>

    <span class="c1"># Asymptotic perfromance......</span>
    <span class="k">if</span> <span class="n">Asymptotic</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">g</span><span class="p">,</span> <span class="n">algo</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">algors</span><span class="p">):</span>
            <span class="n">compare</span> <span class="o">=</span> <span class="n">Compare</span><span class="p">(</span><span class="n">algorithm</span><span class="o">=</span><span class="n">algo</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="n">episodes</span><span class="o">*</span><span class="mi">10</span><span class="p">),</span> <span class="n">runs</span><span class="o">=</span><span class="n">runs</span><span class="p">,</span> <span class="n">hyper</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;α&#39;</span><span class="p">:</span><span class="n">αs</span><span class="p">},</span> 
                             <span class="n">plotR</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">compare</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="n">labels</span><span class="p">[</span><span class="n">g</span><span class="p">]</span><span class="o">+</span><span class="s1">&#39; Asymptotic&#39;</span><span class="o">+</span><span class="n">label</span><span class="p">,</span> <span class="n">frmt</span><span class="o">=</span><span class="n">frmts</span><span class="p">[</span><span class="n">g</span><span class="p">]</span><span class="o">+</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
            <span class="n">Asymptotic_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">compare</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Interim_</span><span class="p">,</span> <span class="n">Asymptotic_</span>

<span class="n">Interim_</span><span class="p">,</span> <span class="n">Asymptotic_</span> <span class="o">=</span> <span class="n">figure_6_3</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_128_1.png" /></p>
<p>As we can see the expected Sarsa performed best in the interim and on the asymptote.</p>
<h2 id="conclusion">Conclusion</h2>
<p>In this lesson, we have further developed our understanding of important and prominent RL online algorithms that are widely used, all based on the value iteration idea. I.e., we keep improving our policy and refining our value-function iteratively in each step until convergence. All of our algorithms are based on the Temporal Difference method. TD uses bootstrapping in its update; instead of using a true return of a state, it uses the current reward + its own estimation of the return for the next state. It is quite surprising to see how well TD works in practice. TD has been proven to converge to a good solution under some basic conditions regarding the learning rate. In practice, however, we assign a fixed small learning rate that works just fine. It is desirable that the learning rate is not decayed when the environment’s dynamics are expected to change.
We have further used TD update in a few control algorithms. Most notable are the Sarsa and Q-learning. The first is an on-policy, while the latter is an off-policy control algorithm. We have compared all algorithms on different problems, studied their strengths and weaknesses, and how they are expected to behave on a certain problem.</p>
<p><strong>Further Reading</strong>:
For further reading you refer chapter 6 from the Sutton and Barto <a href="http://incompleteideas.net/book/RLbook2020.pdf">book</a>. There are more rigorous books that take special care for the mathematics guarantees behind the ideas of RL, such as <a href="http://web.mit.edu/jnt/www/ndp.html">Neuro-Dynamic Programming</a>.</p>
<h2 id="your-turn">Your turn</h2>
<p>Now it is time to experiment further and interact with code in <a href="../../workseets/worksheet8.ipynb">worksheet8</a>.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright Abdulrahman Altahhan
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.sections"], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.f1b6f286.min.js"></script>
      
        <script src="../../javascript/tablecontentsoverride.js"></script>
      
        <script src="../../javascript/config.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js"></script>
      
        <script src="../../videos/my-video.mp4"></script>
      
    
  </body>
</html>