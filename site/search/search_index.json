{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":"<p>By: Abdulrahman Altahhan, Feb 2025</p>"},{"location":"index.html#welcome-to-reinforcement-learning-and-robotics","title":"Welcome to Reinforcement Learning and Robotics!","text":"<p>Welcome to the Reinforcement Learning and Robotics module, where you learn the fundamentals of reinforcement learning(RL) with various simulaitons including robotics. The module focuses on RL as a general robust framework that allows us to deal with autonomous agent learning. </p> <p>In each unit, you will learn essential RL ideas and algorithms and see how these can be implemented in simplified environments. Each lesson involves some reading material, along with some videos explaining these lessons. This will then be followed by running and experimenting with the Jupyter notebooks we provide, which implement the main ideas you have read and show you the RL algorithms you studied in action. It is essential that you read the material and engage with these notebooks in the way you see fit. This includes studying and running the provided code to gain insight into the covered algorithms, observing the algorithm convergence behaviour, experimenting with the hyperparameter and their effect on the agent's behaviour, and taking a turn to implement some concepts that were left out for you. </p> <p>There are also some lessons that familiarise you with robotics as an application domain, which covers some simple concepts in robotics without delving deep into classical robotics, which is outside the scope of this module. Our units conclude with a simple, practical tutorial on utilising a simulated robot. You will use the provided code to control a simulated mobile robot called TurtleBot. The lessons are meant to gradually build your ability to deal with atonomous agents in a simplified environment. The final project will focus on comparing different RL solutions to solve a simulated robot navigation problem. We will provide you access to an Azure Ubuntu virtual machine already set up with ROS2 to run the sheets. You do not need to set up your own VM.</p> <p>Reinforcement Learning (RL) is dedicated to acquiring an optimal policy to enhance agent performance. Traditionally, RL involves the agent seeking an optimal policy to maximise cumulative discounted rewards garnered by navigating various environmental states. While the agent is commonly perceived as a physical entity interacting with its surroundings, it can also encompass abstract systems, with states representing system configurations or settings. Notably, recent RL advancements have ventured into novel territories, such as optimising language models like LLMs for perplexity or other metrics.</p> <p>In this module, our focus primarily revolves around simulated agents, aligning with the nature of our programme. However, the underlying principles remain universal and applicable across diverse scenarios. The concept of guiding learning through rewards is deeply ingrained in biological organisms, from complex human brains to single-cell organisms like amoebas, all driven by the innate urge to maximise survival and proliferation.</p> <p>While RL offers tremendous efficacy when configured appropriately, it is susceptible to spectacular failure when its conditions are unmet. Its inherent stochasticity adds another layer of complexity, contributing to its volatile nature. Nonetheless, this volatility serves as a catalyst for researchers to delve deeper into understanding the governing rules of RL processes.</p> <p>Our module adopts a pragmatic approach, aiming to provide you with a solid theoretical grounding in RL without overly delving into intricate mathematical details. Simultaneously, we will attempt to equip you with practical skills and techniques to harness RL's benefits effectively. This balanced approach ensures that you grasp RL's essence while gaining valuable real-world application tools.</p>"},{"location":"index.html#rl-in-context-advantages-and-challenges","title":"RL in context: advantages and challenges","text":"<p>In our RL coverage, you will notice that we do not do classical robotics and we believe it is not the way to go except possibly for industrial robots. So, we do not need motion planning or accurate trajectory calculations/kinematics to allow the robot to execute a task, we simply let it interact with its environment and learn by itself how to solve the task and this is what reinforcement learning is about. In our coverage we also do not supervise or directly teach the robot, this type of interference is called imitation. </p> <p>This is all great but what about challenges? Obviously, we still have several challenges to this approach. One is the number of experiments required to learn the task which can be numerous, which exposes the physical agents (real robots) to the risk of wear and tear due to repetition and can be very lengthy and tedious for the human that is supervising the task. This can be partially overcome by starting in simulation and then moving to a real robot with a technique called sim-to-real where we can employ GANs(generative adversarial neural networks).  The other challenge is the time required for training regardless whether it is in simulation or in real scenarios. </p> <p>The origin of these problems is actually the exploitation/exploration needed by RL algorithms which is in the heart of what we will be exploring in all of our RL coverage. Reducing the amount of training required is important and remains an active area for research. One approach is via experience replay and the other is via eligibility traces as we shall see later.</p>"},{"location":"index.html#textbook","title":"Textbook","text":"<p>The primary textbook for this unit and the following ones is Introduction to Reinforcement Learning by Sutton and Barto, available online here. While reading the corresponding chapters is not required, they serve as valuable supplementary material to enhance your understanding and explore the subject in greater depth.</p> <p>list of symbols</p> <ul> <li>\\(v0\\): denotes an initial value</li> <li>\\(\u03b8\\): denotes a threshold</li> <li>\\(nS\\): denotes state space dimension </li> <li>\\(nA\\): denotes actions space dimension </li> <li>\\(nR\\): denotes rewards space dimension</li> <li>\\(nU\\): denotes the number of updates</li> <li> <p>goal: a terminal state</p> </li> <li> <p>\\(r\\): current step reward</p> </li> <li>\\(s\\): current step state</li> <li>\\(a\\): current step action</li> <li>\\(rn\\): next step reward</li> <li>\\(sn\\): next step state</li> <li> <p>\\(an\\): next step action</p> </li> <li> <p>\\(\u03b1\\): learning rate</p> </li> <li>\\(\u03b5\\): exploration rate</li> <li>\\(d\u03b1\\): decay factor for \u03b1</li> <li>\\(d\u03b5\\): decay factor for \u03b5</li> <li> <p>\\(G(t+1,t+n)\\): the return between time step t+1 and t+n</p> </li> <li> <p>Rs: is the sum of rewards of an episode</p> </li> <li>Ts: is the steps of a set of an episode</li> <li>Es: is the errors (RMSE) of an episode</li> </ul>"},{"location":"index.html#module-plan","title":"Module Plan","text":"<p>We cover the tabular solution methods in the first three units, while approximate solution methods will be covered in subsequent units.</p> <p>Tabular and approximate solution methods fall under two types of RL methods that we will attempt to deal with </p> <ol> <li>Prediction methods: AKA Policy Evaluation Methods that attempt to find the best estimate for the value-function \\(V\\) or action-value function \\(Q\\) for a policy \\(\\pi\\).</li> <li>Control methods: AKA Policy Improvement Methods that attempt to find the best policy \\(\\pi_*\\), often by starting from an initial policy and then moving into a better and improved policy.</li> </ol> <p>Control methods, or policy improvement methods, in turn, falls under two categories:</p> <ol> <li>Methods that improve the policy via improving the action-value function \\(Q\\)</li> <li>Methods that improve the policy directly \\(\\pi\\)</li> </ol> <p>We start by assuming that the policy is fixed. This will help us develop algorithms predicting the state\u2018s value function (expected return). Then, we will move to the policy improvement methods, i.e., methods that help us compare our policy with other policies and move to a better policy when necessary. We then move to seamlessly integrating both for control case (policy and value iteration methods). Finally, we cover policy gradient methods that improve the policy directly.</p> <p>Note that the guarantee from the policy-improvement theorem no longer applies when we move from the table representation of the value function for small state space to the parametric function approximation representation for large state space. This will encourage us to move to direct policy-improvement methods instead of improving the policy via improving the value function.</p>"},{"location":"index.html#table-of-contents","title":"Table of Contents","text":"<p>Unit 1</p> <ol> <li>Tabular Methods </li> <li>K-Arm Bandit </li> <li>MDP </li> <li>ROS </li> </ol> <p>Unit 2</p> <ol> <li>Dynamic Programming </li> <li>Monte Carlo </li> <li>Mobile Robots </li> </ol> <p>Unit 3</p> <ol> <li>Temporal Difference </li> <li>n-Step Methods </li> <li>Planning in RL (optional) </li> <li>Localisation and SLAM </li> </ol> <p>Unit 4</p> <ol> <li>Function Approximation Methods </li> <li>Linear Approximation for Prediction </li> <li>Linear Approximation for Control </li> </ol> <p>Unit 5</p> <ol> <li>Linear Approximation with Eligibility Traces (Prediction and Control) </li> <li>Nonlinear Approximation for Control </li> <li>Application on Robot Navigation </li> </ol> <p>Unit 6</p> <ol> <li>Application on Games (optional) </li> </ol>"},{"location":"index.html#code-structure-and-notebooks-dependecies","title":"Code Structure and Notebooks Dependecies","text":"<p>Worksheets can be cloned and launched in GitHub Codespaces (repo is: AltahhanAi/RL-worksheets)</p> <p></p> <p>We have provided you with two RL libraries designed for this module. One has bespoke environments and one that has base RL classes that makes working with algorithms very easy and as close as it can be to just provide an update rule.</p> <p>Important note: Please place all worksheets in one folder, and inside this folder you must have the downloaded libraries folders (env and rl) to allow the imports to work appropriately</p>"},{"location":"index.html#installing-other-libraries-that-will-be-needed-later","title":"Installing other libraries that will be needed later","text":"<pre><code>!pip install --upgrade pip\n!pip install opencv-python\n!pip install scikit-learn\n!pip install matplotlib\n!pip install tqdm \n</code></pre> <pre><code>!pip install jupyterlab\n!pip install jupyterthemes\n\n!jt -t solarizedl -T -N  # -T, -N keeps the toolbar and header\n</code></pre>"},{"location":"index.html#available-themes","title":"available themes:","text":"<ul> <li>oceans16 </li> <li>grade3 </li> <li>chesterish </li> <li>solarizedl </li> <li>solarizedd </li> <li>gruvboxl</li> <li>!jt -r # resets back to the default theme</li> </ul>"},{"location":"index.html#better-readability","title":"Better Readability","text":"<p>For better readability and experience, please use Jupyter Lab or Vcode(if you are using Azure VM) to navigate between the different notebooks easily. If you want to use Jupyter Notebooks and not Jupyter Lab, we recommend increasing the cells' width for a better experience. We provided a function that increase your notebook width which is envoked automatically when you import an environment (grid particularly). You may want to utilise also the table of contents button in Jupetr Lab.</p>"},{"location":"unit1/lesson1/lesson1.html","title":"Lesson 1: Introduction to Tabular Methods in Reinforcement Learning","text":"<p>Unit 1 Learning outcomes</p> <p>By the end of this unit, you will be able to:  </p> <ol> <li>Explain the armed bandit problem and how isolating the action space simplifies decision-making.  </li> <li>Describe the value function and the action-value function, highlighting their essential roles in reinforcement learning (RL).  </li> <li>Differentiate between associative and non-associative problems in RL.  </li> <li>Analyze the theoretical foundations of RL, including Markov Decision Processes (MDPs) and the Bellman equation.  </li> <li>Compare prediction and control in RL settings, outlining their respective challenges and solutions.  </li> </ol> <p>In this unit, we start by covering a simplified RL settings in the form of common problems, called Armed-bandit problem. We then move into understanding the main mathematical framework underpining  reinforcement learning, namely Markov Decision Processes(MDPs). As always we take a balaced approach by discussing such concepts from a theoretical and practical perspectives. </p>"},{"location":"unit1/lesson1/lesson1.html#markov-property","title":"Markov property","text":"<p>RL has gained a lot of attention in recent years due to its unmatched ability to tackle difficult control problems with a minimal assumption about the settings and the environment that an agent works in. Controlling an agent (such as a simulated robot) is not trivial and can often require a specific setup and strong assumptions about its environment that make the corresponding solutions sometimes either difficult to attain or impractical in real scenarios. In RL, we try to minimise these assumptions and require that only the environment adheres to the Markov property. In simple terms, the Markov property assumes that inferring what to do (taking action) in a specific state can be fully specified by looking at this state and does not depend on other past states.</p>"},{"location":"unit1/lesson1/lesson1.html#elements-of-rl","title":"Elements of RL","text":"<p>In RL, we have mainly four elements that we deal with: states, actions and rewards and the policy. The state space is the space the agent operates in, whether physical or virtual. The state can represent something specific in the environment, an agent configuration or both. The actions are the set of decisions available for the agent to take that affect the state the agent is in and/or cause the environment to respond to it in a certain way, via a reward signal. An RL agent's main aim is to attain, usually via learning, a cohesive policy that allows it to achieve a specific goal of maximising its reward in the long and short terms. </p>"},{"location":"unit1/lesson1/lesson1.html#the-policy","title":"The Policy","text":"<p>This policy \\(\u03c0\\) can take a simple form \\(\\pi(s)=a\\) or symbolically \\(s \u2192 a\\), which means if the agent is in state \\(s\\), then take action \\(a\\). This type of policy is deterministic because the agent will definitely take the action \\(a\\) if it is in state \\(s\\). Below we show an example of a deterministic policy.</p> State Action \\(S_1\\) \\(A_1\\) \\(S_2\\) \\(A_2\\) \\(S_3\\) \\(A_2\\) <p>Another type of policy that we deal with is stochastic policy. A stochastic policy takes the form of \\(\\pi(a|s)\\), which represents the probability of taking action \\(a\\) given that the agent is in state \\(s\\). For such a policy, the agent draws from the set of available actions according to the conditional probability, which we call its policy. The higher the probability of an action, the more likely it will be chosen, when the probability is 1 it means the action will be taken definitely, when it is 0 it means it will not be taken. below we show an example of stochastic policy.</p> State Action Action's Probability \\(S_1\\) \\(A_1\\) .8 \\(S_1\\) \\(A_2\\) .2 \\(S_2\\) \\(A_1\\) .6 \\(S_2\\) \\(A_2\\) .4 \\(S_3\\) \\(A_1\\) 0. \\(S_3\\) \\(A_2\\) 1. <p>A deterministic policy is a special case of a stochastic policy with all of its actions' probabilities being 0 except one action.</p>"},{"location":"unit1/lesson1/lesson1.html#the-reward","title":"The Reward","text":"<p>The reward function can take the simple form of \\(r(s,a)\\) or symbolically \\((s,a) \u2192r\\), which is interpreted as follows: if the agent is in state \\(s\\) and applied action \\(a\\) it obtains a reward \\(r\\). This reward can be actual or expected. The general setting that we deal with is the probabilistic one with the form of \\(p(r|s,a)\\), which provides us with the probability of the agent obtaining reward \\(r\\) given it was in state \\(s\\) and applied action \\(a\\). </p>"},{"location":"unit1/lesson1/lesson1.html#the-transition-probability","title":"The Transition Probability","text":"<p>Another conditional probability that we deal with takes the form of \\(p(s\u2019|s,a)\\), which is the probability of transitioning to state \\(s\u2019\\) given the agent was in state \\(s\\) and applied action \\(a\\). This is called the transition probability. </p>"},{"location":"unit1/lesson1/lesson1.html#the-dynamics-of-an-enviornment","title":"The Dynamics of an Enviornment","text":"<p>Both the transition and reward probabilities can be inferred from a more general probability that specifies the dynamics of the environment. This is a joint conditional probability that takes the form of \\(p(s\u2019,r|s,a)\\). This probability is interpreted as the joint conditional probability of transitioning to state \\(s\u2019\\) and obtaining reward \\(r\\) | given that the agent was in state \\(s\\) and applied action \\(a\\). We will deal mostly with the dynamics in the second lesson of this unit. Bear in mind that obtaining the dynamics is difficult or intractable in most cases except for the simplest environment. Nevertheless, the dynamics are very useful theoretically for understanding the basic ideas of RL.</p>"},{"location":"unit1/lesson1/lesson1.html#the-reward-and-the-task","title":"The Reward and the Task","text":"<p>The reward function is strongly linked to the task that the agent is trying to achieve, this is the minimal information provided for the agent to indicate to it whether it is on the right track or not. We need to be careful not to devise a complicated reward function that directly supervise the agent. This is not only usually unnecessary, but it is also harmful for the agent perfromance, since when doing so, we may be directly solving the problem for the agent which defies the purpos of the RL framework. Instead, we would want the agent to solve the problem by utilising a simple reward signal and interacting with its environment to gain experience and sharpen and improve its decision-making policy. </p> <p>Improving the agent's decision-making involves two things: evaluating its current policy and changing it in a way that will improve its performance, basically collecting as much reward as possible in the long and short terms. This is where the rewards, particualrly the sum of rewards an agent can collect while achieving a task, plays a major role. </p> <p>In RL, we link the policy to maximising the discounted sum of the rewards an agent can obtain while moving towards achieving a task. The reward can be negative, and in this case, the agent will be trying to minimise the sum of negative rewards that it is collecting before terminating. </p> <p>The termination happens when the agent achieves the required task, has taken a pre-specified number of steps or consumed pre-set computational resources. </p>"},{"location":"unit1/lesson1/lesson1.html#types-of-rewards","title":"Types of Rewards","text":"<p>An example of a good simple reward is giving a robot a reward of 1 when it reaches a goal location and 0 otherwise. This is the most generic and most sparse reward we can set for an agent. It basically just tells the agent whether it succeeded or not. This kind of reward demonstrates the essential capabilities and advantage RL can provide in constrat to supervised learning. </p> <p>Another example is giving a robot a negative reward (penalty) of -1 for each step it takes before reaching the goal location, where the agent can be given a reward of 0 (no penalty), or positive reward. It is effectively informing the agent in each step whether it has achieved the task or not. This kind of reward is useful for situations that invloves achieving a task in a minimum number of steps. Therefore, it is a good fit for shortest path problems and navigation tasks.</p> <p>Both of these rewards have their advantages, and we will explore and experiment with them in our exercises. The advantage of the first type is its simplicity, generality and inforced sparsity, but it can take longer to explore the environment and longer to populate its estimations. The advantage of the second type is also its relative generality,albeit less generic than the first, and that it provides intermediate general information that the agent can immediately utilise to improve its policy before achieving the task or reaching the goal location. The second type is specifically useful for online learning and when we want to alleviate the agent from having to change its starting position to cover all possible starts. The first type is useful for studying the capabilities of a learning algorithm with minimum information.</p>"},{"location":"unit1/lesson1/lesson1.html#the-return","title":"The Return","text":"<p>The sum of rewards from a specific state to the end of the task is called the return. Because we do not know how long the agent may take to achieve the task and to fairly maximise the rewards and allow for variability, we discount the rewards so that more recent rewards have more effect than later rewards. Nevertheless, our aim is to maximise the rewards in the long run. To be more explicit, we call the sum of discounted rewards from the current state to the end of the task the return that the agent will obtain in the future- the whole idea is to predict these rewards and be able to collect as much as possible.</p>"},{"location":"unit1/lesson1/lesson1.html#the-task","title":"The Task","text":"<p>The task we give the agent can be a continuous, infinite interaction with the environment. It can also take the form of a task with a specific goal or termination state, and when it is reached, the task is naturally aborted and repeated or reattempted. The former is called a continuous task, while the latter is called an episodic task. Episodic tasks have a start and end, while continuous tasks have a start but never end. The horizon (the number of steps) of episodic tasks is finite, while the horizon for continuous tasks is infinite. It turns out that discounting is necessary for theoretical guarantees for continuous tasks, which must be strictly &lt; 1, while for episodic tasks, it can be set to 1. We will deal mostly with episodic tasks.</p>"},{"location":"unit1/lesson1/lesson1.html#the-value-function-and-action-value-function","title":"The Value Function and Action-Value Function","text":"<p>The function that specifies each state's return when the agent follows a specific policy is called the value function and is denoted as \\(v(s)\\). On the other hand, we call the function that specifies the return of the current state given that the agent takes a specific action \\(a\\) and then just follows a specific policy, the action-value function and is denoted as \\(q(s,a)\\). These two functions provide a great utility for us in guiding our search for an optimal policy. The action-value function can be directly utilised to provide a policy that greedily chooses the action with the maximum value; when we do so, we call the algorithms that follow this pattern a value-function algorithm. Alternatively, we can maximise the policy directly without having to maximise the expected return first. These types of algorithms that do so are called policy-gradient algorithms and depend on approximation.</p> <p>We will largely deal with two types of algorithms: tabular algorithms, which use a tabular representation of the value function \\(v\\) and the action-value function \\(q\\). These will be covered in the first three units. In the subsequent units, we cover the second type of algorithms that deal with function approximation, where representing the state and actions in a table is intractable or impractical. In these algorithms, we generalise the tabular algorithms that we cover in the first three units to be able to use the models that we covered in machine learning, such as linear regression models or neural networks</p>"},{"location":"unit1/lesson1/lesson1.html#tabular-representation-and-example","title":"Tabular Representation and Example","text":"<p>In the first three units, you will learn about the main ideas of reinforcement learning that use lookup tables. These tables identify a certain action that will be taken in a certain state and are called a policy. Our main concern is to design algorithms that learn a suitable policy.</p> <p>A Policy Lookup Table For Commuting to Work</p> State Action have energy and have time walk have energy and no   time cycle no  energy  and have time take a bus no  energy  and no   time take a taxi"},{"location":"unit1/lesson1/lesson1.html#unit-overview","title":"Unit Overview","text":"<p>More formally, in the first three units, we will study important reinforcement learning algorithms that use tabular policy representation. These algorithms learn a lookup table that identifies a suitable action that can be taken for a certain state. Our main concern is to design practical and efficient algorithms that can learn an optimal policy either via direct interaction with an environment, via an environment model that captures the dynamics of the environment, or both. An optimal policy is a policy that maximises the sum of discounted rewards obtained by following this policy.</p> <p>The main underlying framework we assume is a Markov Decision Process (MDP). In a nutshell, as we said earlier this framework assumes that the probability of moving to the next state is only dependent on the current state and not on past states.</p> <p>We start by covering non-associated problems, such as K-armed Bandit. These problems have no states. This will help us focus on the action space as it will be isolated from the state's effect. We then study how to solve MDP problems using Dynamic Programming (DP). DP assumes that we have a model of the environment that the agent is acting on. By model, we mean the dynamics or probabilities of landing in a state and obtaining a specific reward given an action and a previous state. This kind of conditional probability provides a comprehensive framework to reach an optimal policy, but it is hard to obtain in the real world. We then reside in sample methods, particularly Monte Carlo. This method allows us to gather samples of an agent running, making environmental decisions, and collecting rewards. We use averages to obtain an estimate of the discounted returns of an episode. We conclude our units by developing suitable core classes that allow us to study and demonstrate the advantages and disadvantages of these and other methods.</p>"},{"location":"unit1/lesson2/lesson2.html","title":"Lesson 2- Understanding Q via K-armed Bandit","text":"<p>In this lesson, you will learn about the k-armed bandit problem and its applications in reinforcement learning (RL). This problem is useful for understanding the basics of RL, particularly how an algorithm can learn an action-value function, commonly denoted as Q in RL.</p>"},{"location":"unit1/lesson2/lesson2.html#learning-outcomes","title":"Learning Outcomes","text":"<p>By the end of this lesson, you will:</p> <ol> <li>Understand the role the action-value function plays in RL and its relationship with a policy.</li> <li>Appreciate the difference between stationary and non-stationary problems.</li> <li>Understand how to devise a sample-averaging solution to approximate an action-value function.</li> <li>Appreciate the different types of policies and the role a policy plays in RL algorithms.</li> </ol> <p>In this lesson, we develop the foundational concepts of actions and policies, which are the key elements that distinguish RL from other machine learning sub-disciplines. We study a simple yet effective problem: the k-armed bandit. This problem can be applied to scenarios such as a medical specialist deciding which treatment to administer to a patient from a set of medications, some of which they are trying for the first time (exploring).</p> <p>Our toy problem is similar to the classic bandit problem, but with the assumption that there is a set of k actions the agent can choose from. The goal is to develop an effective policy that allows the agent to maximize its returns (wins). The bandit is assumed to have a Gaussian distribution centered around a mean reward, which differs for each action (arm). Each time an arm is pulled (or an action is taken, as we say in RL terminology), the bandit will return a reward (positive or negative) by drawing from its Gaussian reward distribution. The agent's task is to identify which action has the highest mean reward and consistently choose it to maximize its wins. Note that the distributions are fixed and do not change, though we will relax this assumption later.</p> <p>Now, let\u2019s get started!</p>"},{"location":"unit1/lesson2/lesson2.html#motivating-example","title":"Motivating Example","text":"<p>Let us assume that we have an armed bandit with two levers.</p>"},{"location":"unit1/lesson2/lesson2.html#scenario-a","title":"Scenario A","text":"<ul> <li>We have a deterministic reward function that returns a reward of -5 for action \\(a_1\\) (pulling bandit 1).</li> <li>We have a deterministic reward function that returns a reward of 0 for action \\(a_2\\) (pulling bandit 2).</li> </ul> <p>Question: What is the optimal policy for this bandit?</p>"},{"location":"unit1/lesson2/lesson2.html#scenario-b","title":"Scenario B","text":"<ul> <li>We have a nondeterministic reward function that returns a reward of either -5 or 15, each with an equal probability of 0.5 for action \\(a_1\\) (pulling bandit 1).</li> <li>We have a nondeterministic reward function that returns a reward of either 0 or 10, each with an equal probability of 0.5 for action \\(a_2\\) (pulling bandit 2).</li> </ul> <p>Questions:</p> <ol> <li>What is the net overall reward for actions \\(a_1\\) and \\(a_2\\)?</li> <li>What is the optimal policy for this bandit?</li> <li>How many optimal policies do we have for this bandit?</li> </ol>"},{"location":"unit1/lesson2/lesson2.html#scenario-c","title":"Scenario C","text":"<ul> <li>We have a nondeterministic reward function that returns a reward of either -5 or 15, with probabilities of 0.4 and 0.6 respectively for action \\(a_1\\) (pulling bandit 1).</li> <li>We have a nondeterministic reward function that returns a reward of either 0 or 10, each with a probability of 0.5 for action \\(a_2\\) (pulling bandit 2).</li> </ul> <p>Questions:</p> <ol> <li>What is the net overall reward for actions \\(a_1\\) and \\(a_2\\)?</li> <li>What is the optimal policy for this bandit?</li> <li>How many optimal policies do we have for this bandit?</li> <li>Can you find a way to represent this?</li> </ol> <p>The figure below gives an insight about some of the above questions. Each line represents a bandit Q1 and Q2.</p> <p></p> <p>As can be seen, the two bandit functions intersect with each other at a probability of 0.5, meaning they are equivalent at this probability. For other probabilities, Bandit 1 is superior for \\( pr &gt; 0.5 \\) (and hence the optimal policy will be to select this bandit always), while Bandit 2 is superior for \\( pr &lt; 0.5 \\) (and hence the optimal policy will be to select this bandit always). However, bear in mind that we do not know the underlying probability beforehand, and we would need to try out both bandits to estimate their corresponding value functions in order to come up with a suitable policy.</p> <p>Now, let's move on to covering the different concepts of the multi-armed bandit in more detail.</p> <p>We proceed by developing simple functions for: 1. Averaging rewards for statinary policy. 2. Moving Average of rewards for non-stationary policy.</p>"},{"location":"unit1/lesson2/lesson2.html#averaging-the-rewards-for-greedy-and-greedy-policies","title":"Averaging the Rewards for Greedy and \u03b5-Greedy Policies","text":"<p>A greedy policy is a simple policy that always selects the action with the highest action-value. On the other hand, an \u03b5-greedy policy is similar to a greedy policy but allows the agent to take random exploratory actions from time to time. The percentage of exploratory actions is designated by \u03b5 (epsilon). Typically, we set \u03b5 to 0.1 (10%) or 0.05 (5%). A third type of greedy policy is the dynamic \u03b5-greedy policy, which anneals or decays the exploration factor (\u03b5) over time. In practice, the \u03b5-greedy policy generally works well and often better than more sophisticated policies that aim to strike a balance between exploration and exploitation (where taking the greedy action is called exploitation, and taking other actions is called exploration). Regardless of the approach, we need to allow for some exploratory actions; otherwise, it would not be possible for the agent to improve its policy.</p> <p>Below, we show an implementation of a bandit function that uses an \u03b5-greedy policy. <code>nA</code> denotes the number of actions (the number of arms to be pulled). Since we are only dealing with actions, the Q-function has the form of Q(a). The armed bandit is a non-associative problem, meaning we do not deal with states. Later, we will address associative problems, where Q(s, a) has two inputs: the state and the action.</p> <p>We now create a function that takes a bandit (in the form of a set of rewards, each corresponding to an action) and generates a value Q that quantifies the value/benefit obtained by taking each action. This is a simple improvement over the earlier code, where we calculated the expected reward of an action. This time, we choose actions randomly instead of uniformly, so we need to keep track of each action. At the end, we divide the sum of the obtained rewards by the count of each action to compute the average, which serves as a good estimator of the expected reward.</p> <pre><code>def Q_bandit_fixed(bandit, \u03b5=.1, T=1000):\n    r  = bandit      # the bandit is assumed to be a set of rewards for each action\n    nA = len(r)      # number of actions\n    Q = np.zeros(nA) # action-values\n    C = np.ones(nA)  # action-counts\n    avgR = np.zeros(T+1)\n\n    for t in range(T):\n        # \u03b5-greedy action selection (when \u03b5=0 this turns into a greedy selection)\n        if rand()&lt;= \u03b5: a = randint(nA)\n        else:          a = (Q/C).argmax()\n\n        Q[a] += r[a]\n        C[a] += 1\n        avgR[t+1] = (t*avgR[t] + r[a])/(t+1)\n\n\n    plt.plot(avgR, label='average reward \u03b5=%.2f'%\u03b5)\n    plt.legend()\n    return Q/C\n</code></pre> <p>Let us see how the Q_bandit_fixed will learn to choose the best action that yields the maximum returns.</p> <p><pre><code>Q_bandit_fixed(bandit=[.1, .2, .7])\n</code></pre>     array([0.09772727, 0.19874214, 0.699125  ])</p> <p></p> <p>As we can see it has improved to more than 0.6.</p> <p>Let us see how the completely greedy policy would do on average:</p> <p><pre><code>Q_bandit_fixed(bandit=[.1, .2, .7], \u03b5=0)\n</code></pre>     array([0.0999001, 0.       , 0.       ])</p> <p></p> <p>As we can see it could not improve beyond 0.1</p> <p>The main restriction in Q_bandit_fixed( ) function is that we assume that the reward function is fixed (hence the name Q_bandit_fixed). I.e., each action will receive a specific reward that does not change. This made the above solution a bit excessive since we could have just summed the rewards and then took their max. Nevertheless, this is useful as a scaffolding for our next step.</p> <p>In the next section we develop a more general armed-bandit function that allows for the reward to vary according to some unknown distribution. The Q_banditAvg function will learn the distribution and find the best action that will allow it to obtain a maximal reward on average, similar to what we have done here.</p>"},{"location":"unit1/lesson2/lesson2.html#10-armed-bandet-testbed-estimating-q-via-samples-average","title":"10-armed Bandet Testbed: Estimating Q via Samples Average","text":"<p>Remember that Q represents the average/expected reward of an action from the start up until time step \\(t\\) exclusive. Later we will develop this idea to encompass what we call the expected return of an action. q* (q\u02e3 in the code) represents the actual action-values for the armeds which are a set of reward that has been offset by a normal distribution randn(). This guarantees that on average the rewards of an action a is q*[a] but it will make it not easy for an observer to know exactly what the expected reward is.</p>"},{"location":"unit1/lesson2/lesson2.html#generate-the-experiencesampling","title":"Generate the experience(sampling)","text":"<p>generate an experience (rewards)</p> <p>We use a function that get us a multivariate normal distribution of size k. Below see how we will generate a sample bandit with all of its possible data and plot it.</p> <pre><code>def generate_a_bandit_data(q\u02e3, T):\n    nA = q\u02e3.shape[0]\n    r = multivariate_normal(q\u02e3, np.eye(nA), T)\n    plt.violinplot(r)\n    plt.show()\n\ngenerate_a_bandit_data(normal(0, 1, 10), T=100)\n</code></pre> <p></p>"},{"location":"unit1/lesson2/lesson2.html#learning-the-bandit-q-action-values","title":"Learning the bandit Q action-values","text":"<p>Now we turn our attention to learning the Q function for an unknown reward distribution. Each action has its own Gaussian distribution around a mean but we could use other distributions. The set of means are themselves drawn from a normal distribution of mean 0 and variance of 1.</p> <pre><code># learn the Q value for bandit and use it to select the action that will win the most reward\ndef Q_banditAvg(q\u02e3, \u03b5=.1, T=1000):    \n\n    # |A| and max(q*)\n    nA   = q\u02e3.shape[0]            # number of actions, usually 10\n    amax = q\u02e3.argmax()            # the optimal action for this bandit\n\n    # stats.\n    r  = np.zeros(T)                  # reward at time step t\n    a  = np.zeros(T, dtype=int)       # chosen action at time step t, needs to be int as it will be used as index\n    oA = np.zeros(T)                  # whether an optimal action is selected at time step t\n\n    # estimates\n    Q = np.zeros(nA)                  # action-values all initialised to 0\n    N = np.ones(nA)                   # actions selection count\n\n\n    for t in range(T):\n        # action selection is what prevents us from vectorising the solution which must reside in a for loop\n        if rand()&lt;= \u03b5: a[t] = randint(nA)       # explore\n        else:          a[t] = (Q/N).argmax()    # exploit\n\n        # update the stats.\n        r[t]  = bandit(a[t], q\u02e3)\n        oA[t] =        a[t]==amax\n\n        # update Q (action-values estimate)\n        N[a[t]] += 1\n        Q[a[t]] += r[t] \n\n    return r, oA\n</code></pre> <p>Let us now run this function and plot one 10-armed bandits</p> <pre><code>R, oA = Q_banditAvg(normal(0, 1, 10) , \u03b5=.1, T=1000)\n\nplt.gcf().set_size_inches(16, 3.5)\nplt.subplot(121).plot(R); plt.xlabel('Steps'); plt.ylabel('Average rewrads')\nplt.subplot(122).plot(oA,'.'); plt.xlabel('Steps'); plt.ylabel('%Optimal action')\n</code></pre> <p></p> <p>Note how the % of optimal actions for one trial (run) takes either 1 or 0. This figure to the left seems not be conveying useful information. However when we average this percentage over several runs we will see a clear learning pattern. This is quite common theme in RL. We often would want to average a set of runs/experiments due to the stochasticity of the process that we deal with.</p>"},{"location":"unit1/lesson2/lesson2.html#multiple-runs-aka-trials","title":"Multiple runs (aka trials)","text":"<p>We need to average multiple runs to obtain a reliable unbiased results that reflect the expected performance of the learning algorithm. We do that via running the same function or algorithm multiple times, which is what the Q_bandits_runs function does. We do not show the code, we only show </p> <p>Note that we obtain different set of 10-bandit distributions and conduct an experimental run on them. Because all of them are normally standard distribution their sums of rewards (values) converges to the same quantity around 1.5.</p>"},{"location":"unit1/lesson2/lesson2.html#action-selection-policy-comparison","title":"Action Selection (Policy) Comparison:","text":"<p>Now we are ready to compare between policies with different exploration rates \u03b5. Note that \u03b5 kw(keyword argument) has been passed on to the Q_bandit() function from the Q_bandits_runs() function.</p> <pre><code>Q_bandits_runs(\u03b5=.1,  label='\u03b5=.1',  Q_bandit=Q_banditAvg)\nQ_bandits_runs(\u03b5=.01, label='\u03b5=.01', Q_bandit=Q_banditAvg)\nQ_bandits_runs(\u03b5=.0,  label='\u03b5=.0' , Q_bandit=Q_banditAvg)\n</code></pre> <p></p> <p>As we can see the \u03b5=.1 exploration rate seems to give us a sweet spot. Try \u03b5=.2 and see the effect. This empirically indicates that indeed we need to allow the agent to explore in order to come up with a viable optimal or close to optimal policy.</p>"},{"location":"unit1/lesson2/lesson2.html#incremental-implementation","title":"Incremental Implementation","text":"<p>If we look at the sum</p> \\[ % \\begin{aligned} Q_{t+1}  = \\frac{1}{t}\\sum_{i=1}^{t}R_i = \\frac{1}{t}\\left(\\sum_{i=1}^{t-1}R_i + R_t\\right)          = \\frac{1}{t}\\left((t-1)\\frac{\\sum_{i=1}^{t-1}R_i}{t-1} + R_t\\right)          = \\frac{1}{t}\\left(\\left(t-1\\right)Q_t + R_t\\right)  \\] \\[ Q_{t+1} = Q_t + \\frac{1}{t}\\left(R_t - Q_t\\right)  \\] <p>We can see that we can write the estimate in an incremental form that allows us to update our estimate \\(Q_t\\) instead of recalculate the sum in each time step. This is very handy when it comes to efficiently implement an algorithm to give us the sum. Further, it  turns out that it also has other advantages. To realise this, note that the \\(\\frac{1}{t}\\) diminishes when \\(t\\) grows, which is natural for averages. But if we want the latest rewards to have a bigger impact (weights) then we can simply replace this fraction by a constance\\(\\alpha\\) to obtain the following incremental update</p> \\[     Q_{t+1} = Q_t + \\alpha\\left(R_t - Q_t\\right) \\] <p>Note that incremental updates plays a very important role in RL and we will be constantly seeking them due to their efficiency in online application.</p> <p>Below we show the results of running a code that was designed to capture the ideas of incremental implementation via a function called Q_banditN. We do not show the code of Q_banditN here</p> <pre><code>#Q_bandits_runs(\u03b5=.2)\nQ_bandits_runs(\u03b5=.1,  label='\u03b5 =.1',  Q_bandit=Q_banditN)\nQ_bandits_runs(\u03b5=.01, label='\u03b5 =.01', Q_bandit=Q_banditN)\nQ_bandits_runs(\u03b5=.0,  label='\u03b5 =.0',  Q_bandit=Q_banditN)\n</code></pre> <p></p>"},{"location":"unit1/lesson2/lesson2.html#non-stationary-problems","title":"Non-stationary Problems","text":"<p>The limitation of the above implementation is that it requires actions counts and when the underlying reward distribution changes (non-stationary reward distribution) it does not respond well to take these changes into account. A better approach when we are faced with such problems is to use a fixed size step &lt;1 instead of dividing by the actions count. This way, because the step size is small the estimate gets updated when the underlying reward distribution changes. Of course this means that the estimates will keep changing even when the underlying distribution is not changing, however in practice this is not a problem when the step size is small enough. This effectively gives more weights to recent updates which gives a good changes-responsiveness property for this and similar methods that use a fixed size learning step \\(\\alpha\\).</p> <pre><code># returns one of the max Q actions; there is an element of stochasticity in this policy\ndef greedyStoch(Q):   \n    return choice(np.argwhere(Q==Q.max()).ravel())\n\n\n# returns the first max Q action most of the time (1-\u03b5)\ndef \u03b5greedy(Q, \u03b5):\n    return Q.argmax() if rand() &gt; \u03b5 else randint(Q.shape[0])\n\n\ndef \u03b5greedyStoch(Q, \u03b5):\n    return greedyStoch(Q) if rand() &gt; \u03b5 else randint(Q.shape[0])\n</code></pre> <pre><code>def Q_bandit\u03b1(q\u02e3,  \u03b1=.1, \u03b5=.1, T=1000, q0=0, policy=\u03b5greedy):\n    nA, amax, r, a, Q, _ = bandit_init(q\u02e3, T, q0)\n\n    for t in range(T):\n        # using a specific policy\n        a[t] = policy(Q, \u03b5)               \n\n        # get the reward from bandit\n        r[t]  = bandit(a[t], q\u02e3)\n\n        # update Q (action-values estimate)\n        Q[a[t]] += \u03b1*(r[t] - Q[a[t]])  \n\n\n    return r, a==amax\n</code></pre> <p>Note that the majority of RL problem are actually non-stationary. This is because, as we shall see later, when we gradually move towards an optimal policy by changing the Q action-values, the underlying reward distribution changes in response to taking actions that are optimal according to the current estimation. This is also the case here but in a subtle way.</p>"},{"location":"unit1/lesson2/lesson2.html#compare-different-learning-rates","title":"Compare different learning rates","text":"<p>Let us compare different learning rates \u03b1 to see how our Q_bandits() function reacts to them. </p> <pre><code>Q_bandits_runs(\u03b1=.1,  label='\u03b1=.1',  Q_bandit=Q_bandit\u03b1)\nQ_bandits_runs(\u03b1=.01, label='\u03b1=.01', Q_bandit=Q_bandit\u03b1)\nQ_bandits_runs(\u03b1=.5,  label='\u03b1=.5',  Q_bandit=Q_bandit\u03b1)\nQ_bandits_runs(\u03b1=.0,  label='\u03b1=.0',  Q_bandit=Q_bandit\u03b1)\n</code></pre> <p></p>"},{"location":"unit1/lesson2/lesson2.html#policies-exploration-vs-exploitation","title":"Policies: Exploration vs. Exploitation","text":"<p>Getting the right balance between exploration and exploitation is a constant dilemma in RL. One simple strategy as we saw earlier is to explore constantly occasionally \u03b5% of the time! which we called \u03b5-greedy. Another strategy is to insure that when we have multiple actions that are greedy we chose ebtween them equally and not bias one over the other. This is what we do in the greedyStoch policy below.</p> <p>Let us now compare different exploration rates for this learning function. As before we show the results only not the code.</p> <p><pre><code>Q_bandits_runs(\u03b5=.1,  label='\u03b5=.1',  Q_bandit=Q_bandit\u03b1, T=5000, runs=2000)\nQ_bandits_runs(\u03b5=.01, label='\u03b5=.01', Q_bandit=Q_bandit\u03b1, T=5000, runs=2000)\nQ_bandits_runs(\u03b5=.0,  label='\u03b5=.0',  Q_bandit=Q_bandit\u03b1, T=5000, runs=500)\n</code></pre> </p>"},{"location":"unit1/lesson2/lesson2.html#optimistic-initial-values-as-an-exploration-strategy","title":"Optimistic Initial Values as an Exploration Strategy","text":"<p>It turns out that we can infuse exploration in the RL solution by optimistically initialising the Q values. This encourages the agent to explore due to its disappointment when its initial Q values are not matching the reward values that are coming from the ground (interacting with the environment). This intrinsic exploration motive to explore more actions at the start, vanishes with time when the Q values become more realistic. </p> <p>This is a good and effective strategy for exploration. But of course it has its limitations, for example it does not necessarily work for if there a constant or renewed need for exploration. This could happen either when the task or the environment are changing. Below, we show the effect of optimistically initiating the Q values on the 10-armed bandit testbed. We can clearly see that without exploration i.e. when \u03b5=0 and Q=5 initial values outperformed the exploratory policy \u03b5=.1 with Q=0 initial values.</p> <p>Ok, we will apply the same principle to stochastically return one of the max Q actions which is coded in greedyStoch() policy. This type of policy will prove useful later when we deal with control.</p> <pre><code>Q_bandits_runs(\u03b5=.1, q0=0, label='\u03b5=.1  Realistic Q=0',  Q_bandit=Q_bandit\u03b1, policy=\u03b5greedyStoch)\nQ_bandits_runs(\u03b5=0 , q0=5, label='\u03b5=0   Optimistic Q=5', Q_bandit=Q_bandit\u03b1, policy=\u03b5greedyStoch)\n</code></pre> <p></p> <p>As we can see above the optimistic initialization has actually beaten the constant exploration rate and it constitutes a very useful trick for us to encourage the agent to explore while still acting greedily. Of course we can combine both strategies and we will leave this for you as a task. We will use this trick in our coverage of RL in later lessons.</p>"},{"location":"unit1/lesson2/lesson2.html#conclusion","title":"Conclusion","text":"<p>In this lesson you have learned about the importance of the action value function Q and stationary and non-stationary reward distribution and how we can devise a general algorithms to address them and we concluded by showing an incremental learning algorithm to tackle the k-armed bandit problem. You have seen different exploration strategy and we extensively compared between exploration rates and learning rates for our different algorithms.</p>"},{"location":"unit1/lesson2/lesson2.html#further-reading","title":"Further Reading","text":"<p>For further information refer to chapters 1 and 2 of the rl book. </p>"},{"location":"unit1/lesson2/lesson2.html#your-turn","title":"Your Turn","text":"<p>Worksheet2 implement the above concepts and more. Please experiment with the code and run it to get familiar with the essential concepts presented in the lessons.</p>"},{"location":"unit1/lesson3/lesson3--.html","title":"Lesson3","text":"<p>To help you, Abdulrahman recorded a set of videos that covers important concepts of RL.</p>"},{"location":"unit1/lesson3/lesson3--.html#markov-decision-processes-mdp","title":"Markov Decision Processes (MDP)","text":"<p>We provided you with a simple code to in the worksheet below to demosntrate how the G changes with time steps.</p> <p><pre><code>G = 0\nR = 1\n\u03b3 = 0.9\nT = 100\nfor t in range(T,0,-1):\n    if t&gt; 70: print('G_',t,'=',round(G,3))\n    G = R + \u03b3*G \n</code></pre>     G_ 100 = 0     G_ 99 = 1.0     G_ 98 = 1.9     G_ 97 = 2.71     G_ 96 = 3.439     ...</p> <p><pre><code>G = 0\n\u03b3 = 0.9 # change to 1 to see the effect\nT = 100\nfor t in range(T,1,-1): # note that if we use a forward loop, our calculations will be all wrong although the code runs\n    if t&gt;70: print('G_',t,'=',round(G,2), round(\u03b3**(T-t-1),2) if t&lt;T else 0)\n    R=1 if t==T else 0\n    G = R + \u03b3*G\n</code></pre>     G_ 100 = 0 0     G_ 99 = 1.0 1.0     G_ 98 = 0.9 0.9     G_ 97 = 0.81 0.81     ...</p>"},{"location":"unit1/lesson3/lesson3--.html#bellman-equation-for-v-and-q","title":"Bellman Equation for V and Q","text":""},{"location":"unit1/lesson3/lesson3--.html#bellman-optimality-equations-for-v-and-q","title":"Bellman Optimality Equations for V and Q","text":""},{"location":"unit1/lesson3/lesson3--.html#grid-world-environments","title":"Grid World Environments","text":"<p>Ok, so now we are ready to tackle the practicals, please go ahead and download the worksheet and run and experiement with the provided code to build some grid world environments and visualise them and make a simple robot agent takes some steps/actions within these environments!.</p> <p>You will need to download a python library (Grid.py) that we bespokley developed to help you run RL algorithms on toy problems and be abel to easily visualise them as needed, the code is optimised to run efficiently and you will be able to use these environmnets to test different RL algorithms extensively. Please place the library in the same directory of the worksheet. In general it would be a good idea to place all worksheets and libraries provided in one directory. This will make importing and runing code easier and more streamlined.</p> <p>In a grid world, we have a set of cells that the agent can move between them inside a box. The agent can move left, right, up and down. We can also allow the agent to move diagonally, but this is uncommon. </p> <p>We needed to be as efficient as possible, and hence we have chosen to represent each state by its count, where we count from the bottom left corner up to the right top corner, and we start with 0 up to nS-1 where nS is the number of states. This will allow us to streamline the process of accessing and storing a state and will be of at most efficiency. We also deal with actions similarly, i.e. each action of the nA actions is given an index 0..nA-1. For the usual grid, this means 0,1,2,3 for actions left, right, up and down. We represent a 2-d grid by a 1-d array, and so care must be taken on how the agent moves between cells. </p> <p>Moving left or right seems easy because we can add or subtract from the current state. But when the agent is on the edge of the box, we cannot allow for an action that takes it out of the box. So if the agent is on the far right, we cannot allow it to go further to the right. To account for this issue, we have written a valid() function to validate an action. Moving up and down is similar, but we need to add and subtract a full row, which is how many columns we have in our grid. the valid() function checks for the current state and what would be the next state, and it knows that an agent will overstep the boundary as follows: if s%cols!=0, this means that the agent was not on the left edge and executing a right action (s+a)%cols==0 will take it to the left edge. This means it was on the right edge and wanted to move off this right edge. Other checks are similar. We have also accounted for moving diagonally so the agent will not overstep the boundaries.</p> <p>We have also accounted for different reward schemes that we might want to use later in other lessons. These are formulated as an array of 4 elements [intermediate, goal1, goal2, cliff] the first reward represents the reward the agent obtains if it is on any intermediate cell. Intermediate cells are non-terminal cells. Goals or terminal states are those that a task would be completed if the agent steps into them. By setting the goals array, we can decide which cells are terminal/goals. As we can see, there are two goals, this will allow us to deal with all the classical problems we will tackle in our RL treatments, but we could have set up more. So, our reward array's second and third elements are for the goals. The last entry is for a cliff. A cliff cell is a special type of non-terminal cell where the agent will emulate falling off a cliff and usually is given a high negative reward and then will be hijacked and put in its start position when it steps into these types of cells. These types of cells are non-terminal in the sense that the agent did not achieve the task when it went to them, but they provoke a reset of the agent position with a large negative reward.</p> <p>The most important function in our class is the step(a) function. An agent will take a specific action in the environment via this function. Via this function, we return the reward from our environment and a flag (done) indicating whether the task is accomplished. This makes our environment compatible with the classic setup of an OpenAI Gym Atari games, which we deal with towards the end of our RL treatment.</p>"},{"location":"unit1/lesson3/lesson3--.html#conclusion","title":"Conclusion","text":"<p>In this lesson, we covered the basics of RL functions and concepts that we will utilise in other lessons. We also provided you with a environment that you can directly utilise to build simple grid world environments. Please note that you are not required to study the Gris.py file or understand how the grid is programmatically built, but you need to understand how it operates.!</p>"},{"location":"unit1/lesson3/lesson3.html","title":"3. MDP","text":""},{"location":"unit1/lesson3/lesson3.html#lesson-3-markov-decision-processes-dynamics-and-bellman-equaitons","title":"Lesson 3- Markov Decision Processes, Dynamics and Bellman Equaitons","text":"<p>Learning outcomes</p> <ol> <li>understand MDP and its elements</li> <li>understand the return for a time step \\(t\\)</li> <li>understand the expected return of a state \\(s\\)</li> <li>understand the Bellman optimality equations</li> <li>become familiar with the different types of grid world problems</li> </ol>"},{"location":"unit1/lesson3/lesson3.html#1-markov-decision-process-mdp","title":"1. Markov Decision Process (MDP)","text":"<p>A Markov Decision Process (MDP) provides a mathematical framework to model decision-making problems where an agent interacts with an environment. It is characterized by a tuple \\( (S, A, P, R, \\gamma) \\) where:</p> <ul> <li>\\( S \\) is the set of states in the environment.</li> <li>\\( A \\) is the set of actions available to the agent.</li> <li>\\( P(s' | s, a) \\) is the transition function, representing the probability of transitioning from state \\( s \\) to state \\( s' \\) after taking action \\( a \\).</li> <li>\\( R(s, a, s') \\) is the reward function, representing the immediate reward received when transitioning from state \\( s \\) to state \\( s' \\) after taking action \\( a \\).</li> <li>\\( \\gamma \\) is the discount factor, a value between 0 and 1 that determines the importance of future rewards relative to immediate rewards.</li> </ul> <p>The MDP framework is central to reinforcement learning (RL), as it allows the agent to plan and optimize its actions over time to maximize the expected return.</p>"},{"location":"unit1/lesson3/lesson3.html#markov-property","title":"Markov Property","text":"<p>The Markov Property asserts that the future state depends only on the current state and action, not on any previous states or actions. This is a core assumption in MDPs and ensures that the system has no memory of past actions or states.</p> <p>Formally:</p> \\[ P(s_{t+1} | s_t, a_t, \\dots, s_0, a_0) = P(s_{t+1} | s_t, a_t) \\]"},{"location":"unit1/lesson3/lesson3.html#stationarity","title":"Stationarity","text":"<p>In many MDPs, the transition and reward functions are stationary, meaning that they do not change over time. This ensures that the transition probabilities and rewards are the same at every time step.</p> <p>Formally:</p> \\[ P(s' | s, a) = P(s' | s, a) \\quad \\forall t \\]"},{"location":"unit1/lesson3/lesson3.html#deterministic-vs-stochastic-dynamics","title":"Deterministic vs. Stochastic Dynamics","text":"<ul> <li>Deterministic Dynamics: If the transition function \\( P(s' | s, a) \\) always produces the same next state, the system is deterministic.</li> <li>Stochastic Dynamics: If \\( P(s' | s, a) \\) is probabilistic, the system is stochastic.</li> </ul>"},{"location":"unit1/lesson3/lesson3.html#reward-structure","title":"Reward Structure","text":"<p>The reward function \\( R(s, a, s') \\) can either be deterministic (fixed reward) or stochastic (random reward). The structure of the rewards influences the agent's behavior and learning.</p>"},{"location":"unit1/lesson3/lesson3.html#irreducibility-and-aperiodicity","title":"Irreducibility and Aperiodicity","text":"<p>For algorithms like Value Iteration, it is important that the MDP is irreducible (all states are reachable from any other state) and aperiodic (there are no cycles of fixed lengths that prevent convergence).</p>"},{"location":"unit1/lesson3/lesson3.html#2-policy-and-its-stationarity","title":"2. Policy and its Stationarity","text":"<p>A policy in reinforcement learning is a strategy or function that defines the agent's actions at each state in an environment. Mathematically, a policy is often represented as \\( \\pi(a|s) \\), where \\( s \\) is a state and \\( a \\) is an action. The policy \\( \\pi(a|s) \\) gives the probability of taking action \\( a \\) when in state \\( s \\). </p>"},{"location":"unit1/lesson3/lesson3.html#stationary-policy","title":"Stationary Policy","text":"<p>A stationary policy is one where the action probabilities depend only on the current state and remain constant over time. Formally, a stationary policy satisfies: [ \\pi_t(a|s) = \\pi(a|s) \\quad \\text{for all time steps} \\, t ] This means the policy does not change as the environment evolves. This is common in many reinforcement learning settings where the dynamics of the problem do not change over time.</p>"},{"location":"unit1/lesson3/lesson3.html#non-stationary-policy","title":"Non-Stationary Policy","text":"<p>In contrast, a non-stationary policy is one where the action probabilities can change with time: [ \\pi_t(a|s) \\neq \\pi_{t'}(a|s) \\quad \\text{for some} \\, t \\neq t' ] This occurs when the policy is adapted or modified based on external factors, such as learning or changes in the environment. A non-stationary policy is useful in situations where the environment or the agent's understanding of it evolves over time.</p>"},{"location":"unit1/lesson3/lesson3.html#2-transition-and-reward-dynamics","title":"2. Transition and Reward Dynamics","text":""},{"location":"unit1/lesson3/lesson3.html#transition-dynamics","title":"Transition Dynamics","text":"<p>The transition dynamics describe how the environment behaves when the agent takes an action in a given state. The transition function \\( P(s' | s, a) \\) specifies the probability of transitioning from state \\( s \\) to state \\( s' \\) when the agent takes action \\( a \\).</p> <p>Formally, the transition function is expressed as:</p> \\[ P(s' | s, a) = \\mathbb{P}(s_{t+1} = s' | s_t = s, a_t = a) \\] <p>Where: - \\( s_t \\) is the state at time step \\( t \\), - \\( a_t \\) is the action taken at time step \\( t \\), - \\( s_{t+1} \\) is the next state after taking action \\( a_t \\) from \\( s_t \\).</p>"},{"location":"unit1/lesson3/lesson3.html#key-properties-of-transition-dynamics","title":"Key Properties of Transition Dynamics:","text":"<ul> <li>Stochastic Nature: Transition dynamics are typically stochastic, meaning that taking the same action in the same state may result in different next states with some probability.</li> <li>Markov Property: The system satisfies the Markov Property, meaning the next state depends only on the current state and action, not on the history of previous states or actions.</li> </ul> <p>Example: In a grid world, if the agent is at state \\( s = (2, 2) \\) and takes action \\( a = \\text{move left} \\), the transition probability might be deterministic:</p> \\[ P(s' | (2, 2), \\text{move left}) =  \\begin{cases}  1 &amp; \\text{if } s' = (1, 2) \\\\ 0 &amp; \\text{otherwise} \\end{cases} \\] <p>This means that the agent always moves from \\( (2, 2) \\) to \\( (1, 2) \\) when taking the action \"move left\".</p>"},{"location":"unit1/lesson3/lesson3.html#reward-dynamics","title":"Reward Dynamics","text":"<p>The reward dynamics define the reward that the agent receives when it takes an action in a given state and transitions to a new state. The reward function \\( R(s, a, s') \\) specifies the immediate reward when transitioning from state \\( s \\) to state \\( s' \\) after taking action \\( a \\).</p> <p>Formally, the reward function is expressed as:</p> \\[ R(s, a, s') = \\mathbb{E}[r_{t+1} | s_t = s, a_t = a, s_{t+1} = s'] \\] <p>Where: - \\( r_{t+1} \\) is the reward received at time step \\( t+1 \\), - \\( s_t \\) and \\( a_t \\) represent the state and action at time \\( t \\), - \\( s_{t+1} \\) is the resulting state at time \\( t+1 \\).</p>"},{"location":"unit1/lesson3/lesson3.html#reward-function-properties","title":"Reward Function Properties:","text":"<ul> <li>Immediate Reward: \\( R(s, a, s') \\) gives the immediate reward for transitioning from state \\( s \\) to state \\( s' \\) after action \\( a \\).</li> <li>Stochastic Reward: Rewards can be stochastic, meaning the same action in the same state can yield different rewards.</li> </ul> <p>Example: If the agent takes action \\( a = \\text{move right} \\) from state \\( s = (1, 1) \\), the reward function might be:</p> \\[ R((1, 1), \\text{move right}, (2, 1)) = 10 \\] <p>Indicating that moving to the goal state \\( (2, 1) \\) yields a reward of 10.</p> <p>Conversely, if the agent moves to a dangerous state:</p> \\[ R((1, 1), \\text{move left}, (0, 1)) = -5 \\] <p>The agent receives a penalty of -5.</p>"},{"location":"unit1/lesson3/lesson3.html#3-the-return-g_t-of-a-time-step-t","title":"3. The Return \\(G_t\\) of a time step \\(t\\)","text":"<p>We start by realising the </p> \\[ \\begin{align*}     G_t = R_{t+1} + &amp;\\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\gamma^3 R_{t+4} + ... + \\gamma^{T-t-1} R_{T} \\\\     G_{t+1} =       &amp; R_{t+2} + \\gamma R_{t+3} + \\gamma^2 R_{t+4} + ... + \\gamma^{T-t-2} R_{T} \\end{align*} \\] <p>Hence by multiplying \\(G_{t+1}\\) by \\(\\gamma\\) and adding R_{t+1} we get</p> \\[ \\begin{equation}     G_t = R_{t+1} + \\gamma G_{t+1} \\end{equation} \\] <p>The above equation is the most important equation in RL that the Bellman Equations are built on it. In turn, we build all of our incremental updates in RL on Bellman optimality equation</p> <p>In the video below we talk more about this important concept.</p>"},{"location":"unit1/lesson3/lesson3.html#g_t-monotonicity-for-mdp-rewards","title":"\\(G_t\\) Monotonicity for MDP Rewards","text":"<p>Let us see how the return develops for an MDP with a reward of 1 or -1 for each time step. To calculate \\(G_t\\) we will go backwards, i.e. we will need to calculate \\(G_{t+1}\\) to be able to calculate \\(G_t\\) due to the incremental form of \\(G_t\\) where we have that \\(G_t = R_{t+1} + \\gamma G_{t+1}\\).</p> <ul> <li>Mathematically, we can prove that \\(G_t\\) is monotonically decreasing iff(if and only if) \\(\\frac{R_{t}}{1 - \\gamma} &gt;  G_{t}\\) \\(\\forall t\\) and \\(G_T=R_T &gt; 0\\). <ul> <li>Furthermore, when \\(R_t=1\\) \\(\\forall t\\) and \\(\\gamma=.9\\) then \\(G_t\\) converges in the limit to 10, i.e. 10 will be an upper bound for \\(G_t\\). </li> <li>Similarly, when \\(R_t=1\\) \\(\\forall t\\) and \\(\\gamma=.09\\) then \\(G_t\\) converges in the limit to 100</li> <li>More generally, when \\(1-\\gamma = 1/\\beta\\) then \\(R_t \\beta &gt; G_t\\) </li> </ul> </li> <li>On the other hand, we can prove that \\(G_t\\) is monotonically increasing iff \\(\\frac{R_{t}}{1 - \\gamma} &lt;  G_{t}\\).<ul> <li>Furthermore, when \\(R_t=-1\\) \\(\\forall t\\) and \\(\\gamma=.9\\) then \\(G_t\\) converges to -10, i.e. -10 is its lower bound. </li> <li>More generally, when \\(1-\\gamma = 1/\\beta\\) then \\(R_t \\beta &lt; G_t\\) </li> </ul> </li> </ul> <p>Below we prove the former and leave the latter for you as homework.</p> <p>\\(G_t = R_{t+1} + \\gamma G_{t+1}\\)</p> <p>We start by assuming that \\(G_t\\) is strictly monotonically decreasing (we dropped the word strictly in th above for readability)</p> <p>\\(G_t &gt; G_{t+1} &gt; 0\\) \\(\\forall t\\) (which entails that \\(G_T=R_T &gt; 0\\) when the horizon is finite, i.e. ends at \\(t=T\\)) we substitute by the incremental form of \\(G_t\\)</p> <p>\\(G_t &gt; G_{t+1} &gt; 0\\) \\(\\forall t \\implies R_{t+1} + \\gamma G_{t+1} &gt;G_{t+1} \\implies\\) \\(R_{t+1} &gt;  G_{t+1} - \\gamma G_{t+1} \\implies\\) \\(R_{t+1} &gt;  (1 - \\gamma) G_{t+1} \\implies\\)</p> <p>\\(\\frac{R_{t+1}}{1 - \\gamma} &gt; G_{t+1}\\) ( \\(\\gamma \\ne 1\\))</p> <p>The inequality \\(\\frac{R_{t+1}}{1 - \\gamma} &gt;  G_{t+1}\\) (which also can be written as \\(\\frac{R_{t}}{1 - \\gamma} &gt;  G_{t}\\)) must be satisfied whenever \\(G_t\\) is monotonically decreasing, i.e. it is a necessary condition. We can show that this inequality is also a sufficient condition to prove that \\(G_t\\) is monotonically decreasing by following the same logic backwards. Similar things can be proven for the non-strictly monotonically decreasing case i.e. when \\(G_t\\ge G_{t+1} \\ge 0\\) \\(\\forall t\\).</p> <p>Now when \\(R_{t+1}=1\\) and \\(\\gamma=.9\\), then by substituting these values in the inequality, we get that \\(\\frac{1}{1 - .9} &gt;  G_{t+1} \\implies\\) \\(10 &gt; G_{t+1}\\) </p>"},{"location":"unit1/lesson3/lesson3.html#g_t-monotonicity-for-sparse-mdp-rewards","title":"\\(G_t\\) Monotonicity for Sparse MDP Rewards","text":"<p>For sparse positive end-of-episode rewards, the above strict inequality is not satisfied since \\(R_t=0\\) \\(\\forall t&lt;T\\) and \\(R_T&gt;0\\). 1. In this case, we can show that \\(G_t \\le G_{t+1}\\) i.e. \\(G_t\\) it is a monotonically increasing function.     1. Furthermore, when \\(\\gamma&lt;1\\) then \\(G_t\\) is strictly increasing, i.e.  \\(G_t &lt; G_{t+1}\\) 1. Furthermore, \\(G_{t} = \\gamma^{T-t-1} R_{T}\\).     1. when \\(R_T=1\\) then \\(G_{t} = \\gamma^{T-t-1}\\)      1. when \\(R_T=-1\\) then \\(G_{t} = -\\gamma^{T-t-1}\\)</p> <ul> <li> <p>To prove the monotonicity we start with our incremental form for the return:      \\(G_t = R_{t+1} + \\gamma G_{t+1}\\):</p> <p>Since we have that \\(R_{t+1} = 0\\) \\(\\forall t&lt;T\\) then</p> <p>\\(G_t = \\gamma G_{t+1}\\) \\(\\forall t&lt;T\\), therefore, since \\(\\gamma \\le 1\\) then \\(G_t \\le G_{t+1}\\) \\(\\forall t&lt;T\\).</p> </li> <li> <p>To prove that  \\(G_{t} = \\gamma^{T-t-1} R_{T}\\) we can also utilise the incremental form and perform a deduction, but it is easier to start with the general form of a return, we have:</p> <p>\\(G_t = R_{t+1} + \\gamma R_{t+2}  + \\gamma^2 R_{t+3} + \\gamma^3 R_{t+4} + ... + \\gamma^{T-t-1} R_{T}\\)</p> <p>Since we have that \\(R_{t+1} = 0\\) \\(\\forall t&lt;T\\) then</p> <p>\\(G_t = \\gamma^{T-t-1} R_{T}\\)</p> </li> </ul> <p>This gives us guidance on the type of behaviour that we expect our agent to develop when we follow one of these reward regimes (sparse or non-sparse). </p> <p>The above suggests that for sparse end-of-episode rewards, decisions near the terminal state(s) have far more important effects on the learning process than earlier decisions. While for non-sparse positive rewards MDPs, earlier states have higher returns and hence more importance than near terminal states. </p> <p>If we want our agent to place more importance on earlier states, and near-starting state decisions, then we will need to utilise non-sparse (positive or negative) rewards. Positive rewards encourage repeating certain actions that maintain the stream of positive rewards for the agent. An example will be the pole balancing problem. Negative rewards, encourage the agent to speed up towards ending the episode so that it can minimise the number of negative rewards received.</p> <p>When we want our agent to place more importance for the decisions near the terminal states, then a sparse reward is more convenient. Sparse rewards are also more suitable for offline learning as they simplify the learning and analysis of the agent's behaviour. Non-sparse rewards suit online learning on the other hand, because they give a quick indication of the agent behaviour suitability and hence speed up the early population of the value function. </p>"},{"location":"unit1/lesson3/lesson3.html#4-the-expected-return-function-v","title":"4. The Expected Return Function V","text":"<p>Once we move form an actul return that comes froma an actual experience at time step \\(t\\) to try to estimate this return, we move to an expectaiton function. This function, traditionally called the value function v, is an important function. But now isntead of tying the value of the return to a particular experience at a step t which would be less useful in generalising the lessons an agent can learn from interacting with the environment, it makes more sense to ty this up to a certain state \\(s\\). This will allow the agent to learn a useful expectation of the return(discounted sum of rewards) for a particualr state when the agent follows a policy \\(\\pi\\). I.e. we are now saying that a we will get an expected value of the return for a particular state under a policy \\(\\pi\\). So we moved from subscripting by a step \\(t\\) into passing a state \\(s\\) to the function and subscripting by a policy.</p> \\[ \\begin{equation}     v_{\\pi}(s) = \\mathbb{E}_{\\pi}(G_t)   \\label{eq:v}  %\\tag{1} \\end{equation} \\] <p>Equation \\(\\eqref{eq:v}\\) gives the definition of v function.</p> <p>In the following video we tackle this idea in more details.</p> <p></p>"},{"location":"unit1/lesson3/lesson3.html#5-the-bellman-equations","title":"5. The Bellman Equations","text":"<p>The Bellman equations provide recursive relationships between the value of a state (or state-action pair) and the values of neighboring states. These equations are fundamental in solving MDPs and are the basis for many reinforcement learning algorithms.</p>"},{"location":"unit1/lesson3/lesson3.html#bellman-equation-for-the-value-function","title":"Bellman Equation for the Value Function","text":"<p>The value function \\( V_{\\pi}(s) \\) represents the expected return starting from state \\( s \\) and following policy \\( \\pi \\). The Bellman equation for \\( V_{\\pi}(s) \\) is:</p> \\[ V_{\\pi}(s) = \\mathbb{E}_{\\pi}\\left[ R(s, a, s') + \\gamma \\sum_{s'} P(s' | s, a) V_{\\pi}(s') \\right] \\] <p>Where: - \\( V_{\\pi}(s) \\) is the value of state \\( s \\) under policy \\( \\pi \\), - \\( R(s, a, s') \\) is the immediate reward for transitioning from \\( s \\) to \\( s' \\) after action \\( a \\), - \\( \\gamma \\) is the discount factor, and - \\( P(s' | s, a) \\) is the transition probability.</p>"},{"location":"unit1/lesson3/lesson3.html#bellman-equation-for-the-q-function","title":"Bellman Equation for the Q-Function","text":"<p>The Q-function \\( Q_{\\pi}(s, a) \\) represents the expected return after taking action \\( a \\) in state \\( s \\) and then following policy \\( \\pi \\). The Bellman equation for \\( Q_{\\pi}(s, a) \\) is:</p> \\[ Q_{\\pi}(s, a) = \\mathbb{E}\\left[ R(s, a, s') + \\gamma \\sum_{s'} P(s' | s, a) V_{\\pi}(s') \\right] \\] <p>Where: - \\( Q_{\\pi}(s, a) \\) is the action-value function, - The terms \\( R(s, a, s') \\), \\( \\gamma \\), and \\( P(s' | s, a) \\) are the same as in the value function equation.</p>"},{"location":"unit1/lesson3/lesson3.html#bellman-optimality-equations","title":"Bellman Optimality Equations","text":"<p>The Bellman optimality equations describe the relationship between the optimal value function \\( V^*(s) \\) or the optimal Q-function \\( Q^*(s, a) \\) and the transition and reward dynamics. These equations are used to compute the optimal policy that maximizes the expected return.</p>"},{"location":"unit1/lesson3/lesson3.html#bellman-optimality-equation-for-the-value-function","title":"Bellman Optimality Equation for the Value Function:","text":"\\[ V^*(s) = \\max_a \\mathbb{E}\\left[ R(s, a, s') + \\gamma \\sum_{s'} P(s' | s, a) V^*(s') \\right] \\]"},{"location":"unit1/lesson3/lesson3.html#bellman-optimality-equation-for-the-q-function","title":"Bellman Optimality Equation for the Q-Function:","text":"\\[ Q^*(s, a) = \\mathbb{E}\\left[ R(s, a, s') + \\gamma \\sum_{s'} P(s' | s, a) \\max_{a'} Q^*(s', a') \\right] \\] <p>Where: - \\( V^*(s) \\) is the optimal value function, - \\( Q^*(s, a) \\) is the optimal Q-function, - The max operator ensures that the agent chooses the action \\( a \\) that maximizes the expected return.</p> <p>You can adjust the video settings in SharePoint (speed up to 1.2 and reduce the noise if necessary)</p> <p>Exercise 1: If you realise there is a missing symbol in the [video: Bellman Equation for v] last equations, do you know what it is and where it has originally come from?</p> <p>Exercise 2: Can you derive Bellman Optimality Equation for \\(q(s,a)\\) from first principles?</p> <p>video:  Bellman Optimality for q from first principles</p>"},{"location":"unit1/lesson3/lesson3.html#summary","title":"Summary","text":"<p>The Markov Decision Process (MDP) framework models decision-making problems where an agent interacts with an environment. It includes transition dynamics \\( P(s' | s, a) \\) and reward dynamics \\( R(s, a, s') \\), which describe the behavior of the environment. The Bellman equations provide recursive relationships for computing the value of states or actions, while the Bellman optimality equations help find the optimal policy. Properties like the Markov Property, stationarity, and the stochastic nature of the dynamics are key factors in MDPs. Understanding these dynamics and equations is central to reinforcement learning algorithms designed to find optimal decision-making strategies.</p> <p>Further Reading: For further info refer to chapter 3 of the Sutton and Barto book. </p>"},{"location":"unit1/lesson3/lesson3.html#your-turn","title":"Your turn","text":"<p>Go ahead and play around with some grid world environment by executing and experiementing with the code in the following worksheet.</p> <p>worksheet3</p>"},{"location":"unit1/lesson4/lesson4.html","title":"Basic ROS Concepts","text":"<p>picture credit</p>"},{"location":"unit1/lesson4/lesson4.html#table-of-contents","title":"Table of Contents","text":"<ul> <li>Basic ROS Concepts</li> <li>Table of Contents</li> <li>Basic ROS Terms<ul> <li>ROS Node</li> <li>Discovery</li> <li>ROS Package</li> <li>ROS Stack</li> <li>ROS Workspace</li> <li>ROS Topic</li> <li>ROS Publisher</li> <li>ROS Subscriber</li> <li>ROS Service</li> <li>URDF File</li> <li>ROS Launch file</li> <li>ROS Parameter</li> <li>ROS Messages</li> <li>Robot Web Tools</li> <li>ROS Serial</li> </ul> </li> <li>VM and Prerequisite Packages</li> </ul> <p>## Basic ROS Terms</p> <p>This Wiki page will help you understand the basic definitions of the most common ROS terms. ROS 2 is a middleware based on a strongly-typed, anonymous publish/subscribe mechanism that allows for message passing between different processes.</p> <p>At the heart of any ROS 2 system is the ROS graph. The ROS graph refers to the network of nodes in a ROS system and the connections between them by which they communicate.</p> <p>These are the concepts that will help you get started understanding the basics of ROS 2.</p> <p>These concepts are same for both ROS 1 and ROS 2</p>"},{"location":"unit1/lesson4/lesson4.html#ros-node","title":"ROS Node","text":"<p>A ROS node is a simple program that publishes or subscribes to a topic or contains a program that enables a ROS service. It should be highly specialized and accomplish a single task. Nodes communicate with other nodes by sending messages, which will be discussed later. Examples of tasks which should be carried out in a specific node are:</p> <ul> <li>Controlling motors</li> <li>Interpreting commands from an input source</li> <li>Planning a path to drive on</li> <li>Reading a specific sensor.</li> </ul> <p>A node is a participant in the ROS 2 graph, which uses a client library to communicate with other nodes. Nodes can communicate with other nodes within the same process, in a different process, or on a different machine. Nodes are typically the unit of computation in a ROS graph; each node should do one logical thing.</p> <p>Nodes can publish to named topics to deliver data to other nodes, or subscribe to named topics to get data from other nodes. They can also act as a service client to have another node perform a computation on their behalf, or as a service server to provide functionality to other nodes. For long-running computations, a node can act as an action client to have another node perform it on their behalf, or as an action server to provide functionality to other nodes. Nodes can provide configurable parameters to change behavior during run-time.</p> <p>Nodes are often a complex combination of publishers, subscribers, service servers, service clients, action servers, and action clients, all at the same time.</p> <p>Connections between nodes are established through a distributed discovery process</p>"},{"location":"unit1/lesson4/lesson4.html#discovery","title":"Discovery","text":"<p>Discovery of nodes happens automatically through the underlying middleware of ROS 2. It can be summarized as follows:</p> <p>When a node is started, it advertises its presence to other nodes on the network with the same ROS domain (set with the ROS_DOMAIN_ID environment variable). Nodes respond to this advertisement with information about themselves so that the appropriate connections can be made and the nodes can communicate.</p> <p>Nodes periodically advertise their presence so that connections can be made with new-found entities, even after the initial discovery period.</p> <p>Nodes advertise to other nodes when they go offline.</p> <p>Nodes will only establish connections with other nodes if they have compatible Quality of Service settings.</p> <p>Take the talker-listener demo for example. Running the C++ talker node in one terminal will publish messages on a topic, and the Python listener node running in another terminal will subscribe to messages on the same topic.</p> <p>You should see that these nodes discover each other automatically, and begin to exchange messages.</p>"},{"location":"unit1/lesson4/lesson4.html#ros-package","title":"ROS Package","text":"<p>A package is a container for closely related nodes and utilities used by those nodes. Include files, message definitions, resources, etc are stored along with nodes inside a package. As an example, imagine a robot with many different sensors, such as IR rangefinders, sonar, laser scanners, and encoders. Each of these types of sensors would ideally have their own node. Because these nodes are all for sensors, you might group them into a package called my_robots_sensor_drivers.</p>"},{"location":"unit1/lesson4/lesson4.html#ros-stack","title":"ROS Stack","text":"<p>A stack is a collection of closely related packages. You might name a stack after your robot, with individual packages for sensors, motors, and planning. These packages would contain more specific nodes that do specific tasks. You will often hear about the \u201cNavigation stack\u201d in ROS. This is a collection of packages used for helping robots navigate in the world and is very useful.</p> <p>Important note: ROS is trying to get rid of the term Stack for Metapackage, but everyone still uses Stack</p>"},{"location":"unit1/lesson4/lesson4.html#ros-workspace","title":"ROS Workspace","text":"<p>All of the development you do for ROS must be done in your ROS workspace. This is so ROS knows where to look for all of the programs you write and their respective utilities and resources.</p>"},{"location":"unit1/lesson4/lesson4.html#ros-topic","title":"ROS Topic","text":"<p>ROS Topics are named buses that allow nodes to pass messages. A ROS topic can be published to, or subscribed to with many different message types. This setup allows the publishing and subscribing to happen completely independent of each other assuming they have matching message types. For a more detailed information please follow the link</p>"},{"location":"unit1/lesson4/lesson4.html#ros-publisher","title":"ROS Publisher","text":"<p>A ROS Publisher is a program or portion of a program that \"publishes\" or sends data into a ROS topic.</p>"},{"location":"unit1/lesson4/lesson4.html#ros-subscriber","title":"ROS Subscriber","text":"<p>A ROS subscriber is a program or portion of a program that \"subscribes to\" a topic in order to receive the messages published to that topic.</p>"},{"location":"unit1/lesson4/lesson4.html#ros-service","title":"ROS Service","text":"<p>A ROS service is an object that can be called similar to a function from other nodes. It allows a request and a reply. An example might be an \"add two ints\" service.</p>"},{"location":"unit1/lesson4/lesson4.html#urdf-file","title":"URDF File","text":"<p>ROS URDF files are used to define a robots physical characteristics. The URDF file describes the robot model, Robot sensors, scenery, and objects. These models are displayed in rviz and use the tf (transform) topics to define where the models are. If you are creating a robot you can/will be able to export the URDF file straight from Solidworks.</p>"},{"location":"unit1/lesson4/lesson4.html#ros-launch-file","title":"ROS Launch file","text":"<p>A ROS launch file is a file used to execute multiple nodes and allows for remapping of topics, setting of parameters, and will automatically start roscore if it is not already running. See roslaunch or ROS Launch file type.</p>"},{"location":"unit1/lesson4/lesson4.html#ros-parameter","title":"ROS Parameter","text":"<p>A ROS parameter is a name that has an associated value. A central [parameter server] keeps track of parameters in one place that can be updated. . As it is not designed for high-performance, it is best used for static, non-binary data such as configuration parameters.</p>"},{"location":"unit1/lesson4/lesson4.html#ros-messages","title":"ROS Messages","text":"<p>ROS Messages are the individual sets of data that are published to a topic. Messages can have many different types but the standard messages can be found here. The fields in messages can be any of the base types or another message. Common Message types can be found here.</p>"},{"location":"unit1/lesson4/lesson4.html#robot-web-tools","title":"Robot Web Tools","text":"<p>Robot Web Tools is a suite that allows most ROS information to be sent over the internet. Robot Web Tools allows you to build ROS webpages or pass the information directly through the internet.</p>"},{"location":"unit1/lesson4/lesson4.html#ros-serial","title":"ROS Serial","text":"<p>ROS Serial allows ROS to talk with any serial device, primarily embedded controllers such as Arduino's.</p>"},{"location":"unit1/lesson4/lesson4.html#vm-and-prerequisite-packages","title":"VM and Prerequisite Packages","text":"<p>Please see the following video to get started with teh VM. </p> <p>You should recieve an email inviting you to have access to an Azure VM. The VM has ROS 2 Foxy Fitzroy already installed. ROS2 commands need to be run from the terminal not from a conda-activated terminal (due to compatibility), and they use the default system Python 3.8. The VM has the libraries required for ROS2 along with TurtleBot3 installed with the worlds required for assessment.</p> <p>We have tested the notebooks on Python 3.8, so they should work smoothly for higher versions. Note that ROS2 code must be run with the default VM Python3.8 kernel. For the best experience, use VScode</p> <p>The machine has decent cores and memory (according to Azure 4 cores | 8GB RAM | 128GB Standard SSD). The VM has Ubuntu 20 and Xfce (Xubuntu) interface due to its lightweight (to give you the best experience remotely- to come as close as a local machine feeling) and it is tailored to give the same feeling as the usual Ubuntu Genome. You can run hardinfo in the terminal to check the VM specs. I hope you will enjoy it. </p> <p>To access the VM, please use the usual remote desktop app available on your system. You will receive an email with access to your VM. The username is rl, and the password is rl@ros2. </p> <p>You will have sudo access. Please apply caution when dealing with the system and avoid installing packages so as not to break it, which can be time-consuming. You will have around a 40-hour time limit, so please be mindful not to leave the system running unless necessary so as not to run out of time. Usually, you would want time for running the exercises and save plenty of time (\u00bd) for your project training (this is where the VM will be most useful).</p> <p>If the VM becomes corrupted for some reason, then you can reimage it by going to Azure Lab page and selecting the three dots, then reimage. That will cause all the data you have on the machine to be lost. You are advised to back up your data, you may want to use OneDrive or other backup methods.</p> <p>Now go ahead and try doing worksheet4 to experiement more with some of the ros concepts.</p> <p>ROS Worksheet 4: Robotics Operating System</p>"},{"location":"unit2/lesson5/lesson5.html","title":"5. Dynamic Programming","text":"<p>The notebook uses a library of functionality in RL that aims for simplicity and general insight into how algorithms work, these libraries are written from scratch using standard Python libraries (numpy, matplotlib etc.). Please note that you will need permission from the author to use the code for research, commercially or otherwise.</p>"},{"location":"unit2/lesson5/lesson5.html#lesson-5-dynamic-programming-model-based-approach","title":"Lesson 5- Dynamic Programming: Model-Based Approach","text":"<p>Unit 2: Learning Outcomes By the end of this unit, you will be able to:  </p> <ol> <li>Compute the value function for a given policy in tabular settings.  </li> <li>Implement control methods that infer an agent\u2019s policy from an action-value function.  </li> <li>Explain the concept of Generalized Policy Iteration (GPI) and how it underpins many RL methods.  </li> <li>Compare full-backup action-value-based control methods with direct policy estimation control methods.  </li> <li>Evaluate how Monte Carlo (MC) methods provide unbiased but high-variance estimates through interaction with the environment.  </li> <li>Analyze how REINFORCE achieves unbiased but high-variance policy gradient estimation through interaction with the environment.  </li> </ol> <p>In the first lesson, you looked at a basic RL problem, the k-arm bandit, which involves only actions and no states (non-associative problem). In general, in RL, we are faced with different situations, and we need to take different actions in each situation in order to achieve a certain goal. This general type of environment with states and actions imposes a different flavour to the solution we can design. From now on, we will tackle associative problems. For associative problems, there are two approaches:</p> <ol> <li>Model-based approach</li> <li>Model-free approach</li> </ol> <p>In this lesson, we will take the first approach. We will learn how to use a model of the environment to solve an RL problem. The model is given in the form of the dynamics of the environment. These usually come in the form of 4 dimensions of conditional probability involving an answer to the following question: what is the probability of obtaining a certain reward r in a certain state s' given that the agent was previously in a state s and applied action a.</p> <p>We will assume that there is already a model for the environment and try to take advantage of this model to come up with the best policy. Nevertheless, we will see simple ways to build such models and come back to this question later when we tackle planning algorithms in RL.</p> <p>Reading: The accompanying reading of this lesson are chapters 3 and 4 from our textbook by Sutton and Barto available online here.</p> <p>Plan As usual, in general there are two types of RL problems that we will attempt to design methods to deal with  1. Prediction problem For These problems we will design Policy Evaluation Methods that attempt to find the best estimate for the value-function given a policy.</p> <ol> <li>Control problems  For These problems we will design Value Iteration methods which utilise the idea of Generalised Policy Iteration. They attempt to find the best policy, via estimating an action-value function for a current policy then moving to a better and improved policy by choosing a greedy action often.</li> </ol> <p>To help you, Abdulrahman recorded a set of videos that covers the material</p>"},{"location":"unit2/lesson5/lesson5.html#basics-dynamic-programming","title":"Basics Dynamic Programming","text":""},{"location":"unit2/lesson5/lesson5.html#policy-evaluation-and-value-iteration","title":"Policy Evaluation and Value Iteration","text":"<pre><code>%matplotlib inline\nfrom Grid import *\n</code></pre> <p>We start by generating conditional probabilities from random joint probabilities, then we move into generating them from a model.</p>"},{"location":"unit2/lesson5/lesson5.html#1d-probability","title":"1D Probability","text":"<p>Let us start by defining a 1D probability function </p> <pre><code>def P(nS):\n    p = np.zeros(nS)\n    for s in range(nS):\n        p[s] = rand()\n\n    return p/p.sum()\np = P(5)\nprint(p, p.sum())\n</code></pre> <pre><code>[0.37548211 0.24436176 0.03478805 0.01064735 0.33472073] 1.0\n</code></pre>"},{"location":"unit2/lesson5/lesson5.html#2d-probability-joint-probability","title":"2D Probability: Joint Probability","text":"<p>Let us move to defining a joint probability function </p> <pre><code>def P(nS,nA):\n    p = np.zeros((nS,nA))\n    for s in range(nS):\n        for a in range(nA):\n            p[s,a] = rand()\n\n\n    # /p.sum() to make sure that this is a joint probability density, i.e. p.sum()==1\n    return p/p.sum() \np = P(5,2)\nprint(p, p.sum())\n</code></pre> <pre><code>[[0.17464052 0.12339209]\n [0.10853332 0.1073298 ]\n [0.03067933 0.03055074]\n [0.06509827 0.06226649]\n [0.15006026 0.14744918]] 0.9999999999999999\n</code></pre> <p>Note that p[s,a] in the above is interpreted as the probability of being in state s and taking action a at the same time</p>"},{"location":"unit2/lesson5/lesson5.html#2d-probability-conditional-probability","title":"2D Probability: Conditional Probability","text":"<p>Let us move to defining a conditional probability function. Remember that a conditional probability must satisfy that Bayes rule.</p> <p>In the below pr[sn,a] is interpreted as pr[sn|a] the probability of moving to state sn given that we took action a </p> <pre><code>def P(nS,nA):\n    pr = np.zeros((nS,nA)) # joint\n    p  = np.zeros((nS,nA)) # conditional\n\n    # first create a joint probability\n    for sn in range(nS):\n        for a in range(nA):\n            pr[sn,a] = rand()\n\n    # to make sure that this is a joint probability density\n    pr=pr/pr.sum() \n\n    # now create a conditional probability via Bayes rule\n    for a in range(nA):\n        p[:,a] = pr[:,a]/pr[:,a].sum()\n\n\n    return p\n\np = P(5,2)\nprint(p)\nprint(p.sum(0))\n</code></pre> <pre><code>[[0.16261181 0.29707477]\n [0.19121227 0.1445241 ]\n [0.41842684 0.19879146]\n [0.14277596 0.06602533]\n [0.08497312 0.29358434]]\n[1. 1.]\n</code></pre>"},{"location":"unit2/lesson5/lesson5.html#4d-probability-random-dynamic","title":"4D Probability: Random Dynamic","text":"<p>Our goal now is to be able to obtain a probability that can represent a random environment dynamics. Such a probability has the form of p[sn,rn | s,a] which represents the probability of moving to next state sn and obtaining the reward rn given that the agent was in state s and took action a.</p> <p>We generalise the above method of obtaining conditional probabilities from random joint probability density to 4-d. We have not dealt with 3-d cases for brevity. This time we will make it a bit more efficient by avoiding the four for loops and utilising the vectorised version of the rand() function in numpy and we convert to a conditional in-place to avoid having two probabilities p and pr.</p> <pre><code>def dynrand(nS, nA, nR): # states, actions, rewards dimensions \n    #first joint: p[sn,rn, s,a]\n    p  = np.random.rand(nS,nR,  nS,nA)\n    p /= p.sum()\n\n    #convert it to conditional: p[sn,rn| s,a] \n    for s in range(nS):\n        for a in range(nA):\n            p[:,:, s,a] /= p[:,:, s,a].sum()\n\n    return p\n\np = dynrand(3,2,2)\nprint(p)\nprint(p.sum(0).sum(0))\n</code></pre> <pre><code>[[[[0.33929711 0.0608615 ]\n   [0.24024123 0.02720279]\n   [0.19929774 0.23603235]]\n\n  [[0.28847357 0.25786912]\n   [0.20711515 0.15895581]\n   [0.1864589  0.03933985]]]\n\n\n [[[0.09390373 0.23816355]\n   [0.09103142 0.22100381]\n   [0.01013988 0.14884069]]\n\n  [[0.05826466 0.13310033]\n   [0.14192641 0.17366847]\n   [0.2242753  0.32088934]]]\n\n\n [[[0.06008514 0.28096335]\n   [0.14680999 0.2805682 ]\n   [0.28111859 0.14183835]]\n\n  [[0.15997578 0.02904214]\n   [0.1728758  0.13860091]\n   [0.09870959 0.11305943]]]]\n[[1. 1.]\n [1. 1.]\n [1. 1.]]\n</code></pre>"},{"location":"unit2/lesson5/lesson5.html#inducing-the-dynamics-by-interacting-with-the-environment","title":"Inducing the dynamics by interacting with the environment","text":"<p>Now we move into obtaining the dynamics from an actual environment instead of generating the dynamics randomly as we did earlier.</p> <p>We will use mainly the random walk environment and the grid world environment and generate their dynamics. These are deterministic simple environments. Nevertheless, they are very useful to demonstrate the ideas of RL. </p> <p>Note that when we move to the real world the dynamics become much more complex and building or obtaining the dynamic becomes impractical in most cases. Therefore, towards that end instead of dealing directly with the environment's dynamics, we will see later how we can substitute this requirement by having to interact with the environment to gain experience which will help us infer a good estimate of the expected value function (discounted sum of rewards) which in turn will help us to infer a close to optimal policy for the task in hand. </p> <p>The exercise of dealing with probabilities and then using them in designing a Dynamic Programming solution is valuable since most of the other solutions utilise the basic ideas (policy iteration, value iteration algorithms and policy improvements theorem) that we cover here and will mainly show us that we can devise a form of Bellman equation that is suitable for interaction, where we use sampling, model-free algorithms instead of using probabilities(dynamics), model-based algorithms. </p> <p>Dynamic programming suffers from what Bellman described as the curse of dimensionality which indicates that the computational resources required to solve a problem grow exponentially with the dimensionality of the problem. So in our case, the dimensionality is the number of states (as well as actions and rewards). So for example if the dynamic programming solution computational complexity is \\(2^{|S|}\\) and the number of states \\(|S|=10\\) then it costs \\(2^{10}=1024\\) but when the number of states \\(|S|\\) grows to 100 the cost becomes \\(2^{100}=1267650600228229401496703205376\\).</p> <pre><code>print(2**10)\nprint(2**100)\n</code></pre> <pre><code>1024\n1267650600228229401496703205376\n</code></pre>"},{"location":"unit2/lesson5/lesson5.html#sources-stochasticity-dynamics-and-policy","title":"Sources Stochasticity- Dynamics and Policy","text":"<p>One important point to make is that stochasticity comes from different elements of the MDP and from the policy itself.</p> <ol> <li>There might be stochasticity in the dynamics at the state transition level,    where applying action \\(a\\) in a state \\(s\\) may cause the agent to transition to different states, each with a different probability</li> <li> <p>There might be stochasticity in the dynamics at the reward level,    where applying action \\(a\\) in a state \\(s\\) may result in different rewards, each with a certain probability</p> </li> <li> <p>There might be stochasticity in the policy itself,    where the policy applies different actions in a state \\(s\\)  with different probabilities</p> </li> <li> <p>There might be stochasticity or randomness in observing the current state due to the complexity of the state space.     For example, when a robot moves around in the environment, after a while, we cannot reliably designate its position from its motor encoders even when we know the start position due to dead-reckoning. This is called partial observability, and there is a framework called BOMDP or partially observable MDP to tackle this problem. However, we will not study this branch. The field is divided about the necessity of BOMDP with a line of thought that considers that we can overcome this difficulty by encoding our states differently but staying in the MDP framework.</p> </li> </ol> <p>These sources of stochasticity dictate using suitable techniques to obtain the dynamics and to evaluate or improve stochastic and deterministic policy.</p> <pre><code>def dynamics(env=randwalk(), show=False, stoch=True, repeat=1000 ): #, maxjump=1\n    rewards = env.rewards_set()\n    nS, nA, nR = env.nS, env.nA, rewards.shape[0]\n    p  = np.zeros((nS,nR,  nS,nA))\n\n    for i in range(repeat if not stoch else 1): # in case the env is stochastic (non-deterministic)\n        for s in range(nS):\n            # if s in env.goals: continue # uncomment to explicitly make pr of terminal states=0\n            for a in range(nA):\n                if not i and show: env.render() # render the first repetition only\n                env.s = s\n                sn,rn,_,_ = env.step(a)\n                rd = np.where(rewards==rn)[0][0] # get which reward index we need to update\n                p[sn,rd, s,a] +=1\n\n    # making sure that it is a conditional probability that satisfies Bayes rule\n    for s in range(nS):\n        for a in range(nA):\n            sm=p[:,:, s,a].sum()\n            if sm: p[:,:, s,a] /= sm\n\n    return p\n</code></pre> <p>Note that we excluded the terminal states so that their dynamics are always 0. This is not entirely necessary, since in our algorithm implementations we always initiate the terminal states' values to 0 and avoid updating them. </p> <p>Note also that states and actions can be immediately interpreted as indexes. Rewards on the other hand are allowed to be real values. Therefore, we need to obtain the index of the reward value that we need to update since p[sn, rn, s,a] expects an index in rn.</p> <p>Let us now test our dynamics extraction procedure on a deterministic random walk environment:</p> <pre><code>p = dynamics(env=randwalk(), show=True)\nprint(p.shape)\nprint(p.sum(0).sum(0))\n# print(p)\n</code></pre> <p></p> <pre><code>(7, 2, 7, 2)\n[[1. 1.]\n [1. 1.]\n [1. 1.]\n [1. 1.]\n [1. 1.]\n [1. 1.]\n [1. 1.]]\n</code></pre> <p>The above shows that the probabilities of agent having been in state 0:6 (rows) and have gone left (column 0) all sum up to 1 except for the last terminal state 6 because once the agent is there it stays there and cannot move left. Also it shows equally that the probabilities of agent having been in state 0:6 and have gone right (column 1) all sum up to 1 except for the first terminal state 0 because once the agent is there it stays there and cannot move right. </p> <p>To further explain and dissect the generated probability dynamics we examine different states probabilities with different rewards.</p> <pre><code>def print_dynamcis(s):\n    print('-----------------------------Agent was in non-terminal state %d---------------------------------'%s)\n    print('Pr[of moving to states 0:6 and obtaining reward 0 |give agent was in state %d and moved left(0)]'%s)\n    print(p[:,0, s,0])\n\n    print('Pr[of moving to states 0:6 and obtaining reward 0 |give agent was in state %d and moved right(1)]'%s)\n    print(p[:,0, s,1])\n\n    print('Pr[of moving to states 0:6 and obtaining reward 1 |give agent was in state %d and moved left(0)]'%s)\n    print(p[:,1, s,0])\n\n    print('Pr[of moving to states 0:6 and obtaining reward 1 |give agent was in state %d and moved right(1)]'%s)\n    print(p[:,1, s,1])\n</code></pre> <pre><code>print_dynamcis(s=2)\n</code></pre> <pre><code>-----------------------------Agent was in non-terminal state 2---------------------------------\nPr[of moving to states 0:6 and obtaining reward 0 |give agent was in state 2 and moved left(0)]\n[0. 1. 0. 0. 0. 0. 0.]\nPr[of moving to states 0:6 and obtaining reward 0 |give agent was in state 2 and moved right(1)]\n[0. 0. 0. 1. 0. 0. 0.]\nPr[of moving to states 0:6 and obtaining reward 1 |give agent was in state 2 and moved left(0)]\n[0. 0. 0. 0. 0. 0. 0.]\nPr[of moving to states 0:6 and obtaining reward 1 |give agent was in state 2 and moved right(1)]\n[0. 0. 0. 0. 0. 0. 0.]\n</code></pre> <pre><code>print_dynamcis(s=5)\n</code></pre> <pre><code>-----------------------------Agent was in non-terminal state 5---------------------------------\nPr[of moving to states 0:6 and obtaining reward 0 |give agent was in state 5 and moved left(0)]\n[0. 0. 0. 0. 1. 0. 0.]\nPr[of moving to states 0:6 and obtaining reward 0 |give agent was in state 5 and moved right(1)]\n[0. 0. 0. 0. 0. 0. 0.]\nPr[of moving to states 0:6 and obtaining reward 1 |give agent was in state 5 and moved left(0)]\n[0. 0. 0. 0. 0. 0. 0.]\nPr[of moving to states 0:6 and obtaining reward 1 |give agent was in state 5 and moved right(1)]\n[0. 0. 0. 0. 0. 0. 1.]\n</code></pre> <p>Note that the probability of the penultimate state is a bit different than a mid-state such as 2. The reason is that when the agent move left it will get a 0 reward while when it moves right it will get a reward of 1.</p> <pre><code>print_dynamcis(s=0)\n</code></pre> <pre><code>-----------------------------Agent was in non-terminal state 0---------------------------------\nPr[of moving to states 0:6 and obtaining reward 0 |give agent was in state 0 and moved left(0)]\n[1. 0. 0. 0. 0. 0. 0.]\nPr[of moving to states 0:6 and obtaining reward 0 |give agent was in state 0 and moved right(1)]\n[0. 1. 0. 0. 0. 0. 0.]\nPr[of moving to states 0:6 and obtaining reward 1 |give agent was in state 0 and moved left(0)]\n[0. 0. 0. 0. 0. 0. 0.]\nPr[of moving to states 0:6 and obtaining reward 1 |give agent was in state 0 and moved right(1)]\n[0. 0. 0. 0. 0. 0. 0.]\n</code></pre> <pre><code>print_dynamcis(s=6)\n</code></pre> <pre><code>-----------------------------Agent was in non-terminal state 6---------------------------------\nPr[of moving to states 0:6 and obtaining reward 0 |give agent was in state 6 and moved left(0)]\n[0. 0. 0. 0. 0. 1. 0.]\nPr[of moving to states 0:6 and obtaining reward 0 |give agent was in state 6 and moved right(1)]\n[0. 0. 0. 0. 0. 0. 0.]\nPr[of moving to states 0:6 and obtaining reward 1 |give agent was in state 6 and moved left(0)]\n[0. 0. 0. 0. 0. 0. 0.]\nPr[of moving to states 0:6 and obtaining reward 1 |give agent was in state 6 and moved right(1)]\n[0. 0. 0. 0. 0. 0. 1.]\n</code></pre> <p>Uncomment the xor relationship and see the effect on the resultant policy.</p>"},{"location":"unit2/lesson5/lesson5.html#accommodating-for-jumps-in-the-environment","title":"Accommodating for jumps in the environment","text":"<p>Some environments might allow the agent to jump over some obstacles or simply skip cells. For these, we define slightly altered dynamics to take the jumps into account. Below we show the definition.</p> <pre><code>def dynamics(env=randwalk(), stoch=False, show=False, repeat=1000): # , maxjump=1\n\n    rewards = env.rewards_set()\n    nS, nA, nR = env.nS, env.nA, rewards.shape[0]\n    p  = np.zeros((nS,nR,  nS,nA))\n    randjump = env.randjump\n    env.randjump = False # so that probability of all intermed. jumps is correctly calculated\n    for i in trange(repeat if stoch else 1): # in case the env is stochastic (non-deterministic)\n        for s in range(nS):\n            if s in env.goals: continue # uncomment to explicitly make pr of terminal states=0\n            for a in range(nA):\n                for jump in (range(1,env.jump+1) if randjump else [env.jump]):\n                    if not i and show: env.render() # render the first repetition only\n                    env.s = s\n                    env.jump = jump\n                    rn = env.step(a)[1]\n                    sn = env.s\n                    rn_ = np.where(rewards==rn)[0][0] # get reward index we need to update\n                    p[sn,rn_, s,a] +=1\n\n    env.randjump = randjump\n    # making sure that it is a conditional probability that satisfies Bayes rule\n    for s in range(nS):\n        for a in range(nA):\n            sm=p[:,:, s,a].sum()\n            if sm: p[:,:, s,a] /= sm\n\n    return p\n</code></pre>"},{"location":"unit2/lesson5/lesson5.html#state-transition-probability-reward-marginalisation","title":"State-Transition probability: reward marginalisation","text":"<p>Now that we have induced the dynamics from the environment, we are ready to infer the state-transition probability from the dynamics.  Note that many older papers of RL refer to the 3-d state-transition probability without referring to the 4-d dynamics. The state-transition probability p[sn | s,a] is a conditional probability that specifies the probability of moving to the next state sn given that the agent was in state s and applied action a regardless of the rewards. In other words, the state-transition probability does not refer to the reward altogether. Therefore, all we have to do to infer it from the dynamics is to marginalise the dynamics with respect to the reward, which is what the below code snippet is doing.</p> <pre><code># state-transition probability induced from the dynamics\ndef ssa(p): \n    # states dim, action dim\n    nS, nA = p.shape[0], p.shape[3]\n    tr = np.zeros((nS, nS, nA))\n    for s in range(nS):\n        for a in range(nA):\n            for sn in range(nS):\n                tr[sn, s,a] = p[sn,:,s,a].sum()\n    return tr\n</code></pre> <p>Let us now apply this function on the obtained random walk dynamics.</p> <pre><code>tr = ssa(p)\nprint(tr.sum(0))\n# print(tr)\n</code></pre> <pre><code>[[1. 1.]\n [1. 1.]\n [1. 1.]\n [1. 1.]\n [1. 1.]\n [1. 1.]\n [1. 1.]]\n</code></pre> <p>Again the state-transition probability satisfies similar properties of the dynamics.</p> <p>Finally, we can apply a similar logic to obtain the reward function, see page 49 of the book.</p> <pre><code># reward function induced from the dynamics\ndef rsa(p, rewards):\n    # state dim, reward dim\n    nS, nA, nR = p.shape[0], p.shape[3], p.shape[1]\n    r = np.zeros((nS,nA))\n    for s in range(nS):\n        for a in range(nA):\n             for rn_, rn in enumerate(rewards): # get the reward rn and its index rn_\n                # print(rn_, rn)\n                r[s,a] += rn*p[:,rn_, s,a].sum()\n    return r\n</code></pre> <p>Note: that in the book authors use \\(r +\\gamma v(s)\\) to mean the reward at time step \\(t+1\\), in fact for consistency they should have used \\(r'\\) instead as they do for \\(s'\\). In our code treatment we have chosen to use \\(rn\\) in place of \\(r'\\).</p> <pre><code>rewards = randwalk().rewards_set()\nr = rsa(dynamics(env=randwalk(),  show=True), rewards)\nprint(r)\n</code></pre> <p></p> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,  2.05it/s]\n\n[[0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 0.]\n [0. 1.]\n [0. 0.]]\n</code></pre> <p>As we can see, the expected reward of the state-action pair (5, right)=1 because the agent obtains a reward of 1 when it is at state 5 and moves right (as it will end up in the far right terminal state with a reward of 1). The state-action pairs (6, right)=0 because the return/value for terminal states is always 0 (to be able to apply the recursive rule for the return, i.e. once the agent is at the terminal state 6 the process terminates and the agent will not receive any further reward).</p> <p>We can do the same for the longer randwalk_()</p> <pre><code>rewards = randwalk_().rewards_set()\nr = rsa(dynamics(env=randwalk_(), show=True), rewards)\nprint(r)\n</code></pre> <p></p> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:02&lt;00:00,  2.54s/it]\n\n[[ 0.  0.]\n [-1.  0.]\n [ 0.  0.]\n [ 0.  0.]\n [ 0.  0.]\n [ 0.  0.]\n [ 0.  0.]\n [ 0.  0.]\n [ 0.  0.]\n [ 0.  0.]\n [ 0.  0.]\n [ 0.  0.]\n [ 0.  0.]\n [ 0.  0.]\n [ 0.  0.]\n [ 0.  0.]\n [ 0.  0.]\n [ 0.  0.]\n [ 0.  0.]\n [ 0.  1.]\n [ 0.  0.]]\n</code></pre> <p>Can you explain the numbers we get above.</p>"},{"location":"unit2/lesson5/lesson5.html#dynamic-programming-methods","title":"Dynamic Programming Methods","text":"<p>Ok so we are ready now to move to Dynamic programming algorithms to solve the RL problem of finding a best estimate of a value function and or finding an optimal policy.</p>"},{"location":"unit2/lesson5/lesson5.html#policy-evaluation","title":"Policy evaluation","text":"<p>The first step to improving any policy is to evaluate how good or bad the policy is for the given task. This fundamental question can be addressed by tying up the task with a reward function that basically rewards the agent for achieving the task or a subtask that leads to the final goal. The agent's aim then becomes to collect as many rewards as possible (or to incur as few losses as possible), which should help the agent achieve the given task. One example is when a robot is moving in an environment, and we want it to reach a specific location, then we can reward/punish the robot for each step that is taking it close to the goal or away from it. But this awareness of the goal location is usually difficult to attain in real environments. Hence it is replaced by rewarding the agent when it reaches the goal or punishing the agent for each step taken without reaching the goal location.</p> <p>We can devise an evaluation strategy based on the discounted sum of rewards the agent is expected to collect while executing the task. The strategy depends on the dynamics of the environment. You may want to read section 4.1 and come back here to continue reading the code for the policy evaluation algorithm to get an insight into how it works.</p> <pre><code>def Policy_evaluation(env=randwalk(), p=None, V0=None, \u03c0=None, \u03b3=.99, \u03b8=1e-3, show=False): \n\n    # env parameters\n    nS, nA, nR, rewards = env.nS, env.nA, env.nR, env.rewards_set()\n    p = dynamics(env) if p is None else np.array(p)\n\n    # policy parameters\n    V = np.zeros(nS)     if V0 is None else np.array(V0); V[env.goals] = 0 # initial state values\n    \u03c0 = np.zeros(nS,int) if \u03c0  is None else np.array(\u03c0) # policy to be evaluated **stochastic/deterministic**\n\n    i=0\n    # policy evaluation --------------------------------------------------------------\n    while True:\n        \u0394 = 0\n        i+= 1\n        # show indication to keep us informed\n        if show: clear_output(wait=True); rng = trange(nS) # \n        else: rng = range(nS)\n        for s in rng:\n            if s in env.goals: continue # only S not S+\n            v, V[s] = V[s], 0            \n            for sn in range(nS): # S+\n                for rn_, rn in enumerate(rewards): # get the next reward rn and its index rn_\n                    if \u03c0.ndim == 1: # deterministic policy\n                        V[s] += p[sn,rn_, s,\u03c0[s]]*(rn + \u03b3*V[sn])\n                    else:           # stochastic policy \n                        V[s] += sum(\u03c0[s,a]*p[sn,rn_, s,a]*(rn + \u03b3*V[sn]) for a in range(nA))\n            \u0394 = max(\u0394, abs(v-V[s]))\n        if \u0394&lt;\u03b8: break\n    if show: \n        env.render(underhood='V', V=V)\n        print('policy evaluation stopped @ iteration %d:'%i)\n\n    return V\n</code></pre> <p>Note that we assume that when the policy is deterministic, it takes the shape (nS,) and its entries are actions ex. \u03c0[s]=1 means take right(1) when in state s.</p> <p>On the other hand if the policy is probabilistic, it has a shape of (nS, nA) and its entries are probabilities of each action given a state s, i.e \u03c0[a|s] written as \u03c0[s,a] in the code.</p> <p>Note that \\(\\gamma\\) must be \\(&lt; 1\\) to guarantee convergence of the Bellman equation because, in general, we do not know whether the policy guarantees reaching a terminal(goal) state; if we do, then \\(\\gamma=1\\) is ok. </p> <p>Refer to section 4.1 in the book: 'The existence and uniqueness of \\(v_\\pi\\) guaranteed as long as either  \\(\\gamma&lt; 1\\) or eventual termination is guaranteed from all states under the policy \\(\\pi\\)'.</p> <p>This condition can be relaxed when we move to sampling instead of dynamic programming in the next consequent lessons.</p> <p>Ok, let us test our policy evaluation method on a policy where the agent goes right always.</p> <pre><code>Policy_evaluation(\u03c0=[-1,1,1,1,1,1,-1], show=True)\n</code></pre> <p></p> <pre><code>policy evaluation stopped @ iteration 6:\n\n\n\n\n\narray([0.        , 0.96059601, 0.970299  , 0.9801    , 0.99      ,\n       1.        , 0.        ])\n</code></pre> <p>As we can see, the values for the deterministic policy that always move the agent to the right produce a set of state values of 1 for the non-terminal states 1:5. This is what we expect since the accumulated return of this policy, given that \u03b3=1, is 1 for these states since the agent is rewarded with 1 only when it reaches the far right terminal state, while when it is terminated at the far left state, it is rewarded with 0 (all other intermediate states also has a reward of 0).</p> <p>To further test our policy evaluation method, we will pass a different policy to it. This time we pass a simple probabilistic policy that gives left and right actions the same probability of 0.5. In this case, since \u03b3=1, the expected analytical values of the intermediate states are given as \u2159, 2/6, 3/6, 4/6, \u215a, which represent the probabilities of ending up in the far right state given the agent has started in them (so pr(s=1)=\u2159).</p> <pre><code>env = randwalk()\n\u03c0 = np.ones((env.nS,env.nA))*.5 # probability of moving left and right are equal\nPolicy_evaluation(env=env, \u03c0=\u03c0, show=True)\n</code></pre> <p></p> <pre><code>policy evaluation stopped @ iteration 19:\n\n\n\n\n\narray([0.        , 0.14840421, 0.30070568, 0.45997221, 0.62919208,\n       0.81145008, 0.        ])\n</code></pre> <pre><code>np.array([1/6, 2/6, 3/6, 4/6, 5/6])\n</code></pre> <pre><code>array([0.16666667, 0.33333333, 0.5       , 0.66666667, 0.83333333])\n</code></pre> <p>As we can see the values of this optimal policy (moving always right) progressively increase from left to right towards the goal that is most rewarding and the obtained values are exactly what we expected which is reassuring.</p> <p>Let us try to evaluate a random policy</p> <pre><code>env = randwalk()\n\u03c0 = np.random.randint(env.nA, size=env.nS, dtype=np.uint32)\nprint(\u03c0)\n</code></pre> <pre><code>[1 1 1 1 1 0 0]\n</code></pre> <p>Now we call our policy evaluaiton subroutine.</p> <pre><code>Policy_evaluation(env=env, \u03c0=\u03c0, show=True)\n</code></pre> <p></p> <pre><code>policy evaluation stopped @ iteration 1:\n\n\n\n\n\narray([0., 0., 0., 0., 0., 0., 0.])\n</code></pre> <p>As we can see the set of values are different than the above optimal policy and they are in harmony with the policy.</p>"},{"location":"unit2/lesson5/lesson5.html#policy-evaluation-for-2d-mdp-grid-world","title":"Policy Evaluation for 2d MDP Grid World","text":"<p>Let us test our policy evaluation algorithm on the following simple 3x3 grid world</p> <pre><code>env3x3 = Grid(gridsize=[3, 3], s0=0, goals=[8], figsize=[10,1])\n\u03c0 = [3, 1, 3, 1, 1, 3, 2, 2, -1] # guarantee to reach the terminal state hence \u03b3=1 is ok\n\u03c0_ = [3, 0, 0, 1, 2, 0, 0, 0, -1] # no way to terminal state hence \u03b3=1 leads to an infinite loop\nenv3x3.render(underhood='\u03c0', \u03c0=\u03c0)\n</code></pre> <p></p> <pre><code>V0 = [.6, .5, .5, .2, .1, .2, .1, .5, 0]\nV = Policy_evaluation(env=env3x3, \u03c0=\u03c0, V0=V0, \u03b3=.9, show=True)\nenv3x3.render(underhood='V', V=V)\n</code></pre> <p></p> <pre><code>V = Policy_evaluation(env=env3x3, \u03c0=\u03c0, V0=V0, \u03b3=1, show=True)\n# V = Policy_evaluation(env=env3x3, \u03c0=\u03c0_, V0=V0, \u03b3=1) #finite \n</code></pre> <p></p> <pre><code>policy evaluation stopped @ iteration 5:\n</code></pre> <p>Let us now try this on a slightly more complex environment such as a the maze world</p> <pre><code>env = maze()\n\u03c0 = np.ones(env.nS, dtype=np.uint32)*3 # always go up\nV = Policy_evaluation(env=env, \u03c0=\u03c0, show=True)\n</code></pre> <p></p> <pre><code>policy evaluation stopped @ iteration 6:\n</code></pre> <p>As we can see moving up yeild some benefits mainly in the cells that lead to the goal.</p> <p>Let us now generate a random policy and evaluate it for a maze environment.</p> <pre><code>env = maze()\n\u03c0 = np.random.randint(env.nA, size=env.nS, dtype=np.uint32)\nprint(\u03c0)\n</code></pre> <pre><code>[1 0 1 2 1 3 2 1 0 1 1 2 3 1 0 3 1 2 3 3 3 3 3 2 0 0 2 1 0 1 0 3 0 0 3 3 0\n 2 1 0 2 0 1 1 2 0 2 1 1 1 1 0 3 1]\n</code></pre> <pre><code>env.render(underhood='\u03c0', \u03c0=\u03c0)\n</code></pre> <p></p> <pre><code>V = Policy_evaluation(env=env, \u03c0=\u03c0, show=True)\n</code></pre> <p></p> <pre><code>policy evaluation stopped @ iteration 1:\n</code></pre> <p>As we can see, the randomly generated policy is chaotic and carry little value for the agent. However, evaluating different policies is highly important for an agent since it can guide the improvement of its adopted policy (based on this ability). One example is to keep evaluating random policies until some computational resources are consumed and pick the best. Below we show such a strategy of searching for an optimal policy. You can apply all other search algorithms that you have come across before in conventional AI (breadth-first etc.).</p> <pre><code>env = maze()\nVmax= np.zeros((env.nS))\np = dynamics(env)\nprint('Vmax before search', Vmax.sum())\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 945.09it/s]\n\nVmax before search 0.0\n</code></pre> <pre><code>import random\nrandom.seed(0)\nnp.random.seed(0)\n\nfor _ in trange(1000):\n    # \u03c0 = np.random.randint(env.nA, size=env.nS)\n    \u03c0 = choices(range(env.nA), k=env.nS)\n    V = Policy_evaluation(env=env, \u03c0=\u03c0, p=p, show=False)\n    if V.sum() &gt; Vmax.sum(): \n        Vmax = V\n        \u03c0max = \u03c0\n\nenv.render(underhood='\u03c0', \u03c0=\u03c0max)\nprint('Vmax after search', Vmax.sum())\n</code></pre> <p></p> <pre><code>Vmax after search 10.704460219301\n</code></pre> <p>As we can see, finding the optimal policy by random search is difficult since the space of policies is huge, making exhaustive or random search infeasible (dimensionality problem). We need a way to take and maintain a step in the right direction of improving the policy. As you have already seen in another module, a greedy search can often lead to a good result. The next section shows a simple but effective strategy to gradually improve a policy by taking a greedy step towards the solution.</p>"},{"location":"unit2/lesson5/lesson5.html#policy-iteration","title":"Policy Iteration","text":"<p>Now that we know how to evaluate a policy, it is time to improve it. Policy iteration is a basic and simple algorithm. It explicitly and iteratively tries first to reach a highly accurate estimate of the value function of the current policy, then it tries to improve the policy by maximising the probability of greedy actions as per the current value function. Evaluating the current policy fully and then improving it via policy iteration is inefficient, but it shows the fundamental ideas behind reinforcement learning. Please spend some time comprehending the code and reading the corresponding section 4.3 in the book.</p> <pre><code>def Policy_iteration(env=randwalk(), p=None, V0=None, \u03c00=None, \u03b3=.99, \u03b8=1e-3, show=False, epochs=None): \n\n    # env parameters\n    nS, nA, nR, rewards = env.nS, env.nA, env.nR, env.rewards_set()\n    p = dynamics(env) if p is None else np.array(p)\n\n    # policy parameters \n    V = np.zeros(nS)     if V0 is None else np.array(V0); V[env.goals] = 0 # initial state values\n    \u03c0 = np.zeros(nS,int) if \u03c00 is None else np.array(\u03c00); # initial **deterministic** policy \n    Q = np.zeros((nS,nA))  # state action values storage\n    # \u03c0 = randint(0,nA,nS)\n\n    j=0\n    while True if epochs is None else j&lt;epochs:\n        j+=1\n        # 1. Policy evaluation---------------------------------------------------\n        i=0\n        while True if epochs is None else i&lt;epochs:\n            \u0394 = 0\n            i+= 1\n            for s in range(nS): \n                if s in env.goals: continue # S not S+\n                v, V[s] = V[s], 0\n                for sn in range(nS): # S+\n                    for rn_, rn in enumerate(rewards): # get the reward rn and its index rn_\n                        V[s] += p[sn,rn_,  s, \u03c0[s]]*(rn + \u03b3*V[sn])\n\n                \u0394 = max(\u0394, abs(v-V[s]))\n            if \u0394&lt;\u03b8: print('policy evaluation stopped @ iteration %d:'%i); break\n\n        # 2. Policy improvement----------------------------------------------------\n        policy_stable=True\n        for s in range(nS):\n            if s in env.goals: continue # S not S+\n            \u03c0s = \u03c0[s]\n            for a in range(nA):\n                Q[s,a]=0\n                for sn in range(nS): # S+\n                    for rn_, rn in enumerate(rewards): # get the reward rn and its index rn_\n                        Q[s,a] += p[sn,rn_,  s,a]*(rn + \u03b3*V[sn]) \n\n            \u03c0[s] = Q[s].argmax() # simple greedy step\n            if \u03c0[s]!=\u03c0s: policy_stable=False\n\n        if policy_stable: print('policy improvement stopped @ iteration %d:'%j); break\n        if show: env.render(underhood='\u03c0', \u03c0=\u03c0)\n\n    return \u03c0, Q\n</code></pre> <p>We can now apply policy iteration to a proper Environments. We start by a random walk and then we move into grid world.</p> <pre><code>env=randwalk()\n\u03c0, Q = Policy_iteration(env, show=True)\n</code></pre> <p></p> <pre><code>policy evaluation stopped @ iteration 2:\npolicy improvement stopped @ iteration 6:\n</code></pre> <p>To visualise the policy we would need to pass the environment explicitly and then render the environment.</p> <pre><code>print(\u03c0)\n</code></pre> <pre><code>[0 1 1 1 1 1 0]\n</code></pre> <pre><code>env.render(underhood='\u03c0', \u03c0=\u03c0)\n</code></pre> <p></p> <pre><code>env.render(underhood='\u03c0', \u03c0=\u03c0)\n</code></pre> <p></p> <p>Note that we do not need to break ties randomly for the policy (we jsu use Q[s].argmax()) because we are not sampling.</p>"},{"location":"unit2/lesson5/lesson5.html#2-d-grid-world-mdp-examples","title":"2-d Grid World MDP Examples","text":"<p>Let us take another example, of a simple 3x3 grid world and run the policy iteration algorithm on. We start with a deterministic policy. The reward is 0 everywhere except for the goal where the agent will be rewarded by 1.</p> <pre><code>env3x3 = Grid(gridsize=[3, 3], reward='reward_1', s0=0, goals=[8], figsize=[10,1])\n\u03c00 = [3, 1, 3, 1, 1, 3, 2, 2, -1] # guarantee to reach the terminal state only if \u03b3&lt;1\n\u03c00_ = [3, 0, 0, 1, 2, 0, 0, 0, -1] # guarantee to reach the terminal state only if \u03b3&lt;1\n</code></pre> <p>Whether the initial policy is guaranteed to reach the terminal state or not, \u03b3=1 leads to an infinite loop, because subsequent policies are not guaranteed to reach the terminal state.</p> <pre><code> env3x3.render(underhood='\u03c0', \u03c0=\u03c00)\n</code></pre> <p></p> <p>Let us show the values before applying the agorithms</p> <pre><code>V0 = [.6, .5, .5, .2, .1, .2, .1, .5, 0]\nenv3x3.render(underhood='V', V=V0)\n</code></pre> <p></p> <pre><code>\u03c0, Q = Policy_iteration(env=env3x3, V0=V0, \u03c00=\u03c00, show=True)\n\u03c0, Q\n</code></pre> <p></p> <pre><code>policy evaluation stopped @ iteration 3:\npolicy improvement stopped @ iteration 2:\n\n\n\n\n\n(array([ 1,  1,  3,  1,  1,  3,  1,  1, -1]),\n array([[-4.90099501, -3.940399  , -4.90099501, -3.940399  ],\n        [-4.90099501, -2.9701    , -3.940399  , -2.9701    ],\n        [-3.940399  , -2.9701    , -2.9701    , -1.99      ],\n        [-3.940399  , -2.9701    , -4.90099501, -2.9701    ],\n        [-3.940399  , -1.99      , -3.940399  , -1.99      ],\n        [-2.9701    , -1.99      , -2.9701    , -1.        ],\n        [-2.9701    , -1.99      , -3.940399  , -2.9701    ],\n        [-2.9701    , -1.        , -2.9701    , -1.99      ],\n        [ 0.        ,  0.        ,  0.        ,  0.        ]]))\n</code></pre> <p>Let us now check the values after applying the algorithm</p> <pre><code>env3x3.render(underhood='V', V=Q.max(1))\n</code></pre> <p></p> <p>Note that if we set \u03b3=1 we need then to set epochs, otherwise, the algorithm enters an infinite loop because all state values converge to 1.</p> <p>Let us try it on a slightly more complex environment such as the maze.</p> <pre><code>env=maze()\n\u03c0 = Policy_iteration(env, show=True)[0]\n</code></pre> <p></p> <pre><code>policy evaluation stopped @ iteration 2:\npolicy improvement stopped @ iteration 16:\n</code></pre> <pre><code>env.render(underhood='\u03c0', \u03c0=\u03c0)\n</code></pre> <p></p>"},{"location":"unit2/lesson5/lesson5.html#stochastic-policy-iteration","title":"Stochastic Policy Iteration","text":"<p>We can also create a stochastic policy iteration method. It only differs from the deterministic policy iteration in two places (in the if \u03b5_greedy statements):  1. in the policy evaluation, we need to marginalize by multiplying by the action probability as we did earlier for the policy evaluation method 2. In calculating the probability of taking an optimal action (to account for the stochastic non-determinism nature of the policy), we allow the agent to take random actions \u03b5% of the time. </p> <p>We show the method below.</p> <pre><code>np.random.seed(1) # change the seed to get a different dynamics\n</code></pre> <pre><code># stochast policy iteration\ndef Policy_iteration_stoch(env=randwalk(), p=None, V0=None, \u03c00=None, \u03b3=.99, \u03b8=1e-4, \u03b5=.1, show=False): \n\n    # env parameters\n    nS, nA, nR, rewards = env.nS, env.nA, env.nR, env.rewards_set()\n    p = dynamics(env) if p is None else np.array(p)\n\n    # policy parameters\n    V = np.zeros(nS)      if V0 is None else np.array(V0); V[env.goals] = 0 # initial state values\n    \u03c0 = np.ones ((nS,nA)) if \u03c00 is None else np.array(\u03c00); \u03c0=\u03c0/\u03c0.sum(1)[:,None] # initial **stochastic** policy \n    Q = np.zeros((nS, nA)) # state action values storage\n    # \u03c0 = randint(1,nA,(nS,nA))\n\n    j=0\n    while True:\n        j+=1\n        # 1. Policy evaluation---------------------------------------------------\n        i=0\n        while True:\n            \u0394 = 0\n            i+= 1\n            for s in range(nS):\n                if s in env.goals: continue # S not S+\n                v, V[s] = V[s], 0\n                for sn in range(nS): # S+\n                    for rn_, rn in enumerate(rewards): # get the reward rn and its index rn_ \n                        # stochastic policy\n                        # V[s] += sum(\u03c0[s,a]*p[sn,rn_, s,a ]*(rn + \u03b3*V[sn]) for a in range(nA)) \n                        V[s] += p[sn,rn_,  s, \u03c0[s].argmax()]*(rn + \u03b3*V[sn])\n\n                \u0394 = max(\u0394, abs(v-V[s]))\n            if \u0394&lt;\u03b8: print('policy evaluation stopped @ iteration %d:'%i); break\n\n        # 2. Policy improvement----------------------------------------------------\n        policy_stable=True\n        for s in range(nS):\n            if s in env.goals: continue # S not S+\n            Qs = Q[s]\n            for a in range(nA):\n                Q[s,a]=0\n                for sn in range(nS): # S+\n                    for rn_, rn in enumerate(rewards): # get the reward rn and its index rn_\n                        Q[s,a] += p[sn,rn_, s,a]*(rn + \u03b3*V[sn])\n\n                if abs(Q[s,a]-Qs[a]) &gt; 0: policy_stable=False\n            \u03c0[s] = Qs*0 + \u03b5/nA\n            \u03c0[s,Q[s].argmax()] += 1-\u03b5     # greedy step  \n\n        if policy_stable: print('policy improvement stopped @ iteration %d:'%j); break\n\n    if show: env.render(underhood='maxQ', Q=Q)\n\n    return \u03c0, Q\n</code></pre> <pre><code>env=randwalk(figsize=[25,.5])\n\u03c0, Q = Policy_iteration_stoch(env=env, show=False)\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00, 3279.36it/s]\n\npolicy evaluation stopped @ iteration 1:\npolicy improvement stopped @ iteration 1:\n</code></pre> <pre><code>env.render(underhood='Q', Q=\u03c0)\n</code></pre> <p></p> <p>As we can see the policy chooses to take right majority of the times  1-\u03b5+ \u03b5/nA=.95 while taking left occasionally  \u03b5/nA.</p> <pre><code>env3x3 = Grid(gridsize=[3, 3], s0=0, goals=[8], figsize=[10,1])\nV0 = [.6, .5, .5, .2, .1, .2, .1, .5, 0]\nenv3x3.render(underhood='V', V=V0)\n</code></pre> <p></p> <pre><code>Q, \u03c0 = Policy_iteration_stoch(env3x3, \u03b3=.9, \u03b5=0, \u03b8=1e-5, show=True)\n\u03c0\n</code></pre> <p></p> <pre><code>array([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 1.],\n       [0., 0., 0., 0.],\n       [0., 1., 0., 0.],\n       [0., 0., 0., 0.]])\n</code></pre>"},{"location":"unit2/lesson5/lesson5.html#value-iteration-algorithm","title":"Value Iteration Algorithm","text":"<p>Our final step to fully develop the ideas of dynamic programming is to shorten the time it takes for a policy to be evaluated and improved. One simple idea we will follow here is to slightly improve the evaluation and immediately improve the policy. We do these two steps iteratively until our policy has stopped to improve. This is a very effective strategy because we do not wait until the policy is fully evaluated to improve it; we weave and interleave the two loops together in one loop. Below we show this algorithm. Read section 4.4 to further your understanding of this algorithm.</p> <pre><code>np.random.seed(1) # change the seed to get a different dynamics\n</code></pre> <pre><code>np.random.seed(1)\n\ndef value_iteration(env=randwalk(), p=None, V0=None, \u03b3=.99, \u03b8=1e-4, epochs=None, show=False): \n\n    # env parameters\n    nS, nA, nR, rewards, i = env.nS, env.nA, env.nR, env.rewards_set(), 0\n    p = dynamics(env) if p is None else np.array(p)\n\n    # policy parameters\n    V = np.zeros(nS) if V0 is None else np.array(V0); V[env.goals] = 0 # initial state values\n    Q = np.zeros((nS,nA)) # state action values storage\n\n    while True if epochs is None else i&lt;epochs:\n        \u0394 = 0\n        i+= 1\n        for s in range(nS):\n            if s in env.goals: continue\n            v, Q[s] = V[s], 0\n            for a in range(nA):\n                for sn in range(nS):\n                    for rn_, rn in enumerate(rewards):            # get the reward rn and its index rn_\n                        Q[s,a] += p[sn,rn_,  s,a]*(rn + \u03b3*V[sn])  # max operation is embedded now in the evaluation\n\n            V[s] = Q[s].max()                                     # step which made the algorithm more concise \n            \u0394 = max(\u0394, abs(v-V[s]))\n\n        if \u0394&lt;\u03b8: print('loop stopped @ iteration: %d , \u0394 = %2.f'% (i, \u0394)); break\n        if show: env.render(underhood='\u03c0', \u03c0=Q.argmax(1))\n\n    return Q\n</code></pre> <pre><code>env = randwalk()\n\u03c0 = value_iteration(env, show=True).argmax(1)\nprint('optimal action for state', \u03c0)\n</code></pre> <p></p> <pre><code>loop stopped @ iteration: 6 , \u0394 =  0\noptimal action for state [0 1 1 1 1 1 0]\n</code></pre> <p>As we can see our value iteration algorithm is effective in finding the optimal policy. Note that for the random walk environment the best policy is to move right to get a reward.</p>"},{"location":"unit2/lesson5/lesson5.html#value-iteration-on-a-grid-world","title":"Value Iteration on a Grid World","text":"<p>Now that you understand the value-iteration algorithm, you can apply it on a different and more complex envornment such as the grid world. As we did for the random walk, you have access to a grid world via the Grid class and its subroutines.</p> <pre><code>env3x3 = Grid(gridsize=[3, 3], reward='reward_1', s0=0, goals=[8], figsize=[10,1])\nV0 = [.6, .5, .5, .2, .1, .2, .1, .5, 0]\nenv3x3.render(underhood='V', V=V0)\n</code></pre> <p></p> <pre><code>Q = value_iteration(env3x3, \u03b3=.5, V0=V0, show=True)\nprint('optimal action for state', Q.argmax(1))\nQ\n</code></pre> <p></p> <pre><code>loop stopped @ iteration: 6 , \u0394 =  0\noptimal action for state [1 1 3 1 1 3 1 1 0]\n\n\n\n\n\narray([[-1.9375, -1.875 , -1.9375, -1.875 ],\n       [-1.9375, -1.75  , -1.875 , -1.75  ],\n       [-1.875 , -1.75  , -1.75  , -1.5   ],\n       [-1.875 , -1.75  , -1.9375, -1.75  ],\n       [-1.875 , -1.5   , -1.875 , -1.5   ],\n       [-1.75  , -1.5   , -1.75  , -1.    ],\n       [-1.75  , -1.5   , -1.875 , -1.75  ],\n       [-1.75  , -1.    , -1.75  , -1.5   ],\n       [ 0.    ,  0.    ,  0.    ,  0.    ]])\n</code></pre> <p>Let us try on a more complex grid environment.</p> <pre><code>env=grid()\nQ = value_iteration(env=env, show=True)\nprint('optimal action for state', Q.argmax(1))\n# env.render(underhood='\u03c0', \u03c0=Q.argmax(1))\n</code></pre> <p></p> <pre><code>loop stopped @ iteration: 10 , \u0394 =  0\noptimal action for state [1 1 1 1 1 1 3 0 0 0 1 1 1 1 1 1 3 0 0 0 1 1 1 1 1 1 3 0 0 0 1 1 1 1 1 1 0\n 0 0 0 1 1 1 1 1 1 2 0 0 0 1 1 1 1 1 1 2 0 0 0 1 1 1 1 1 1 2 0 0 0 1 1 1 1\n 1 1 2 0 0 0]\n</code></pre> <p>To interpret the policy we provided you with a useful function to render the environemt with its policy as shown above.</p> <p>As we can see the policy-iteration algorithm successfuly gave us the best policy for this simple environment.</p>"},{"location":"unit2/lesson5/lesson5.html#value-iteration-on-a-windy-grid-world","title":"Value Iteration on a Windy Grid World","text":"<p>Below we show the results of applying the value iteration method on a windy grid world. This is almost identical to the previous simple grid world without any obstacles, the only difference is that there is a wind blowing upwards, which shifts the agent 2 or 1 cell depending on its location. See page 130 of the book.</p> <pre><code>env=windy()\nQ = value_iteration(env, show=True)\nprint('optimal action for state', Q.argmax(1))\n</code></pre> <p></p> <pre><code>loop stopped @ iteration: 12 , \u0394 =  0\noptimal action for state [1 1 1 1 1 1 1 3 0 0 1 1 1 1 1 1 1 2 0 2 1 1 1 1 1 1 1 2 1 2 1 1 1 1 1 1 1\n 0 1 2 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 2]\n</code></pre>"},{"location":"unit2/lesson5/lesson5.html#value-iteration-on-a-maze","title":"value iteration on a maze","text":"<p>Let us now apply the policy-iteration on the maze env.</p> <pre><code>env=maze()\npolicy = value_iteration(env, show=True)\n# print('optimal action for state', policy)\n</code></pre> <p></p> <pre><code>loop stopped @ iteration: 14 , \u0394 =  0\n</code></pre>"},{"location":"unit2/lesson5/lesson5.html#conclusion","title":"Conclusion","text":"<p>In this lesson, we covered the main dynamic programming algorithms. We saw how evaluating a policy was extremely useful in being the key component to allowing us to improve the policy. We then developed a policy iteration algorithm which improves the policy in two main steps: 1. a step that evaluates the policy fully to reach an accurate estimation of the action values of the current policy 2. a step that improves the policy by adopting a greedy action. The usage of an action-value function Q(s,a) was key in allowing us to choose between actions since the state-value function V(s) does not differentiate between the values of actions</p> <p>We finally saw how the value-iteration algorithm has a similar structure to the policy-iteration algorithm with one important difference; it can arrive at an optimal policy by just taking a step towards the optimal policy by slightly refines its estimation of the action-value function without fully evaluating it.  Hence, it improves its policy more concisely and with much less overhead than the full policy iteration method.</p> <p>In the next lesson, we will take a different approach and move to cover sampling methods that do not use the dynamics of the environment explicitly and instead try to improve its policy by interacting with the environment.</p>"},{"location":"unit2/lesson5/lesson5.html#your-turn","title":"Your turn","text":"<ul> <li>Try to infer the state transition function tr(sn,s,a) and reward function r(s,a) for the randwalk_()</li> <li>Try the policy iteration on the maze8 which allows the agent to move diagonally, it is fun. You should arrive at an optimal policy for this maze environment where an agent can move diagonally and has 8 actions. You should be able to observe how efficient the policy is and how the agent is able to reach the goal location from anywhere in the environment.</li> </ul>"},{"location":"unit2/lesson5/lesson5.html#challenges","title":"Challenges","text":"<ol> <li>Alter policy_iteration_stoch to use a softmax policy instead of the \u03b5-greedy policy that we used in the policy_iteration_stoch method.</li> <li>You can challenge yourself also by trying to combine policy_iteration and policy_iteration_stoch() in one method. Use a flag such as \u03b5_greedy to distinguish between deterministic and probabilistic policies.</li> </ol>"},{"location":"unit2/lesson6/lesson6.html","title":"6. Monte Carlo","text":"<p>The notebook uses a library of functionality in RL that aims for simplicity and general insight into how algorithms work, these libraries are written from scratch using standard Python libraries (numpy, matplotlib etc.). Please note that you will need permission from the author to use the code for research, commercially or otherwise.</p>"},{"location":"unit2/lesson6/lesson6.html#lesson-5-tabular-methods-monte-carlo","title":"Lesson 5-Tabular Methods: Monte Carlo","text":"<p>Learning outcomes 1. understand the difference between learning the expected return and computing it via dynamic programming 1. understand the strengths and weaknesses of MC methods 1. appreciating that MC methods need to wait till the end of the task to obtain its estimate of the expected return 1. compare MC methods with dynamic programming methods 1. understand the implication of satisfying and not satisfying the explore-start requirement for the MC control and how to mitigate it via the reward function 1. understand how to move from prediction to control by extending the V function to a Q function and make use of the idea of generalised policy iteration-GPI 1. understand how policy gradient methods work and appreciate how they differ from value function methods</p> <p>Reading: The accompanying reading of this lesson is chapter 5 from our textbook by Sutton and Barto available online here. Please note that we explain the ideas of this topic from a practical perspective and not from a theoretical perspective, which is already covered in the textbook. Please note that off-policy methods are not covered and hence can be skipped safely when reading from the textbook.</p> <p>In this lesson, we develop the ideas of Monte Carlo methods. Monte Carlo methods are powerful and widely used in settings other than RL. You may have encountered them in a previous module where they were mainly used for sampling. We will also use them here to sample observations and average their expected returns. Because they average the returns, Monte Carlo methods have to wait until all trajectories are available to estimate the return. Later, we will find out that Temporal Difference methods do not wait until the end of the episode to update their estimate and outperform MC methods.</p> <p>Note that we have now moved to learning instead of computing the value function and its associated policy. This is because we expect our agent to learn from interacting with the environment instead of using the dynamics of the environment, which is usually hard to compute except for a simple lab-confined environment. </p> <p>Remember that we are dealing with expected return, and we are either finding an exact solution for this expected return as when we solve the set of Bellman equations or finding an approximate solution for the expected return as in DP or MC. Remember also that the expected return for a state is the future cumulative discounted rewards given that the agent follows a specific policy.</p> <p>One pivotal observation that summarises the justification for using MC methods over DP methods is that it is often the case that we are able to interact with the environment instead of obtaining its dynamics due to tractability issues.</p>"},{"location":"unit2/lesson6/lesson6.html#plan","title":"Plan","text":"<p>As usual, in general, there are two types of RL problems that we will attempt to design methods to deal with  1. Prediction problem For These problems, we will design Policy Evaluation Methods that attempt to find the best estimate for the value function given a policy.</p> <ol> <li>Control problems  For These problems, we will design Value Iteration methods that utilise Generalised Policy Iteration. They attempt to find the best policy by estimating an action-value function for a current policy and then moving to a better and improved one by often choosing a greedy action. They minimise an error function to improve their value function estimate, used to deduce a policy. We will then move to Policy Gradient methods that directly estimate a useful policy for the agent by maximising its value function.</li> </ol> <p>We start by assuming that the policy is fixed. This will help us develop an algorithm that predicts the state space's value function (expected return). Then we will move to the policy improvement methods, i.e. these methods that help us to compare and improve our policy with respect to other policies and move to a better policy when necessary. Then we move to the control case (policy iteration methods).</p>"},{"location":"unit2/lesson6/lesson6.html#first-visit-mc-policy-evaluation-prediction","title":"First visit MC Policy-evaluation (prediction)","text":"<p>Value-function approximation Method</p> <p>Because MC methods depend entirely on experience, a natural way to approximate the cumulative future discounted reward is by taking their average once they become available through experience. So we must collect the cumulative future discounted reward once this experience has elapsed. In other words, we need to take the sum after the agent has finished an episode for all the rewards obtained from the current state to the end of the episode. Then we average those returns over all of the available episodes. </p> <p>Note that MC methods only apply for episodic tasks, which is one of its limitations in addition to having to wait until the episode is finished.</p> <p>Note also that the agent can visit the same state more than once inside the same episode. We can take the sum starting from the first visit, or every visit, each yields a different algorithm. The first-visit algorithm is more suitable for tabular methods, while the every-visit algorithm is more suitable when using function approximation methods (such as neural networks).</p>"},{"location":"unit2/lesson6/lesson6.html#mrp-environment-for-prediction","title":"MRP environment for prediction","text":"<p>To be able to develop the methods of MC we would need to develop an MRP and MDP classes that is able to interact and collect experience from an environment for prediction and control, respectively. Below we show the skeleton of this class. But first we show some efficiency comparisons, you may skip directly to the MRP class section.</p> <pre><code>%matplotlib inline\n</code></pre> <pre><code>import numpy as np\nimport random\n\nfrom numpy.random import rand, seed, randint, choice\nfrom random import choices, sample\n</code></pre>"},{"location":"unit2/lesson6/lesson6.html#comparing-speed-for-random-number-generation","title":"Comparing speed for random number generation.","text":"<p>Before we get started, it is useful to study our options for random number generation. We will be using random number generation intensively when we sample. Below we show how each function will take to perform 10^6 random number generation for 2 actions. Qs = np.ones(4) Qs[1] = 2 Qs[3] = 2</p> <p>choice(np.where(Qs==Qs.max())[0]) # choices(Qs==Qs.max(), k=1)np.array(sample(range(1000), k=32))np.random.choice(1000, 32, replace=False)n=int(1e5) print('with replacement') %time for _ in range(n): choices(np.where(Qs==Qs.max())[0])[0] %time for _ in range(n): choice (np.where(Qs==Qs.max())[0]) print()</p>"},{"location":"unit2/lesson6/lesson6.html#sampling-without-replacement","title":"sampling without replacement","text":"<p>print('without replacement') %time for _ in range(n): sample(range(10000), k=32) %time for _ in range(n): np.random.choice(10000, 32, replace=False) print()</p>"},{"location":"unit2/lesson6/lesson6.html#when-we-have-a-binary-choice-such-as-when-use-greedy","title":"when we have a binary choice (such as when use \u03b5-greedy)","text":"<p>print('binary choices') %time for _ in range(n): np.random.randint(0,1) %time for _ in range(n): np.random.binomial(1, p=0.5) As we can see, the choice**s**() function is far more efficient. However, this is part of the story because the quality of the random number generation of these functions varies significantly. Usually, the more it takes, the better distribution it maintains. To that end, the binomial distribution seems to give a sweet spot for generating two actions, which is far more efficient than choice(), both of which are from numpy. However, the binomial is good for two actions only. If we want to deal with more actions, we can use np.random.multinomial, but it is less efficient than binomial. The choice**s**() function is the most efficient, but it has less quality which we compensate for by running more experiments (to eliminate the bias), while choice() is the least efficient of all of these functions. With that in mind, we develop the infrastructure for our RL algorithms. Of course, when we use random number generation, we need to use the seed() function to repeat a set of experiments consistently. The call will depend on the library that we use: random.seed(0) np.random.seed(0) Below we also show a useful function to obtain the last n elements of a circular array.</p>"},{"location":"unit2/lesson6/lesson6.html#retruns-indexes-of-last-n-elements-that-spans-two-edges-of-an-array-i-is-current-index","title":"retruns indexes of last n elements that spans two edges of an array, i is current index","text":""},{"location":"unit2/lesson6/lesson6.html#also-it-retruns-the-element-of-current-index","title":"also it retruns the element of current index","text":"<p>def circular_n(A, i, n):     N = len(A)     i, n, inds = i%N, min(i+1, n), np.ones(N, dtype=bool)                 inds[i+1: N+1 - (n-i)] = False  # turn off indexes that we do not want, to deal with circular indexes     return A[inds][-n:], A[i]</p> <p>A = circular_n(A=np.arange(100), i=105, n=10) Adef n_a_side(A=np.arange(24), n=4):     return circular_n(A, i=len(A)+n-1, n=2*n)[0] n_a_side()</p>"},{"location":"unit2/lesson6/lesson6.html#mrp-class-for-prediction","title":"MRP Class for prediction","text":"<p>In the following class, we will try to build a useful and generic MDP/MRP class that will serve our different needs in various RL coverage steps. In particular, we want the interact() and the steps() functions to be as flexible and generic as possible. Towards that end, we have constructed our class to have the following sections:</p> <ol> <li>Initialisation part: initialises the different variables necessary for our treatment</li> <li>Buffer storage section: store experience</li> <li>Steps section: takes a step in the environment and stores its correspondent dynamic (r,s,a). We have two types of steps: step_a(), suitable for most algorithms, and step_an(), which requires knowing the next action in advance. These two are useful in unifying the treatment of different RL algorithms, including prediction and control. For example, TD (prediction) and Q-learning(control) have a similar algorithm structure that entails using step_a(), while the Sarsa algorithm (control) uses step_an(). You will see these algorithms in the next lesson. Just be aware that you might want to change the default step function, step_a(), if your algorithm needs to know the next action, designated as an, to update its value function estimation.</li> <li>Interact section: this part is the heart and soul of our class. It runs several episodes, each with several steps, until a goal is reached, a buffer is full, or some other condition is met.</li> <li>Policy section: this is a set of policies according to which the agent will act. They can be either stationary (i.e., their probabilities do not change) or non-stationary (i.e., their probability will vary with Q, our action-value-function estimation).</li> <li>Metric section: to measure the performance of our algorithms. Basically, we use three metrics: </li> <li>the number of steps an agent took to reach a goal</li> <li>the sum of rewards an agent collected during an episode</li> <li>the root mean squared error of the value function estimation and the true values of an MDP or MTRP problem. This metric implies that we know a solution for a prediction in advance.</li> <li>Visualisation functions can be overridden in children's classes as per our needs.</li> </ol> <pre><code>from env.grid import *\n</code></pre> <pre><code>class MRP:\n\n    def __init__(self, env=randwalk(), \u03b3=1, \u03b1=.1, v0=0, episodes=100, view=1, \n                 store=False, # Majority of methods are pure one-step online and no need to store episodes trajectories \n                 max_t=2000, seed=None, visual=False, underhood='', \n                 last=10, print_=False):\n\n\n        # hyper parameters\n        self.env = env\n        self.\u03b3 = \u03b3\n        self.\u03b1 = \u03b1 # average methods(like MC1st) do not need this but many other methods (like MC\u03b1) do\n        self.v0 = v0\n        self.episodes = episodes\n        self.store = store\n        self.max_t = max_t\n        self.visual = visual\n        self.view = view\n        self.underhood = underhood\n        self.last = last\n        self.print = print_\n\n        # reference to two important functions\n        self.policy = self.stationary\n        self.step = self.step_a\n        # we might want to skip a step\n        self.skipstep = False\n\n        nA = self.env.nA\n        self.As = list(range(nA))\n        self.pAs = [1/nA]*nA\n\n        # useful to repeate the same experiement\n        self.seed(seed)\n        # to protect interact() in case of no training \n        self.ep = -1 \n\n    # set up important metrics\n    def init_metrics(self):\n        self.Ts = np.zeros(self.episodes, dtype=np.uint32)\n        self.Rs = np.zeros(self.episodes)\n        self.Es = np.zeros(self.episodes)  \n\n    def extend_metrics(self):\n        if len(self.Ts)&gt;=self.episodes: return # no need to resize if size is still sufficient\n        self.Ts.resize(self.episodes, refcheck=False)\n        self.Rs.resize(self.episodes, refcheck=False)\n        self.Es.resize(self.episodes, refcheck=False)\n\n    # set up the V table\n    def init_(self):\n        self.V = np.ones(self.env.nS)*self.v0\n\n    # useful for inheritance, gives an expected return (value) for state s\n    def V_(self, s=None): \n        return self.V  if s is None else self.V[s]\n\n    def seed(self, seed=None, **kw):\n        if seed is not None: np.random.seed(seed); random.seed(seed)\n    #-------------------------------------------buffer related-------------------------------------------------\n    # The buffer get reinitialised by reinitialising t only but we have to be careful not to exceed t+1 at any time\n    def allocate(self): \n        if not self.store: return\n        self.r = np.zeros(self.max_t)\n        self.s = np.ones(self.max_t, dtype=np.uint32)*(self.env.nS+10) # states are indices:*(nS+10)for debugging \n        self.a = np.ones(self.max_t, dtype=np.uint32)*(self.env.nA+10) # actions are indices:*(nA+10)for debugging       \n        self.done = np.zeros(self.max_t, dtype=bool)\n\n    def store_(self, s=None,a=None,rn=None,sn=None,an=None, done=None, t=0):\n        if not self.store: return\n\n        if s  is not None: self.s[t] = s\n        if a  is not None: self.a[t] = a\n        if rn is not None: self.r[t+1] = rn\n        if sn is not None: self.s[t+1] = sn\n        if an is not None: self.a[t+1] = an\n        if done is not None: self.done[t+1] = done\n\n    def stop_ep(self, done):\n        return done or (self.store and self.t+1 &gt;= self.max_t-1) # goal reached or storage is full\n\n    # ------------------------------------ experiments related --------------------------------------------\n    def stop_exp(self):\n        if self.stop_early(): print('experience stopped at episode %d'%self.ep); return True\n        return self.ep &gt;= self.episodes - 1\n\n    #----------------------------------- \ud83d\udc3esteps as per the algorithm style --------------------------------\n    def step_0(self):\n        s = self.env.reset()                                 # set env/agent to the start position\n        a = self.policy(s)\n        return s,a\n\n    # accomodates Q-learning and V style algorithms\n    def step_a(self, s,_, t):                          \n        if self.skipstep: return 0, None, None, None, True\n        a = self.policy(s)\n        sn, rn, done, _ = self.env.step(a)\n\n        # we added s=s for compatibility with deep learning\n        self.store_(s=s, a=a, rn=rn, sn=sn, done=done, t=t)\n\n        # None is returned for compatibility with other algorithms\n        return rn,sn, a,None, done\n\n    # accomodates Sarsa style algorithms\n    def step_an(self, s,a, t):                          \n        if self.skipstep: return 0, None, None, None, True\n        sn, rn, done, _ = self.env.step(a)\n        an = self.policy(sn)\n\n        # we added s=s for compatibility with deep learning later\n        self.store_(s=s, a=a, rn=rn, sn=sn, an=an, done=done, t=t)\n        return rn,sn, a,an, done\n\n    #------------------------------------ \ud83c\udf16 online learning and interaction --------------------------------\n    def interact(self, train=True, resume=False, episodes=0, grid_img=False, **kw):\n        if episodes: self.episodes=episodes\n        if train and not resume: # train from scratch or resume training\n            self.init_()\n            self.init()                                        # user defined init() before all episodes\n            self.init_metrics()\n            self.allocate()\n            self.plot0()                                       # useful to see initial V values\n            self.seed(**kw)\n            self.ep = -1 #+ (not train)*(self.episodes-1)\n            self.t_ = 0                                        # steps counter for all episodes\n\n        if resume: \n            self.extend_metrics()\n\n        try:\n            #for self.ep in range(self.episodes):\n            while not self.stop_exp():\n                self.ep += 1\n                self.t  = -1                                    # steps counter for curr episode\n                self.\u03a3r = 0\n                done = False\n                #print(self.ep)\n                # initial step\n                s,a = self.step_0()\n                self.step0()                                    # user defined init of each episode\n                # an episode is a set of steps, interact and learn from experience, online or offline.\n                while not self.stop_ep(done):\n                    #print(self.t_)\n\n                    # take one step\n                    self.t += 1\n                    self.t_+= 1\n\n                    rn,sn, a,an, done = self.step(s,a, self.t)  # takes a step in env and store tarjectory if needed\n                    self.online(s, rn,sn, done, a,an) if train else None # to learn online, pass a one step trajectory\n\n                    self.\u03a3r += rn\n                    self.rn = rn\n                    s,a = sn,an\n\n                    # render last view episodes, for games ep might&gt;episodes\n                    if self.visual and self.episodes &gt; self.ep &gt;= self.episodes-self.view: self.render(**kw)\n\n                # to learn offline and plot episode\n                self.metrics()\n                self.offline() if train else None\n                self.plot_ep()\n\n        except: print('training was interrupted.......!'); plt.pause(3)\n\n        # plot experience   \n        self.plot_exp(**kw)\n\n        return self  \n    #------------------------------------- policies types \ud83e\udde0-----------------------------------\n\n    def stationary(self, *args):\n        #return choice(self.As, 1, p=self.pAs)[0] # this gives better experiements quality but is less efficient\n        return choices(self.As, weights=self.pAs, k=1)[0] if self.env.nA!=2 else np.random.binomial(1, 0.5)\n\n    #---------------------------------------perfromance metrics\ud83d\udccf ------------------------------\n    def metrics(self):\n        # we use %self.episodes so that when we use a different criterion to stop_exp() code will run\n        self.Ts[self.ep%self.episodes] = self.t+1\n        self.Rs[self.ep%self.episodes] = self.\u03a3r\n        self.Es[self.ep%self.episodes] = self.Error()\n\n        if self.print: print(self)\n\n    def __str__(self):\n        # mean works regardless of where we stored the episode metrics (we use %self.episodes)     \n        Rs, R = circular_n(self.Rs, self.ep, self.last) # this function is defined above\n        metrics = 'step %d, episode %d, r %.2f, mean r last %d ep %.2f, \u03b5 %.2f'\n        values = (self.t_, self.ep, R, self.last, Rs.mean().round(2), round(self.\u03b5, 2))\n        return metrics%values\n\n    #------------------------functions that can be overridden in the child class-----------------\n    def init(self):\n        pass\n    def step0(self):\n        pass\n    def Error(self):\n        return 0\n    def stop_early(self):\n        return False\n    def plot0(self):\n        pass\n    def plot_t(self):\n        pass\n    def plot_ep(self):\n        pass\n    def plot_exp(self, *args):\n        pass\n    def offline(self):\n        pass\n    def online(self,*args):\n        pass\n    #---------------------------------------visualise \u270d\ufe0f----------------------------------------\n    # overload the env render function\n    def render(self, rn=None, label='', **kw):\n        if rn is None: rn=self.rn\n        param = {'V':self.V_()} if self.underhood=='V' else {}\n        self.env.render(**param, \n                        label=label+' reward=%d, t=%d, ep=%d'%(rn, self.t+1, self.ep+1), \n                        underhood=self.underhood, \n                        **kw)\n</code></pre> <p>As we can see, we defined a form of Markov Decision Process-MDP called Markov Reward Process-MRP. Like an MDP, an MRP is a stochastic process that concentrates on the rewards and states only and neutralizes the effect of actions. It is useful to study the predictive capabilities of an RL method where there are no decisions(actions) to be taken, and only we try to guess(predict) the returns of a process. </p> <p>Whenever there is a predictive algorithm, we will use MRP, while when we develop a control algorithm, we will use MDP. </p> <p>A typical example of an MRP is a random walk process, where an agent randomly moves left or right in a straight line of cells. A terminal state is at the end of each direction (left and right). The agent can be rewarded differently in each cell. Often, we reward the agent for moving to the far-right terminal state by 1 and everywhere else with 0. Another type of reward is to give the agent a negative -1 penalty on the far-left terminal state and 1 on the far-right state, and 0 everywhere else. See page 125 of the book.</p> <p>Note that the only assumption about the environment is to provide a reset() and a step() functions that abide by the following general form: 1. reset() must return a value of the initial state with a proper representation. So, when we move to function approximation, it must return a vector representing the state. 2. step() must return four values,  the first is the state (observation) that is compatible with what is returned by reset(). The second is the reward for the current state, and the third is a flag to signal the end of an episode; usually, when the agent achieves the required task or fails for some reason, each would have a corresponding suitable reward. A fourth is an empty dictionary of information we provided for compatibility with openAI environments.</p> <p>Let us now move to define our 1<sup>st</sup>-visit Monte Carlo prediction method. This method averages the return for only the first visit of a state in each episode.</p> <pre><code>def MC1st(MRP=MRP):\n    class MC1st(MRP):\n\n        def init(self):\n            self.store = True\n            self.\u03a3V   = self.V*0      #\u00a0the sum of returns for all episodes\n            self.\u03a3epV = self.V*0      #\u00a0counts for numbers of times we add to the return  \n\n        # ----------------------------- \ud83c\udf18 offline, MC learning: end-of-episode learning \ud83e\uddd1\ud83c\udffb\u200d\ud83c\udfeb --------------------------------    \n        # MC1stVisit average all past visits to a state in all episodes to get its return estimates\n        # we simply override the offline() function of the parent class\n        def offline(self):\n\n            #initialise the values\n            Vs = self.V*0\n            epV= self.V*0\n\n            # obtain the return for the latest episode\n            Gt = 0\n            for t in range(self.t, -1, -1):\n                s = self.s[t]\n                rn = self.r[t+1]\n\n                Gt = self.\u03b3*Gt + rn\n                Vs[s] = Gt\n                epV[s] = 1\n\n            # add the counts to the experience and obtain the average as per MC estimates\n            self.\u03a3V   += Vs\n            self.\u03a3epV += epV\n            ind = epV&gt;0 # avoids /0\n            self.V[ind] = self.\u03a3V[ind]/self.\u03a3epV[ind] \n\n    return MC1st\n</code></pre> <p>Let us now try our new class to predict the values of a random walk MRP.</p> <pre><code>MC = MC1st(MRP)\nMC = MC(episodes=1000, seed=10).interact()\nprint(MC.V[1:-1])\n</code></pre> <pre><code>[0.15780998 0.31454784 0.477      0.65342466 0.83831283]\n</code></pre> <p>As we can see the values are close to the analytical true values for this process given below.</p> <pre><code>pr = 1/(randwalk().nS-1) # 1/6 \nnp.arange(pr,1-pr, pr)   # true values\n</code></pre> <pre><code>array([0.16666667, 0.33333333, 0.5       , 0.66666667, 0.83333333])\n</code></pre>"},{"location":"unit2/lesson6/lesson6.html#mrp-with-visualisation","title":"MRP with visualisation","text":"<p>To help us to visualize the learning that is taking place in each episode, we have created a set of visualization functions that we will add to the MRP class. Familiarize yourself with these functions, they are self-explanatory. Mainly we have one function for plotting after each episode, not surprisingly called plot_ep(), and another function called plot_exp() that will be called at the end of the experience (after finishing all episodes). In addition, we have an Error() function to calculate the RMSE  between the true values and the predicted values of the states as well as plot_V() function that visualises the predicted values and true values to see visually how the algorithm is doing to come closer towards the true values.</p> <p>As we did with the Grid class, we will call the child name the same name as the parent (MRP) to help us keep the code consistent and simplify the treatments of our classes when we import a class. The downside is that you would have to re-execute the first parent and its subsequent children if you want to make some changes to the class since it will keep adding to previous definitions, so please be mindful of this point.</p> <p>We have also tried to reduce the overhead as much as possible for the new class by setting up visualisation only when it is necessary (when one of the plot functions is called)</p> <pre><code>class MRP(MRP):\n\n    def __init__(self, plotV=False,  plotT=False, plotR=False, plotE=False, animate=False, Vstar=None, **kw):\n        super().__init__(**kw)\n\n        # visualisation related\n        self.plotT = plotT\n        self.plotR = plotR\n        self.plotE = plotE\n        self.plotV = plotV \n        self.animate = animate\n        self.eplist = []\n\n        nS = self.env.nS\n        self.Vstar = Vstar if Vstar is not None else self.env.Vstar\n    #------------------------------------------- metrics\ud83d\udccf -----------------------------------------------  \n    # returns RMSE but can be overloaded if necessary\n    # when Vstar=0, it shows how V is evolving via training \n    def Error(self):\n        if self.Vstar is None: return 0\n        return np.sqrt(np.mean(((self.V_() - self.Vstar)[1:-1])**2)) #if self.Vstar is not None else 0\n\n    #--------------------------------------------visualise \u270d\ufe0f----------------------------------------------\n\n    def plot0(self):\n        if self.plotV: self.plot_V(); plt.show()\n\n    def plot_exp(self, label='', **kw):\n        self.plot_ep(animate=True, plot_exp=True, label=label)\n\n    def plot_ep(self, animate=None, plot_exp=False, label=''): \n        if len(self.eplist)&lt; self.episodes: self.eplist.append(self.ep+1)\n\n        if animate is None: animate = self.animate\n        if not animate: return\n        frmt='.--'if not plot_exp or self.ep==0 else '--'\n\n        if self.visual: \n            if self.ep==self.episodes-1: self.render(animate=False) # shows the policy \n            else:                        self.env.render(animate=False) \n        if self.plotV:  self.plot_V(ep=self.ep+1)        \n\n        i=2\n        for plot, ydata, label_ in zip([self.plotT, self.plotR, self.plotE], \n                                      [self.Ts,    self.Rs,    self.Es   ], \n                                      ['steps   ', '\u03a3rewards', 'Error   ']):\n            if not plot: continue\n            plt.subplot(1,3,min(i,3)).plot(self.eplist[:self.ep+1], ydata[:self.ep+1], frmt, label=label_+label)\n            plt.xlabel('episodes')\n            plt.legend()\n            i+=1\n\n        # if there is any visualisation required then we need to care for special cases    \n        if self.plotV or self.plotE or self.plotT or self.plotR:\n            figsizes = list(zip(plt.gcf().get_size_inches(), self.env.figsize0))\n            figsize  = [max(figsizes[0]), min(figsizes[1]) if self.plotV or self.plotE else figsizes[1][0]]\n            plt.gcf().set_size_inches(figsize[0], figsize[1])\n            clear_output(wait=True)\n            if not plot_exp: plt.show()\n\n\n    def plot_V(self, ep=0):\n\n        self.env.ax0 = plt.subplot(1,3,1) # to add this axis next to a another axis to save some spaces\n#         plt.gcf().set_size_inches(16, 2)\n\n        # get letter as state names if no more than alphabet else just give them numbers\n        letters = self.env.letters_list()[1:-1] if self.env.nS&lt;27 else list(range(self.env.nS-2))\n\n        # plot the estimated values against the optimal values\n        plt.plot(letters, self.V_()[1:-1], '.-', label='V episode=%d'%(ep)) # useful for randwalk\n        plt.plot(letters, self.Vstar[1:-1],'.-k')\n\n        # set up the figure\n        plt.xlabel('State', fontsize=10)\n        plt.legend()\n        plt.title('Estimated value for %d non-terminal states'%(self.env.nS-2), fontsize=10)\n        plt.gca().spines['right'].set_visible(False)\n        plt.gca().spines['top'].set_visible(False)\n</code></pre> <p>Ok to reap the benefit of this newly defined MRP, we would have had to go back to the Jupyter-cell where we defined the MC1st class and run. However, because we have used a class factory function with MC1st we can just call it and it will dynamically update the MV1st class with teh new MRP base class. Below we show you how to do it.</p>"},{"location":"unit2/lesson6/lesson6.html#applying-mc1st-on-a-prediction-problem","title":"Applying MC1st on a prediction problem","text":"<p>Let us now run MC1st with the latest useful visualisation.</p> <pre><code>MC1st = MC1st(MRP)\n</code></pre> <p>Note that if you get an error after changing something in the MRP class and rerunning then simply restart the kernel and run from scratch.</p> <pre><code>mc = MC1st(env=randwalk(), episodes=100, plotV=True, plotE=True, seed=1).interact()\n</code></pre> <p></p> <p>As you can see we have called  MC = MC1st(MRP) to make sure that we are dealing with latest MRP definition.</p> <pre><code>mc = MC1st(episodes=10, plotE=True).interact()\n</code></pre> <p></p> <pre><code>mc = MC1st(episodes=10, plotV=True).interact()\n</code></pre> <p></p> <p>Ok one more thing, to avoid passing the value of plotE=True, plotV=True, animate=True, whenever we want to demo a prediction algorithm, we can create a dictionary and store these values in it and then pass the reference to the MC1st call, below we show how.</p> <pre><code>demoV = {'plotE':True, 'plotV':True, 'animate':True} # suitable for prediction\n</code></pre> <pre><code>mc = MC1st(episodes=100, **demoV, seed=1).interact()\n</code></pre> <p></p> <pre><code>mc.ep\n</code></pre> <pre><code>99\n</code></pre> <pre><code>mc.V\n</code></pre> <pre><code>array([0.        , 0.16666667, 0.34210526, 0.5       , 0.63291139,\n       0.79365079, 0.        ])\n</code></pre>"},{"location":"unit2/lesson6/lesson6.html#mdp-environment-for-control","title":"MDP environment for control","text":"<p>Let us now extend our MRP class to deal with control. We would need to deal with the Q action-value function instead of the value function V. Also lacking is a set of non-stationary policies that allows us to take advantage of the Q action-value function. Below we show this implementation. </p> <p>We also use a class factory to define our MDP class. Doing so will save us from redefining the class again when we amend our MRP class. We will need to amend the MRP when we change the state representation to use function approximation in the next unit.</p> <p>The q0 is the initial set of values we might want to set up for all our Q estimates. We can also opt for completely random values for each action-value pair, we have left this out for simplicity of the coverage, but you can try it yourself.  \u03b5 is the percentage of time we want our agent to explore. </p> <p>The class defines a set of policy-related functions that revolve around the \u03b5-greedy policy. We have implemented a simple, deterministic greedy policy that always chooses the first max Q action greedy_(). The main difference between greedy_() and \u03b5greedy() for \u03b5=0 is that the latter stochastically chooses between multiple optimum actions with the same action-value function. This is useful when we use exploration by optimistic initialisation since the greedy_() function can cause action starvation (a phenomenon where the action is never selected). Nevertheless, greedy_() is useful to test the optimality of a learned policy (once learning finishes) and is used within \u03c0isoptimal() function.</p> <p>The \u03c0isoptimal() function returns whether the current policy is optimal by checking if the agent can reach the goal in a predefined number of steps stored in self.Tstar. The \u03c0() function returns the probability of taking a certain action under the \u03b5-greedy policy. Finally, the render() function deals with rendering a policy.</p> <p>Below we show a simple example of how choices will work when we use weights to choose an action according to its Q value.</p> <pre><code>Qs = np.array([10, 20, 20, 20])\n\u03c4 = 10\nexp = np.exp(Qs/\u03c4)\nchoices(range(4), weights=exp/exp.sum(), k=1)[0]\n</code></pre> <pre><code>1\n</code></pre> <pre><code>np.where(Qs==Qs.max())[0]\n</code></pre> <pre><code>array([1, 2, 3])\n</code></pre> <pre><code>a=4\nmaxAs = [1,2 ,3]\na in maxAs\n</code></pre> <pre><code>False\n</code></pre> <pre><code>choices([5,2,6])\n</code></pre> <pre><code>[6]\n</code></pre> <pre><code>def MDP(MRP=MRP):\n    class MDP(MRP):\n        def __init__(self, env=grid(), commit_ep=0, \u03b5=.1, \u03b5min=0.01, d\u03b5=1, \u03b5T=0, q0=0, Tstar=0, **kw): \n\n            super().__init__(env=env, **kw)\n            # set up hyper parameters\n            self.\u03b5 = \u03b5 \n            self.\u03b50 = \u03b5  # store initial \n            self.d\u03b5 = d\u03b5 # for exp decay\n            self.\u03b5T = \u03b5T # for lin decay\n            self.\u03b5min = \u03b5min\n\n            # override the policy to \u03b5greedy to make control possible\n            self.policy = self.\u03b5greedy\n\n            # initial Q values\n            self.q0 = q0\n\n            # which episode to commit changes\n            self.commit_ep = commit_ep\n\n            # number of steps for optimal policy\n            self.Tstar = Tstar\n\n        # set up the Q table\n        def init_(self):\n            super().init_() # initialises V\n            self.Q = np.ones((self.env.nS,self.env.nA))*self.q0\n\n        #------------------------------------- add some more policies types \ud83e\udde0-------------------------------\n        # useful for inheritance, gives us a vector of actions values\n        def Q_(self, s=None, a=None):\n            return self.Q[s] if s is not None else self.Q\n\n        # directly calculates V as a \u03c0[s] policy expectation of Q[s] \n        def V_from_Q(self, s=None):\n            return self.Q_(s)@self.\u03c0(s)\n\n        # returns a pure greedy action, **not to be used in learning**\n        def greedy_(self, s):\n            return np.argmax(self.Q_(s))\n\n\n        # greedy stochastic MaxQ\n        def greedy(self, s): \n            self.isamax = True\n            # instead of returning np.argmax(Q[s]) get all max actions and return one of the max actions randomly\n            Qs = self.Q_(s)\n            #print(Qs)\n            if Qs.shape[0]==1: raise ValueError('something might be wrong number of actions ==1')\n            return choices(np.where(Qs==Qs.max())[0])[0] # more efficient than choice\n            #return choice(np.where(Qs==Qs.max())[0])\n\n\n        # returns a greedy action most of the time\n        def \u03b5greedy(self, s):\n            # there is pr=\u03b5/nA that a max action is chosen but is not considered max, we ignored it in favour of efficiency\n            self.isamax = False \n            if self.d\u03b5 &lt; 1: self.\u03b5 = max(self.\u03b5min, self.\u03b5*self.d\u03b5)              # exponential decay\n            if self.\u03b5T &gt; 0: self.\u03b5 = max(self.\u03b5min, self.\u03b50 - self.t_ / self.\u03b5T) # linear      decay\n\n            return self.greedy(s) if rand() &gt; self.\u03b5 else randint(0, self.env.nA)\n\n        # returns the policy probabilities (of selecting a specific action)\n        def \u03c0(self, sn,  a=None):\n            \u03b5, nA, Qsn = self.\u03b5, self.env.nA, self.Q_(sn)\n            \u03c0_ = Qsn*0 + \u03b5/nA\n            \u03c0_[Qsn.argmax()] += 1-\u03b5\n            return \u03c0_ if a is None else \u03c0_[a]\n\n        # returns whether the current policy is optimal by checking if agent can reach the goal in self.Tstar\n        def \u03c0isoptimal(self):\n            s = self.env.reset()\n            done = False\n            for t in range(self.Tstar):\n                s,_, done,_ = self.env.step(self.greedy_(s))\n            return done\n\n        #---------------------------------------visualise \u270d\ufe0f----------------------------------------\n        # override the render function\n        def render(self, rn=None, label='', **kw):\n            if rn is None: rn=self.rn\n            param = {'Q':self.Q_()} if 'Q' in self.underhood else {} # 'maxQ' or 'Q'\n            self.env.render(**param, \n                            label=label+' reward=%d, t=%d, ep=%d'%(rn, self.t+1, self.ep+1), \n                            underhood=self.underhood, **kw)\n\n    return MDP\n</code></pre> <p>You might have realised that we used a class factory for MDP. This is because we want our class to be flexible later to accommodate for changes in the MRP parent class. That is if we change the MRP class in later lessons we do not need to restate the MDP definition to inherit from the new MRP class, instead we just pass MDP(MRP) where MRP will be taken as the latest definition. This is will be appreciated in later lessons.</p>"},{"location":"unit2/lesson6/lesson6.html#first-visit-mc-control","title":"First-visit MC control","text":"<p>Now we extend this class to overload the offline function to offload it with our 1<sup>st</sup>-visit Monte Carlo method for control.</p> <pre><code>class MC1stControl(MDP()):\n\n    def init(self):\n        self.store = True\n        self.\u03a3Q   = self.Q*0      #\u00a0the sum of returns for all episodes\n        self.\u03a3epQ = self.Q*0      #\u00a0counts for numbers of times we add to the return  \n\n    def offline(self): \n        #initialise the values\n        Qs = self.Q*0\n        epQ= self.Q*0\n\n        # obtain the return for the latest episode\n        Gt = 0\n        for t in range(self.t, -1, -1):\n            s = self.s[t]\n            a = self.a[t]\n            rn = self.r[t+1]\n\n            Gt = self.\u03b3*Gt + rn\n            Qs[s,a] = Gt\n            epQ[s,a] = 1\n\n        # add the counts to the experience and obtain the average as per MC estimates\n        self.\u03a3Q   += Qs\n        self.\u03a3epQ += epQ\n        ind = epQ&gt;0 # avoid /0\n        self.Q[ind] = self.\u03a3Q[ind]/self.\u03a3epQ[ind] \n</code></pre>"},{"location":"unit2/lesson6/lesson6.html#applying-mc-on-a-control-problem","title":"Applying MC on a control problem","text":"<p>Similar to what we did for prediction, we get help from a dictionary that stores a set of useful configurations that we use often. In the case of control, the most useful is plotting the number of steps the agent took to reach a terminal state in each episode or the sum of rewards the agent collected in each episode. Each one of these plots can be useful for certain tasks. Bear in mind that if the reward is given only for reaching the goal location or terminal state, the sum of the rewards plot would be a constant line that does not convey useful information. Below we show each.</p> <pre><code>demoT = {'plotT':True, 'visual':True, 'underhood':'maxQ'}                 # suitable for control\ndemoR = {'plotR':True, 'visual':True, 'underhood':'maxQ'}                 # suitable for control\ndemoTR= {'plotT':True, 'plotR':True, 'visual':True, 'underhood':'maxQ'}   # suitable for control\ndemoQ = demoT # alias\n</code></pre> <p>We can go a bit further and define a set of useful functions that we can utilise in all of our lessons which saves us from having to redefine the above dictionaries as follows.</p> <pre><code>def demo(what='V'):\n    switch = {\n        'V':    {'plotE':True, 'plotV':True, 'animate':True},                    # suitable for prediction\n        'T':    {'plotT':True, 'visual':True, 'underhood':'maxQ'},               # suitable for control\n        'R':    {'plotR':True, 'visual':True, 'underhood':'maxQ'},               # suitable for control\n        'TR':   {'plotT':True, 'plotR':True, 'visual':True,'underhood':'maxQ'},  # suitable for control\n        'Game': {'plotT':True, 'plotR':True, 'visual':True, 'animate':True}      # suitable for games\n    }\n    return switch.get(what,{})\ndef demoV(): return demo('V')\ndef demoT(): return demo('T')\ndef demoQ(): return demo('T')# alias\ndef demoR(): return demo('R')\ndef demoTR(): return demo('TR')\ndef demoGame(): return demo('Game')\n</code></pre> <p>Ok, back to our MC algorithm. Unfortunately, applying the MC control algorithm with the default reward function will not yield a useful policy. This is because the explore-start condition is not satisfied (refer to section 5.4 of our book). In addition, averaging solutions may not perform well because they do not track a changing policy well for non-stationary problems (most of the control problems are non-stationary). To see this, uncomment the lines in the cell below and run it. (Note that we have set up the priorities of the actions in a way that will show this issue (right comes before left and down before up)</p> <pre><code>mc = MC1stControl(env=grid(), \u03b3=1, episodes=200,  seed=10, **demoQ()).interact()\n</code></pre> <p></p>"},{"location":"unit2/lesson6/lesson6.html#the-role-of-the-discount-factor-gamma-for-delayed-reward","title":"The role of the discount factor \\(\\gamma\\) for delayed reward","text":"<p>Important Note It is always the case that when we use a delayed reward (which is the default reward for our Grid class), the discount factor \\(\\gamma\\) must not be set to 1. This is because the sum of the discounted rewards of each visited state will be equal to the delayed reward itself, which will not give any particular advantage to follow a shorter path, yielding a useless policy. Therefore, we can solve this issue  1. either by providing a discounted value for \\(\\gamma\\) that &lt; 1. 1. or by changing the reward to have intermediate steps reward, which, when accumulated, will provide distinguished sums for the different paths and hence help distinguish the shortest path or the policy that will yield an optimal reward.</p>"},{"location":"unit2/lesson6/lesson6.html#solution-1","title":"Solution 1","text":"<p>Below we show how we can simply reduce \\(\\gamma\\) to solve this issue.</p> <pre><code>mc = MC1stControl(env=grid(), \u03b3=.99, episodes=30, seed=10, **demoTR()).interact()\n</code></pre> <p></p>"},{"location":"unit2/lesson6/lesson6.html#solution-2","title":"Solution 2","text":"<p>Also we can compensate for the above issue, we would need to set up a reward function that allows the agent to quickly realise when it stuck in some not useful policy.</p> <pre><code>env1 = grid(reward='reward_1')\nmcc = MC1stControl(env=env1, episodes=30, seed=0, **demoTR()).interact()\n</code></pre> <p></p> <p>Compare the above policy with the one produced by the DP solution in lesson 2. You will notice that the MC solution does not give a comprehensive solution from all states because we do not start from different cells. The starting position is fixed. The exploration nature of the policy allowed the agent to develop an understanding through its Q function of where it should head if it finds itself in a specific cell. The Markovian property is essential in guaranteeing that this can be safely assumed.</p> <p>You might have noticed that although the task is very straightforward, the agent detoured a bit from the simplest straight path that leads to the goal. Bear in mind that we are adopting an \u03b5greedy policy by default, which means that the agent will take some explorative actions 10% of the time. But this should not have prevented the maxQ policy from pointing towards the goal. This is because of the nature of MC itself and its sampling averages. The next section demonstrates how we can overcome this difficulty.</p> <p>We can play with the exploration but that is needs lots of trail and is not straightforward.</p> <pre><code>mc = MC1stControl(env=grid(), \u03b3=.97, episodes=50, \u03b5=.5, d\u03b5=.99, seed=20, **demoTR()).interact()\n</code></pre> <p></p> <pre><code>mc = MC1stControl(env=grid(), \u03b3=.97, episodes=50, \u03b5=.5, \u03b5T=3000, seed=20, **demoTR()).interact()\n</code></pre> <p></p> <pre><code>mcc = MC1stControl(env=grid(reward='reward_1'), \u03b3=.97, episodes=100, \u03b5=0.9, d\u03b5=.999, seed=20, **demoTR()).interact()\n</code></pre> <p></p>"},{"location":"unit2/lesson6/lesson6.html#demos-related","title":"Demos Related","text":"<p>Note how the arrows represent the policy change from one episode to another. We have turned off showing the agent's movements inside all but the last episode because it is usually unnecessary. If you want to see a specific episode, just set the 'episodes' variable to it, and you will be able to. For example, if you want to see what is happening in episode 3, set episodes=3 (to guarantee seeing exactly the same episode every time you repeat the experiment, you would need to fix the seed).</p> <p>Please differentiate between seeing the arrows changing from one episode to another and when you see them changing inside an episode. Inside an episode, the arrow of a state changes only when the agent visits a state. The exception to this rule is when we use planning (or eligibility traces), where an arrow of a cell can change way after it has been visited. This is because we store those visits in these methods and reuse them in our updates. We will examine planning and eligibility traces in later lessons.</p> <p>We can choose to plot and animate at the same time. Bear in mind that this will slow down the process a bit.  If the learning is slow anyway, such as in Atari, then it makes sense to animate and plot as it will keep you informed about which episode your agent is in and how well it is doing so far! It is better to keep the animation turned off for ordinary classical environments as those do not take time anyway.</p> <p>Important Notes Regarding Demos Notice how the visualisation behaves for demoGame() vs demoTR(), demoT() and demoR(). </p> <p>When we use demoTR(), demoT() or demoR(), the algorithm will train the agent silently without showing the plots and then at the last few episodes (as per view variable which is usually 1; meaning last episode), it shows a demo and then shows the performance plots. demoTR(), demoT() and demoR() are usually more efficient and take up less time.</p> <p>On the other hand, when we use demoGame(), the algorithm will show the performance plots progress live from one episode to another. Then, in the final few episode (according to view), it shows a demo and the performance plots. During the demo, the plots disappear, and they reappear at the end. This helps keep our code as tidy and efficient as possible.</p> <pre><code>mcc = MC1stControl(env=grid(reward='reward_1'), episodes=100, seed=0, **demoGame()).interact()\n</code></pre> <p></p>"},{"location":"unit2/lesson6/lesson6.html#plots-without-demos","title":"Plots without Demos","text":"<p>Let us see how to plot only The most efficient way is just to turn off animate and set plotT or plotR to True.</p> <pre><code>mcc = MC1stControl(env=grid(reward='reward_1'), episodes=40, seed=10, plotT=True, animate=False).interact()\n</code></pre> <p></p> <p>we can also plot live as the algorithm is training</p> <pre><code>mcc = MC1stControl(env=grid(reward='reward_1'), episodes=40, seed=10, plotT=True, animate=True).interact()\n</code></pre> <p></p>"},{"location":"unit2/lesson6/lesson6.html#demos-without-training","title":"Demos without Training","text":"<p>We can also run without training, Unfortunately that means that we would have to loose the training traces of the last few episodes that we want to visualise and replace them with the latest performance after training. </p> <pre><code>mcc.ep = mcc.ep - 5\nmcc.plotT = False\nmcc.visual = True\n# mcc.underhood='maxQ' # uncomment to see also the policy\nmcc.interact(train=False, **demoGame())\n</code></pre> <p></p> <pre><code>&lt;__main__.MC1stControl at 0x12bc096a0&gt;\n</code></pre> <p></p>"},{"location":"unit2/lesson6/lesson6.html#resume-training-after-stopping-it-during-the-allocated-episodes","title":"Resume training after stopping it during the allocated episodes","text":"<p>We can also resume training after we have stopped it. This can be very useful when we are faced with an error outside our control while training. For example when we train a robot simulation it is sometimes necessary to stop training if the environment become irresponsive. This mechanism is tested below, to do so, run the first cell which train for 1000000 episode, stop the training by pressing on the stop button above or by esc then i i, then execute the next cell.</p> <pre><code>mcc = MC1stControl(env=grid(reward='reward_1'), episodes=100, seed=10, plotT=True, animate=True).interact()\n</code></pre> <p></p> <pre><code>mcc.interact(resume=True)\n</code></pre> <pre><code>&lt;__main__.MC1stControl at 0x12dbb09b0&gt;\n</code></pre> <p></p>"},{"location":"unit2/lesson6/lesson6.html#extend-training-beyond-the-initial-number-of-episodes","title":"Extend training beyond the initial number of episodes","text":"<p>We can also extend training, for example we trained for a 100 episodes and then we would like to extend training for another 100 episodes. To do so we just call interact(episodes=120). We show this below.</p> <pre><code>mcc.interact(resume=True, episodes=120)\n</code></pre> <pre><code>&lt;__main__.MC1stControl at 0x12dbb09b0&gt;\n</code></pre> <p></p> <p>Decreasing the number of episodes will not result any training and it will not remove early training as it should. Below we show that and we also show that it will not matter whether we pass the episodes in interact() or via the algorithms instance.</p> <pre><code>mcc.episodes = 90\nmcc.interact(resume=True)\n</code></pre> <pre><code>&lt;__main__.MC1stControl at 0x12dbb09b0&gt;\n</code></pre> <p></p>"},{"location":"unit2/lesson6/lesson6.html#incremental-constant-mc-every-visit-mc-prediction","title":"Incremental constant-\u03b1 MC: Every-visit MC Prediction","text":"<pre><code>class MC(MRP):\n\n    def init(self):\n        self.store = True\n\n    # ----------------------------- \ud83c\udf18 offline, MC learning: end-of-episode learning ----------------------    \n    def offline(self):\n        # obtain the return for the latest episode\n        Gt = 0\n        for t in range(self.t, -1, -1):\n            s = self.s[t]\n            rn = self.r[t+1]\n\n            Gt = self.\u03b3*Gt + rn\n            self.V[s] += self.\u03b1*(Gt - self.V[s])\n</code></pre> <p>This type of algorithmic design is more flexible and will be used in general in RL instead of the implementation that requires storing the sums or averages.</p>"},{"location":"unit2/lesson6/lesson6.html#apply-incremental-mc-on-prediction-problem","title":"Apply incremental MC on prediction problem","text":"<p>Let us try our new shiny prediction algorithm on the random walk problem.</p> <pre><code>mc = MC( \u03b1=.02, episodes=50, **demoV()).interact()\n</code></pre> <p></p> <p>Notice how jumpy the MC is.</p>"},{"location":"unit2/lesson6/lesson6.html#incremental-mcc-every-visit-mc-control","title":"Incremental MCC: Every-visit MC Control","text":"<pre><code># note that the name has double C: we are dealing with MC+Control\nclass MCC(MDP()):\n\n    def init(self):\n        self.store = True\n\n    # ---------------------------- \ud83c\udf18 offline, MC learning: end-of-episode learning \ud83e\uddd1\ud83c\udffb\u200d\ud83c\udfeb -----------------------    \n    def offline(self):  \n        # obtain the return for the latest episode\n        Gt = 0\n        for t in range(self.t, -1, -1):\n            s = self.s[t]\n            a = self.a[t]\n            rn = self.r[t+1]\n\n            Gt = self.\u03b3*Gt + rn\n            self.Q[s,a] += self.\u03b1*(Gt - self.Q[s,a])\n</code></pre> <pre><code>mcc = MCC(env=grid(reward='reward1'), \u03b1=.2, episodes=1, seed=0, **demoQ()).interact()\n</code></pre> <pre><code>env2x3 = Grid(gridsize=[2, 3],  s0=0, goals=[5], figsize=[10,1])\nV0 = np.array([.3, .4, .5, .2, .3, 0])*10\nenv2x3.render(underhood='V', V=V0)\n</code></pre> <pre><code>mcc = MCC(env=env2x3, \u03b1=.1, \u03b3=.9, episodes=1, seed=0, **demoQ()).interact()\n</code></pre> <pre><code>mcc.s[:mcc.t+2]\n</code></pre> <pre><code>array([0, 3, 4, 5], dtype=uint32)\n</code></pre> <pre><code>mcc.Q\n</code></pre> <pre><code>array([[0.   , 0.   , 0.   , 0.081],\n       [0.   , 0.   , 0.   , 0.   ],\n       [0.   , 0.   , 0.   , 0.   ],\n       [0.   , 0.09 , 0.   , 0.   ],\n       [0.   , 0.1  , 0.   , 0.   ],\n       [0.   , 0.   , 0.   , 0.   ]])\n</code></pre> <pre><code>s = 2\n\u03b5=.1\n\u03c0\u03b5 = [\u03b5/4]*env2x3.nA \n# print(sum(\u03c0\u03b5))\n\u03c0\u03b5[mcc.Q[s].argmax()]+=1-\u03b5\n\n# print((\u03c0\u03b5))\nmcc.\u03c0(s)\n# mcc.Q#*[\u03b5/4, \u03b5/4, \u03b5/4, 1-\u03b5+\u03b5/4]\nV = np.zeros(mcc.env.nS)\nfor s in range(mcc.env.nS):\n    # V[s]= (mcc.Q[s]*mcc.\u03c0(s)).sum()\n    # V[s]= (mcc.Q[s]@mcc.\u03c0(s))\n    print(mcc.V_from_Q(s).round(5))\n\n# mcc.V\n</code></pre> <pre><code>0.07493\n0.0\n0.0\n0.08325\n0.0925\n0.0\n</code></pre>"},{"location":"unit2/lesson6/lesson6.html#apply-incremental-mc-on-control-problem","title":"Apply incremental MC on control problem","text":"<pre><code>mcc = MCC(env=grid(reward='reward100'), \u03b1=.2, episodes=1, seed=0, **demoQ()).interact()\n</code></pre> <p>We can also pass the seed to the interact() function</p> <pre><code>mcc = MCC(env=grid(reward='reward1'), \u03b1=.1, episodes=100,  **demoQ()).interact(seed=0)\n</code></pre> <p></p> <pre><code>mcc = MCC(env=grid(), \u03b1=.1, episodes=100,  **demoQ()).interact(seed=0)\n</code></pre> <p></p> <p>As we can see, although we solved the issue of tracking a non-stationary policy when we used a constant learning rate \u03b1, and we tried to use a reward function that gives immediate feedback to each step instead of a delayed reward, but still the performance is not as good as we wished for. This is due to our final issue, which is the action precedence that we set up to prefer left over right. If we change this precedence, it will help the agent to immediately find the goal, however, we set it up this way to make the problem more challenging. Consider changing this precedence to see the effect.</p> <pre><code>mcc.\u03b1\n</code></pre> <pre><code>0.1\n</code></pre> <p>Let us animate and show progress at the same time, as we said earlier this will slow the learning due to animation overhead.</p> <pre><code>mcc = MCC(env=grid(reward='reward1'), \u03b1=.001, episodes=100, animate=True, **demoQ()).interact(seed=0)\n</code></pre> <p></p> <p>We can also just visualise the last 2 episodes.</p> <pre><code>mcc = MCC(env=grid(reward='reward1'), \u03b1=.001, episodes=100, view=2, visual=True).interact(seed=0)\n</code></pre> <p></p> <p></p> <pre><code>mcc.view\n</code></pre> <pre><code>2\n</code></pre>"},{"location":"unit2/lesson6/lesson6.html#reinforce-mc-for-policy-gradient","title":"REINFORCE: MC for Policy Gradient","text":"<p>So far, we have only seen how to estimate a value function to deduce a policy from this value function and then improve the policy by preferring a greedy action with a bit of exploration (as in \u03b5-greedy policy). When we allow the agent to act according to this new policy, its value function might change, so we must re-estimate the value function. We go into iterations of this process until the policy and value function are both stable (converge). We also saw that we could integrate both operations seamlessly into one iteration, as in the value-iteration algorithm in Dynamic Programming. We can even do both stages in one step as in Q-learning or Sarsa, as we shall see in the next lesson. The policy improvement theorem and the Generalised Policy Iteration process guarantee all of this. The primary approach we took to achieve learning for an action-value method is to minimise an error function between our estimate of a value function and the actual value function. Since the real value function is unavailable, we replaced it with some samples (unbiased as in MC and biased as in TD that we will see later).</p> <p>Policy gradient algorithms, on the other hand, attempt to maximise an objective function instead of minimising an error function.  Can you think of a function that, if we maximise, will help us solve the RL problem...? pause for a moment and think.</p> <p>As you might have guessed, the value function can be used as an objective function. The objective here is to change the policy to maximise the value function. </p> <p>Directly estimating the policy means we are not using a value function to express the policy as in the e-greedy. Instead, we are using the value function to learn the policy directly. So, our algorithm does not need to learn the value function explicitly; it can learn a set of parameters that will maximise the value function without knowing what the value function is. It will come as a consequence of learning a policy. In the same way that we did not need to learn a policy in the value-function approach, we learned a value function, and as a consequence of minimising the error, we can deduce a policy from the learned value function. This is the fundamental difference between value function approaches and policy gradient approaches.</p> <p>Estimating the policy directly means we do not need to restrict the policy parameters to value function estimates and their ranges. The policy parameters that represent the preferences to select an action are free to take on any range of values as long as they comparatively form a cohesive policy that maximises the value function by dictating which action to choose in a specific state. This is a major advantage because the value function is strictly tied to the sum of rewards values, while a policy need not have this coupling. This will give us more freedom in using classification architectures when we use function approximation which excels in deducing the best action for a state, instead of using a regression architecture to regress a value function which is usually more prone to initial condition issues and are harder to train.</p> <p>The best policy representation in a policy gradient method is the action selection softmax policy we came across in our last few lessons. This is a smooth function that, unlike \u03b5-greedy, allows the changes in the probabilities to be continuous and integrates very well with policy gradient methods. One of the significant advantages of policy gradient methods (the policy is differentiable everywhere, unlike stepwise \u03b5-greedy functions) is that it provides better guarantees of convergence than \u03b5-greedy due to this smoothness (\u03b5-greedy can change abruptly due to small changes in the action-value functions, while softmax just smoothly increases or decrease the probability of selecting ana action when its action-value function changes).</p> <p>We start our coverage for policy gradient methods with an offline method; REINFORCE. REINFORCE is an algorithm that takes a policy gradient approach instead of an action-value function approach. The idea is simple, given that an episode provides a sample of returns for the visited states, at the end of an episode, we will take the values of the states and use them to guide our search to find the optimal policy that maximises the value function. </p> <p>Note that policy gradient sections in this lesson, and the next are based on chapter 13 of our book. They can be read as they appear in the notebook or delayed until the end of lesson 9.</p>"},{"location":"unit2/lesson6/lesson6.html#policy-gradient-class","title":"Policy Gradient Class","text":"<p>The softmax is the default policy selection procedure for Policy Gradient methods. \\(\\tau\\) acts like an exploration factor (more on that later) and we need to one-hot encoding for the actions.</p> <pre><code>Ia = np.eye(4)\nprint(Ia)\nprint(Ia[1].shape)\nprint(Ia[1])\n</code></pre> <pre><code>[[1. 0. 0. 0.]\n [0. 1. 0. 0.]\n [0. 0. 1. 0.]\n [0. 0. 0. 1.]]\n(4,)\n[0. 1. 0. 0.]\n</code></pre> <pre><code>pi = np.zeros(4)\npi[1]=1\nprint(pi)\n</code></pre> <pre><code>[0. 1. 0. 0.]\n</code></pre> <pre><code>def PG(MDP=MDP(MRP)):\n    class PG(MDP):\n        def __init__(self, \u03c4=1, \u03c4min=.1, d\u03c4=1, T\u03c4=0, **kw):\n            super().__init__(**kw)\n            # set up hyper parameters\n            self.\u03c4 = \u03c4\n            self.\u03c40 = \u03c4\n            self.d\u03c4 = d\u03c4\n            self.T\u03c4 = T\u03c4\n            self.\u03c4min = \u03c4min\n\n            # softmax is the default policy selection procedure for Policy Gradient methods\n            self.policy = self.\u03c4softmax\n\n        #------------------------------------- add some more policies types \ud83e\udde0-------------------------------\n\n        # returns a softmax action\n        def \u03c4softmax(self, s):\n            Qs = self.Q_(s)\n\n            if self.d\u03c4 &lt; 1: self.\u03c4 = max(self.\u03c4min, self.\u03c4*self.d\u03c4)              # exponential decay\n            if self.T\u03c4 &gt; 0: self.\u03c4 = max(self.\u03c4min, self.\u03c40 - self.t_ / self.T\u03c4) # linear      decay\n\n            exp = np.exp(Qs/self.\u03c4)\n            maxAs = np.where(Qs==Qs.max())[0]\n            #a = choice(self.env.nA, 1, p=exp/exp.sum())[0]\n            a = choices(range(self.env.nA), weights=exp/exp.sum(), k=1)[0]\n            self.isamax = a in maxAs\n            return a\n\n        # overriding \u03c0() in parent class MDP: \n        # in MDP \u03c0() returns probabilities according to a \u03b5greedy,\n        # in PG  \u03c0() returns probabilities accroding to a \u03c4softmax, while\n        def \u03c0(self, s, a=None):\n            Qs = self.Q_(s)\n            exp = np.exp(Qs/self.\u03c4)\n            return exp/exp.sum() if a is None else (exp/exp.sum())[a]\n\n    return PG\n</code></pre> <p>Ok, so now we are ready to define our REINFORCE algorithm. This algorithm and other policy gradient algorithm always have two updates, one for V and one for Q. In other words, the action-value function update will be guided by the state-value update. We usually call the first update deals that with V, the critic and the second update that deals with Q the actor.</p> <pre><code>class REINFORCE(PG()):\n\n    def init(self):\n        self.store = True\n\n    # -------------------- \ud83c\udf18 offline, REINFORCE: MC for policy gradient methdos ----------------------\n    def offline(self):\n        \u03c0, \u03b3, \u03b1, \u03c4 = self.\u03c0, self.\u03b3, self.\u03b1, self.\u03c4\n        # obtain the return for the latest episode\n        Gt = 0\n        \u03b3t = \u03b3**self.t                  # efficient way to calculate powers of \u03b3 backwards\n        for t in range(self.t, -1, -1): # reversed to make it easier to calculate Gt\n            s = self.s[t]\n            a = self.a[t]\n            rn = self.r[t+1]\n\n            Gt = \u03b3*Gt + rn\n            \u03b4 = Gt - self.V[s]\n\n            self.V[s]   += \u03b1*\u03b4\n            self.Q[s,a] += \u03b1*\u03b4*(1 - \u03c0(s,a))*\u03b3t/\u03c4\n            \u03b3t /= \u03b3\n</code></pre>"},{"location":"unit2/lesson6/lesson6.html#the-role-of-discount-factor-gamma-in-policy-gradient-methods","title":"The Role of Discount Factor \\(\\gamma\\) in Policy Gradient Methods","text":"<p>\\(\\gamma\\) seems to play a more important role in policy gradient methods than in action-value methods. The next few examples show how \\(\\gamma\\) can make the difference between convergence and divergence. The main issue is, as usual, whether the reward is delayed or there is an intermediate reward. If the reward is delayed, we would need to assign \\(\\gamma\\) values that are &lt; 1 so that the sum of the rewards is discounted, which helps the agent differentiate between longer and shorter paths solution. However, \\(\\gamma\\) also plays a role in convergence when the reward is not delayed. It complements the role that \\(\\tau\\) plays in the SoftMax policy. Therefore, instead of tuning \\(\\tau\\) we can reduce \\(\\gamma\\) specifically when the goal reward is 0, and the intermediate reward is -1 (reward_0) function. Let us see some examples:</p> <p>The below shows that REINFORCE diverges when \u03c4=1, \u03b3=1, for (reward='reward_1').</p> <pre><code>reinforce = REINFORCE(env=grid(reward='reward0'), \u03b1=.1, \u03c4=1, \u03b3=1, episodes=100, seed=10, plotT=True).interact()\n</code></pre> <p></p> <pre><code>reinforce = REINFORCE(env=grid(reward='reward_1'), \u03b1=.1, \u03c4=1, \u03b3=1, episodes=100, seed=10, plotT=True).interact()\n</code></pre> <p></p> <p>Below we increase the value of \\(\\tau\\) to deal with this issue of diveregnce.</p> <pre><code>reinforce = REINFORCE(env=grid(reward='reward0'), \u03b1=.1, \u03c4=2, \u03b3=1, episodes=100, seed=10 , **demoQ()).interact()\n</code></pre> <p></p> <pre><code>reinforce = REINFORCE(env=grid(reward='reward0'), \u03b1=.1, \u03c4=3, \u03b3=1, episodes=100, seed=10 , **demoQ()).interact()\n</code></pre> <p></p> <pre><code>s=31\nprint(reinforce.Q[s])\nprint(reinforce.\u03c0(s))\n</code></pre> <pre><code>[-9.39998085  5.37572334 -6.32250724 -6.20289915]\n[0.00692454 0.95365926 0.01931528 0.02010092]\n</code></pre> <p>As we can see REINFORCE converged when we increase \\(\\tau\\) which helped the values in SoftMax to become appropriatly smaller to help the algorithm to converge.</p> <p>Let us now decrease the value of \\(\\gamma&lt;1\\) and keep \\(\\tau=1\\)</p> <pre><code>reinforce = REINFORCE(env=grid(reward='reward0'), \u03b1=.1, \u03c4=1, \u03b3=.98, episodes=100, seed=10, **demoQ()).interact()\n</code></pre> <p></p> <pre><code>reinforce = REINFORCE(env=grid(reward='reward_1'), \u03b1=.1, \u03c4=1, \u03b3=.98, episodes=100, seed=10, **demoQ()).interact()\n</code></pre> <p></p> <p>As we can see decreasing \\(\\gamma\\) helped REINFORCE immensely to converge. Although the reward that we used is 'reward_1' which is not delayed, but discounting the return helped the value function to be more meaningful for the problem in hand which helped in turn the policy to be more appropriate for the problem in hand.  </p> <p>Let us now increase \\(\\tau\\) and keep \\(\\gamma&lt;1\\) this will reveal another role for \\(\\tau\\).</p> <pre><code>reinforce = REINFORCE(env=grid(reward='reward0'), \u03b1=.1, \u03c4=2, \u03b3=.98, episodes=100, seed=10, **demoQ()).interact()\n</code></pre> <p></p> <pre><code>reinforce = REINFORCE(env=grid(reward='reward_1'), \u03b1=.1, \u03c4=2, \u03b3=.98, episodes=100, seed=10, **demoQ()).interact()\n</code></pre> <p></p> <p>As we can see increasing \\(\\tau\\) while using \\(\\gamma &lt;1\\) did not help. We will mostly therefore use \\(\\gamma &lt;1\\) for our policy gradient methods.  </p>"},{"location":"unit2/lesson6/lesson6.html#delayed-reward-and-reinforce","title":"Delayed Reward and REINFORCE","text":"<p>Let us now look at a delayed reward</p> <pre><code>reinforce = REINFORCE(env=grid(), \u03b1=.1, \u03c4=1, \u03b3=1, episodes=100, seed=10, plotT=True).interact()\n</code></pre> <p></p> <pre><code>reinforce = REINFORCE(env=grid(), \u03b1=.1, \u03c4=5, \u03b3=1, episodes=500, seed=10, plotT=True).interact()\n</code></pre> <p></p> <p>Note that whether we increase or decrease \\(\\tau\\) her, it will not help REINFORCE to converge since the value function that the algorithmm is learning is not appropriate when \\(\\gamma=1\\).</p> <pre><code>reinforce = REINFORCE(env=grid(), \u03b1=.1, \u03c4=1, \u03b3=.98, episodes=500, seed=10, **demoQ()).interact()\n</code></pre> <p></p> <p>As we can see exploration is actually good, so let us decrease this exploration and see if that helps to reach faster convergence given that the environment is rather simple.</p> <pre><code>reinforce = REINFORCE(env=grid(), \u03b1=.1, \u03c4=.2, \u03b3=.98, episodes=500, seed=10, **demoQ()).interact()\n</code></pre> <p></p> <p>Note how the algorithm converged faster but to sub-optimal solution.</p> <pre><code>reinforce = REINFORCE(env=grid(), \u03b1=.1, \u03c4=1, \u03b3=.98, episodes=500, seed=10, **demoQ()).interact()\n</code></pre> <p></p> <p>Note how exploration lead to a fully covered environment but to a slower convergence.</p>"},{"location":"unit2/lesson6/lesson6.html#conclusion","title":"Conclusion","text":"<p>In this lesson, we studied the properties of Monte Carlo algorithms for prediction and control. We started by covering a basic first visit MC method that averages the returns similar to what we did in lesson 1, this time for the associative problem (i.e., when we have states that we select specific actions for, un-associated problems do not have states and have been studied in lesson 1). We have then created an incremental MC algorithm that allows us to average the returns in a step-by-step manner. To that end, we have developed an essential MRP class that will carry the step-by-step and episode-by-episode interaction with an MRP environment, and then we added a useful set of visualisation routines. We have further inherited the MRP class in an MDP class that defines policies that depend on the Q function to obtain a suitable policy for an agent (i.e., control). We noted that MC needed to wait until the episode was finished to carry out updates. In the next unit, we will study full online algorithms that mitigate this shortcoming of MC with the cost of bootstrapping. We will be using the MRP and MDP classes that we developed here.</p>"},{"location":"unit2/lesson6/lesson6.html#units-conclusion","title":"Unit's conclusion","text":"<p>This lesson concludes our unit where we have studied important formulations of RL all of which assumed that we use a table representation for our state space. In the next unit, we will study other offline and fully online RL algorithms that use bootstrapping. Additionally, we will study planning algorithms and then use function approximation instead of a table to represent the state space that can be continuous and infinite.</p>"},{"location":"unit2/lesson6/lesson6.html#your-turn","title":"Your turn","text":"<ol> <li>Change the probabilities of the actions in the stationary policy of an RMP class, use this policy in a random walk process and see the effect on the results.</li> <li>Alter the MDP class to include a softmax policy, use this policy in a maze environment, instead of the e-greedy, and see the effect on the results.</li> <li>There might be some potential for saving compute time if we check if the maxQ is unique, try to alter the greedy policy and observe if this potential can be realised.</li> <li>Create a new class MCsoft algorithm that inherent from PG. This new class would have access to a SoftMax policy which is its default policy. Now apply it on the grid() and see the result.</li> </ol>"},{"location":"unit2/lesson7/lesson7.html","title":"Lesson 7: Introduction to Intelligent Mobile Robots","text":"<p>Lesson learning outcomes:</p> <p>By completing this lesson, you will be better able to:</p> <ul> <li>outline the basics of intelligent mobile robots, its different types and sensors.</li> <li>discuss why AI is required for a mobile robot to perform its task full autonomously. </li> <li>understand the concepts of openloop and closedloop control schemes. </li> </ul>"},{"location":"unit2/lesson7/lesson7.html#introduction","title":"Introduction","text":"<p>The very first question that comes in mind is that how a mobile robot gets its intelligence. Let's start with some definitions:</p>"},{"location":"unit2/lesson7/lesson7.html#artificial-intelligence","title":"Artificial Intelligence:","text":"<p>There are several definitions of artificial intelligence or AI. The most specific one is \"AI is the attempt to get the robots to do things that, for the moment, people are better at.\" </p>"},{"location":"unit2/lesson7/lesson7.html#intelligent-robots","title":"Intelligent Robots:","text":"<p>Thus, we can define that \"A robot with ability to learn from its environment and perform some tasks autonomously with high precision is called an Intelligent robot\".  </p>"},{"location":"unit2/lesson7/lesson7.html#kinds-of-robots","title":"Kinds of Robots:","text":"<p>When we talk about mobile robots, there are several kinds of robots that come under the umbrella.</p> <ul> <li>Ground mobile systems: Various types of mobile platforms can be found here such as mobile vehicles with wheels or caterpillars, legged robots (humanoids or animal mimicking), or robots that mimic some other type of animal locomotion, for example, snakes. Ground mobile systems with wheels or caterpillars that do not carry the operator are often referred to as unmanned ground vehicles.</li> <li>Aerial mobile systems: This group consists of mobile systems that fly in a certain aerial space (airplanes, helicopters, drones, rockets, animal-mimicking flying systems; when used without a pilot they are referred to as unmanned aerial vehicles) or orbit the Earth or some other celestial body (satellites).</li> <li>Water and underwater mobile systems: In this group we find different types of ships, boats, submarines, autonomous underwater vehicles, etc.</li> </ul> <p>We will be talking about the wheeled mobile robot in the course. There are several types of wheeled mobile robots.</p> <ul> <li>Differential Drive Robots: Two independently driven wheels (common in robots and small vehicles).</li> <li>Holonomic Robots: Wheels that can move in any direction (high maneuverability).</li> <li>Omnidirectional Robots: Wheels with rollers for translation and rotation (e.g., omnidirectional wheels).</li> <li>Tracked Robots: Continuous tracks for stability and off-road capabilities (e.g., tank-like robots).</li> </ul> <p>A robot can be represented by its kinematics model and dynamic model.</p> <ul> <li>The kinematic model describes geometric relationships that are present in the system. It describes the relationship between input (control) parameters and the behavior of a system given by state-space representation. A kinematic model describes system velocities and is presented by a set of differential first-order equations.</li> <li>Dynamic models describe a system motion when forces are applied to the system. This model includes the physics of motion where forces, energies, system mass, inertia, and velocity parameters are used.# Introduction to Intelligent Mobile Robots</li> </ul> <p>Lesson learning outcomes:</p> <p>By completing this lesson, you will be better able to:</p> <ul> <li>outline the basics of intelligent mobile robots, its different types and sensors.</li> <li>discuss why AI is required for a mobile robot to perform its task full autonomously. </li> <li>understand the concepts of openloop and closedloop control schemes. </li> </ul>"},{"location":"unit2/lesson7/lesson7.html#introduction_1","title":"Introduction","text":"<p>The very first question that comes in mind is that how a mobile robot gets its intelligence. Let's start with some definitions:</p>"},{"location":"unit2/lesson7/lesson7.html#artificial-intelligence_1","title":"Artificial Intelligence:","text":"<p>There are several definitions of artificial intelligence or AI. The most specific one is \"AI is the attempt to get the robots to do things that, for the moment, people are better at.\" </p>"},{"location":"unit2/lesson7/lesson7.html#intelligent-robots_1","title":"Intelligent Robots:","text":"<p>Thus, we can define that \"A robot with ability to learn from its environment and perform some tasks autonomously with high precision is called an Intelligent robot\".  </p>"},{"location":"unit2/lesson7/lesson7.html#kinds-of-robots_1","title":"Kinds of Robots:","text":"<p>When we talk about mobile robots, there are several kinds of robots that come under the umbrella.</p> <ul> <li>Ground mobile systems: Various types of mobile platforms can be found here such as mobile vehicles with wheels or caterpillars, legged robots (humanoids or animal mimicking), or robots that mimic some other type of animal locomotion, for example, snakes. Ground mobile systems with wheels or caterpillars that do not carry the operator are often referred to as unmanned ground vehicles.</li> <li>Aerial mobile systems: This group consists of mobile systems that fly in a certain aerial space (airplanes, helicopters, drones, rockets, animal-mimicking flying systems; when used without a pilot they are referred to as unmanned aerial vehicles) or orbit the Earth or some other celestial body (satellites).</li> <li>Water and underwater mobile systems: In this group we find different types of ships, boats, submarines, autonomous underwater vehicles, etc.</li> </ul> <p>We will be talking about the wheeled mobile robot in the course. There are several types of wheeled mobile robots.</p> <ul> <li>Differential Drive Robots: Two independently driven wheels (common in robots and small vehicles).</li> <li>Holonomic Robots: Wheels that can move in any direction (high maneuverability).</li> <li>Omnidirectional Robots: Wheels with rollers for translation and rotation (e.g., omnidirectional wheels).</li> <li>Tracked Robots: Continuous tracks for stability and off-road capabilities (e.g., tank-like robots).</li> </ul> <p>A robot can be represented by its kinematics model and dynamic model.</p> <ul> <li>The kinematic model describes geometric relationships that are present in the system. It describes the relationship between input (control) parameters and the behavior of a system given by state-space representation. A kinematic model describes system velocities and is presented by a set of differential first-order equations.</li> <li>Dynamic models describe a system motion when forces are applied to the system. This model includes the physics of motion where forces, energies, system mass, inertia, and velocity parameters are used.</li> </ul>"},{"location":"unit2/lesson7/lesson7.html#kinematics-of-wheeled-mobile-robots","title":"Kinematics of Wheeled Mobile Robots","text":"<p>Several types of kinematic models exist:</p> <ul> <li>Internal kinematics: explains the relation between system internal variables (e.g., wheel rotation and robot motion).</li> <li>External kinematics: describes robot position and orientation according to some reference coordinate frame.</li> <li>Direct kinematics and inverse kinematics: Direct kinematics describes robot states as a function of its inputs (wheel speeds, joint motion, wheel steering, etc.). From inverse kinematics, one can design a motion planning, which means that the robot inputs can be calculated for a desired robot state sequence.</li> <li>Motion constraints appear when a system has less input variables than degrees of freedom (DOFs). Holonomic constraints prohibit certain robot poses while a nonholonomic constraint prohibits certain robot velocities (the robot can drive only in the direction of the wheels\u2019 rotation).</li> </ul> <p>Figure below depicts an abstract control scheme for mobile robot systems that we will use throughout this course. This figure identifies many of the main bodies of knowledge associated with mobile robotics.</p> <p></p>"},{"location":"unit2/lesson7/lesson7.html#kinematics-mobile-robot-vs-robotic-manipulator","title":"Kinematics: Mobile Robot Vs Robotic Manipulator","text":"<ul> <li> <p>Workspace: A manipulator robot\u2019s workspace is crucial because it defines the range of possible positions that can be achieved by its end effector relative to its fixture to the environment. A mobile robot\u2019s workspace is equally important because it defines the range of possible poses that the mobile robot can achieve in its environment.</p> </li> <li> <p>Controllability: The robot arm\u2019s controllability defines the manner in which active engagement of motors can be used to move from pose to pose in the workspace. Similarly, a mobile robot\u2019s controllability defines possible paths and trajectories in its workspace.</p> </li> <li> <p>Dynamic Places: Robot dynamics places additional constraints on workspace and trajectory due to mass and force considerations. The mobile robot is also limited by dynamics; for instance, a high center of gravity limits the practical turning radius of a fast, car like robot because of the danger of rolling.</p> </li> </ul> <p>But the significant difference between a mobile robot and a manipulator arm also introduces a significant challenge for position estimation. A manipulator has one end fixed to the environment. Measuring the position of an arm\u2019s end effector is simply a matter of understanding the kinematics of the robot and measuring the position of all intermediate joints. The manipulator\u2019s position is thus always computable by looking at current sensor data.</p> <p>On the other hand, mobile robot is a self-contained automaton that can wholly move with respect to its environment. There is no direct way to measure a mobile robot\u2019s position instantaneously. Instead, one must integrate the motion of the robot over time. Add to this the inaccuracies of motion estimation due to slippage and it is clear that measuring a mobile robot\u2019s position precisely is an extremely challenging task.</p> <p>For the further reading on the topic, you are encouraged to read topic 3.2, 3.3 and 3.4 from the Siegwart, R., Nourbakhsh, I.R. and Scaramuzza, D., 2011. Introduction to autonomous mobile robots. MIT press.</p>"},{"location":"unit2/lesson7/lesson7.html#what-is-the-real-challenge-then","title":"What is the real Challenge then?","text":"<p>The major question while designing a mobile robot is \"under what conditions can a mobile robot travel from the initial pose to the goal pose in bounded time?\". Answering this question requires knowledge, both knowledge of the robot kinematics and knowledge of the control systems that can be used to actuate the mobile robot. Mobile robot control is therefore a return to the practical question of designing a real-world control algorithm that can drive the robot from pose to pose using the trajectories demanded for the application.</p>"},{"location":"unit2/lesson7/lesson7.html#control-design-for-mobile-robots","title":"Control Design for Mobile Robots","text":"<p>There are two classical ways to design a control system for a mobile robot.</p> <ul> <li>Open-loop Control Design</li> <li>Closed-loop Control Design</li> </ul>"},{"location":"unit2/lesson7/lesson7.html#open-loop-control","title":"Open-loop Control","text":"<p>The objective of a open-loop kinematic controller is to follow a trajectory described by its position or velocity profile as a function of time. This is often done by dividing the trajectory (path) in motion segments of clearly defined shape, for example, straight lines and segments of a circle. The control problem is thus to pre compute a smooth trajectory based on line and circle segments that drives the robot from the initial position to the final position (figure below).</p> <p></p> <p>This approach can be regarded as open-loop motion control, because the measured robot position is not fed back for velocity or position control. It has several disadvantages:</p> <ul> <li>It is not at all an easy task to pre compute a feasible trajectory if all limitations and constraints of the robot\u2019s velocities and accelerations have to be considered.</li> <li>The robot will not automatically adapt or correct the trajectory if dynamic changes of the environment occur.</li> <li>The resulting trajectories are usually not smooth, because the transitions from one trajectory segment to another are, for most of the commonly used segments (e.g., lines and part of circles), not smooth. This means there is a discontinuity in the robot\u2019s acceleration.</li> </ul>"},{"location":"unit2/lesson7/lesson7.html#feedbackclosed-loop-control","title":"Feedback/Closed-loop Control","text":"<p>A more appropriate approach in motion control of a mobile robot is to use a real-state feedback controller. With such a controller the robot\u2019s path-planning task is reduced to setting intermediate positions (subgoals) lying on the requested path.</p> <p>Closed-loop control is a form of motion control in which the path, or trajectory, of the device is corrected at frequent intervals. After motion begins, a position sensor detects possible errors in the trajectory. If an error is detected, the sensor outputs a signal that operates through a feedback circuit to bring the manipulator back on course. The term derives from the fact that the feedback and control-signal circuits together constitute a closed loop. The main asset of closed-loop control is accuracy. In addition, closed-loop control can compensate for rapid, localized, or unexpected changes in the work environment. The principal disadvantages are greater cost and complexity than simpler schemes such as ballistic control. The most common type of classical control used in industry is Proportional Integral Derivative (PID) controller.</p> <p>A general control system follows the methodology given in picture below. </p> <p></p> <p>The actuators are usually the DC motors, step-by-step motors etc. The end effector, as the name say, is the general purpose tool or gripper or hand to mimic the grasping capability for the robotic manipulator. There are different kinds of Sensors Proprioceptive like encoder, gyro, etc. and Exteroceptive sensors like bumpers, rangefinders (infrared, ultrasonic), laser and vision sensors like mono, stereo etc. In order to make useful decision based on these sensor measurements, there are two level of control in each loop: Low-level control and High-level control. </p> <p>The low level control is shown in figure below. In the low level control, following are the fundamental ideas:</p> <p></p> <ul> <li>High-gain PI controllers control the robot\u2019s motors so that the robot moves according to the desired speed profile. </li> <li>The low-level control deals only with the robot actuators control according to the high-level control instructions.</li> <li>If the gains are high enough, the low-level control makes the robot a purely kinematic system.</li> </ul> <p>On the other hand, the high level controller is given in picture below.</p> <p></p> <ul> <li>It processes and computes the signals to send to the low-level controller using data coming from sensors.</li> <li>From its point of view, the robot behaves as a purely kinematic system.</li> <li>For mobile robots, speed control signals are used.</li> </ul>"},{"location":"unit2/lesson7/lesson7.html#example-control-of-wmr","title":"Example: Control of WMR","text":"<p>For the sake of an example, lets think about a wheeled mobile robots. The two controllers can be expressed as:</p> <p>Low-Level Control:  - Internal loop on the motors side for controlling the robot actuation. - It is a simple PI for electric drives (linear systems). - It is not affected by the non-holonomic constraints introduced by the wheels. - Known and solved issues</p> <p>High-Level Control - It defines the motion and the behavior of the robot based on the task to be performed - It must consider the kinematic model - Subject to the constraints of the wheels - It has to control a nonlinear and complex system</p> <p>Now we know some basics about controlling the mobile robot, the real challenge comes how a robot see the world.</p>"},{"location":"unit2/lesson7/lesson7.html#sensing","title":"Sensing","text":"<p>The most important question for a mobile robot to go from some initial position to a desired goal position is to know about the following questions:</p> <ul> <li>Where am I relative to the world and how can the robot model/recognize the environment?</li> <li>What is around me?</li> </ul> <p>In order to get answers for these questions, a robot is equipped with onboard sensors like vision, stereo, range sensors, LIDAR. The sensors are classified in two major categories, i.e. proprioceptive/exteroceptive and passive/active.</p> <ul> <li>Proprioceptive sensors measure values internal to the system (robot); e.g. motor speed, wheel load, robot arm joint angles, battery voltage.</li> <li>Exteroceptive sensors acquire information from the robot\u2019s environment; e.g. distance measurements, light intensity, sound amplitude. Hence exteroceptive sensor measurements are interpreted by the robot in order to extract meaningful environmental features.</li> <li>Passive sensors measure ambient environmental energy entering the sensor. Examples of passive sensors include temperature probes, microphones and CCD or CMOS cameras.</li> <li>Active sensors emit energy into the environment, then measure the environmental reaction. Because active sensors can manage more controlled interactions with the environment, they often achieve superior performance.</li> </ul> <p>The readers are encouraged to read Chapter 4 of Siegwart, R., Nourbakhsh, I.R. and Scaramuzza, D., 2011. Introduction to autonomous mobile robots. MIT press, for a more details perspective of different sensors and perception for mobile robots.</p>"},{"location":"unit2/lesson7/lesson7.html#perception","title":"Perception","text":"<p>Perception for mobile robots refers to the ability of a robotic system to gather information about its environment through various sensors and then process and interpret that information to make informed decisions and navigate autonomously. Perception is a fundamental component of mobile robot autonomy, as it allows the robot to understand and interact with its surroundings.</p> <p>Key aspects of perception for mobile robots include:</p> <ul> <li> <p>Sensing: Mobile robots are equipped with a variety of sensors, such as cameras, LiDAR (Light Detection and Ranging), ultrasonic sensors, radar, and more. These sensors collect data about the robot's surroundings, including information about objects, obstacles, terrain, and other relevant environmental features.</p> </li> <li> <p>Sensor Fusion: Often, mobile robots use multiple sensors of different types to create a more comprehensive understanding of their surroundings. Sensor fusion techniques combine data from various sensors to provide a more accurate and robust perception of the environment.</p> </li> <li> <p>Object Detection and Recognition: Mobile robots need to detect and recognize objects in their environment. This can include identifying obstacles, people, other robots, landmarks, or any other objects relevant to their task.</p> </li> <li> <p>Mapping: Mobile robots create maps of their surroundings, known as occupancy grids or environmental maps. These maps help the robot understand where it is located in relation to the environment and plan paths or make navigation decisions.</p> </li> <li> <p>Localization: Localization is the process of determining the robot's position in a known map or within its environment. Mobile robots often use techniques like SLAM (Simultaneous Localization and Mapping) to estimate their position and orientation.</p> </li> <li> <p>Obstacle Avoidance: Robots must be able to detect and avoid obstacles in real-time to navigate safely. Perception systems provide information about the location and shape of obstacles, allowing the robot to plan a collision-free path.</p> </li> <li> <p>Environment Understanding: Perception systems may also provide information about the type of terrain, the presence of specific landmarks, or environmental conditions like lighting or weather, which can impact the robot's behavior and decision-making.</p> </li> <li> <p>Semantic Understanding: Some advanced mobile robots can understand the semantics of their environment. For example, they can recognize specific objects or understand human commands and gestures.</p> </li> </ul> <p>Perception for mobile robots is essential for various applications, such as autonomous vehicles, delivery robots, search and rescue robots, agricultural robots, and more. Advances in sensors, computer vision, machine learning, and artificial intelligence have significantly improved the perception capabilities of mobile robots, enabling them to operate safely and effectively in a wide range of environments and tasks.</p>"},{"location":"unit2/lesson7/lesson7.html#why-a-robot-needs-intelligence","title":"Why a robot needs Intelligence?","text":"<p>It is usually fine to ask robot to perform a same task repeatedly without any human intervention under ideal conditions. But this is usually not the case. In a real world scenario, the robot has to learn from its environment, surroundings and interactions with things like we human do. The field of AI can help the robot to get this insight. There are seven main areas in AI which helps the robot to perform tasks intelligently. </p>"},{"location":"unit2/lesson7/lesson7.html#knowledge-representation","title":"Knowledge representation.","text":"<p>An important, but often overlooked, issue is how the robot represents its world, its task, and itself. Suppose a robot is scanning a room for an elderly person to assist. What kind of data structures and algorithms would it take to represent what a human looks like or what the human might need? How does a program capture everything important so as not to become computationally intractable? AI robotics explores the tension between the symbolic world representations that are easy for computers to create optimal paths through versus the direct perception used by animals that work directly from perception.</p>"},{"location":"unit2/lesson7/lesson7.html#understanding-natural-language","title":"Understanding natural language.","text":"<p>Natural language is deceptively challenging, apart from the issue of recognizing words which is now being done by commercial products such as Siri and Alexa. It is not just a matter of looking up words, which is the subject of the following apocryphal story about AI. The story goes that after Sputnik went up, the US government needed to catch up with the Soviet scientists. However, translating Russian scientific articles was time consuming and not many US citizens could read technical reports in Russian. Therefore, the US decided to use these newfangled computers to create translation programs. The day came when the new program was ready for its first test. It was given the proverb: the spirit is willing, but the flesh is weak. The reported output: the vodka is strong, but the meat is rotten. AI robotics explores the implicit and explicit communication needed for comfortable social interaction with robot.</p>"},{"location":"unit2/lesson7/lesson7.html#learning","title":"Learning.","text":"<p>Imagine a robot that could be programmed by just watching a human, or that a robot could learn by just repeatedly trying trying a new task by itself. Or that a robot experimented with a task through trial and error to generate a new solution. AI robotics is a study of the different types of learning and how learning can be applied to different functions.</p>"},{"location":"unit2/lesson7/lesson7.html#planning-and-problem-solving","title":"Planning and problem solving.","text":"<p>Intelligence is associated with the ability to plan actions needed to accomplish a goal and solve problems when those plans fail. One of the early childhood fables, the Three Pigs and the Big, Bad Wolf, involves two unintelligent pigs who do not plan ahead and an intelligent pig who is able to solve the problem of why his brothers\u2019 houses have failed, as well as generate a new plan for an unpleasant demise for the wolf. AI robotics relies on planning and problem solving to cope with the unpredictability of the real world.</p>"},{"location":"unit2/lesson7/lesson7.html#inference","title":"Inference.","text":"<p>Inference is generating an answer when there is not complete information. Consider a planetary rover looking at a dark region on the ground. Its range finder is broken and all it has left is its camera and a fine AI system. Assume that depth information cannot be extracted from the camera. Is the dark region a canyon? Is it a shadow? The rover will need to use inference either to actively or passively disambiguate what the dark region is (e.g., kick a rock at the dark area versus reason that there is nothing nearby that could create that shadow). AI robotics techniques are increasingly engaging in inference.</p>"},{"location":"unit2/lesson7/lesson7.html#search","title":"Search.","text":"<p>Search does not necessarily mean searching a large physical space for an object. In AI terms, search means efficiently examining a knowledge representation of a problem (called a \u201csearch space\u201d) to find the answer. Deep Blue, the computer that beat World Chess master Garry Kasparov, won by searching through almost all possible combinations of moves to find the best choice. The legal moves in chess, given the current state of the board, formed the search space. Data mining or Big Data is a form of search. AI robotics uses search algorithms in generating optimal solutions in navigation or searching a knowledge representation.</p>"},{"location":"unit2/lesson7/lesson7.html#vision","title":"Vision.","text":"<p>Vision is possibly the most valuable sense humans have. Studies by Harvard psychologist, Steven Kosslyn, suggest that much of human problem solving capabilities stems from the ability to visually simulate the effects of actions in our head. As such, AI researchers have pursued creating vision systems both to improve robotic actions and to supplement other work in general machine intelligence. AI robotics relies heavily on computer vision to interpret video data and also the RGBD cameras, such as Microsoft\u2019s Kinect.</p>"},{"location":"unit2/lesson7/lesson7.html#your-turn","title":"Your turn:","text":"<p>Run the following worksheet to understand how ros works.   worksheet8.1</p> <p>worksheet8.2</p>"},{"location":"unit2/lesson7/lesson7.html#units-conclusion","title":"Unit's conclusion","text":"<p>This lesson concludes our unit where we have studied important formulations of RL all of which assumed that we use a table representation for our state space. In the next unit, we will study other offline and fully online RL algorithms that use bootstrapping. Additionally, we will study planning algorithms and then use function approximation instead of a table to represent the state space that can be continuous and infinite.</p>"},{"location":"unit3/lesson10/lesson10.html","title":"10. Planning in RL(optional)","text":"<p>The notebook uses a library of functionality in RL that aims for simplicity and general insight into how algorithms work, these libraries are written from scratch using standard Python libraries (numpy, matplotlib etc.). Please note that you will need permission from the author to use the code for research, commercially or otherwise.</p>"},{"location":"unit3/lesson10/lesson10.html#lesson-9-tabular-methods-planning-and-learning","title":"Lesson 9- Tabular Methods: Planning and Learning","text":"<p>Learning outcomes 1. understand how to embed model learning withing an a reinforcement learning algorithms 2. understand mode-based RL approach 3. appreciate the boost in performance obtained due to infusing planning within the RL framework 4. appreciate that planning is usually possible for tabular RL while function approximation is still work in progress (mainly via replaying)</p> <p>In this lesson, we cover a set of methods that use a blend of model-based and model-free to achieve planning and learning simultaneously. Planning involves using some model of the environment to predict the next action or reward of the environment and then use that to aid it in obtaining an estimate of the value function. We have already seen model-based algorithms in the early lesson. Can you remember what they are?... Dynamic programming it is. In this lesson, however, we take these concepts one step ahead, and we will develop a unified view of model-free (MC and TD etc.) and model-based (DP and other) algorithms. </p> <p>Model-based methods can be stochastic or deterministic this depending on the problem, but even if the environment itself is deterministic, we might want to come up with a stochastic model of it that will give us a distribution over all the states that represent the belief that an agent would be in state sn given that it was in state s, these are called distribution models. </p> <p>In fact, we had already seen such an approach when we dealt with the policy_evaluation() function in the DP lesson, where this function depends on the dynamics \\(p(s',r|s,a)\\) to account for all of the different next state and rewards combinations to find an estimate. Distribution models are powerful because they allow us to sample from them at any point in time, but they are computationally expensive.</p> <p>Another possibility is to use sampling. This might appear to contradict what we said earlier about model-free, which uses sampling to estimate the value function. The difference here is that we would use sampling to obtain what might sn be according to the model and then obtain an estimate of the value function. </p> <p>When the environment is deterministic, sampling can be used relatively efficiently to build the dynamics. In fact, we had already seen such an approach when we dealt with the dynamics() function in the DP lesson, where this function tries to estimate the dynamics of the environment by observing the next state and reward from a current state and action combination.</p> <p>In all cases, the main advantage of planning is that we can plan ahead many steps without actually taking action, unlike non-planning algorithms such as TD. All we would be doing is to assume that the agent took action and then build on that. We can produce a whole imaginary episode using sampling, which is what we call a simulated experience which means it is an experience that the agent might take but is not real. With model-free sampling methods, we use an actual experience that the agent went through to estimate the value function. With distribution models, in theory, we can produce all possible episodes that the agent can take with their probabilities. Of course, that would mean accounting for many possibilities in both sampling and distribution models, and this is the main source of complexity in planning algorithms because they usually have a complexity of \\(O(n^2)\\) where n is the number of states.</p> <p>However, this difference between using a simulated or real experience is irrelevant from the update rules perspective. So, we can apply many of the covered methods in a simulated experience and a real experience. This is quite convenient since it will require less treatment and help us develop our unified view of planning and learning. Note that in planning, we will not deal with prediction. We will cover control only because executing a plan naturally involves an advanced control scenario.</p> <p>Finally, we should point out that due to the above complexity and assumptions, planning is usually confined to the tabular methods, and we will not see it when we move to function approximation which assumes that the state space can be infinite.</p> <p>Reading: The accompanying reading of this lesson is chapter 8 of our text book available online here. Please note that we explain the ideas of this topic from a practical perspective and not from a theoretical perspective which is already covered in the textbook.</p> <pre><code>%matplotlib inline\n</code></pre> <pre><code>from rl.rl import *  # an RL base alg lib, value-based and policy-grad RL tabular algorithms\nimport bisect        # this will help to insert in a sorted Queue list\n</code></pre> <p>Let us start by covering a basic algorithm, that shows the basic idea of planning using sampling.  Below we show the Random sample one-step Q-planning method.</p>"},{"location":"unit3/lesson10/lesson10.html#dyna-q-integrating-model-learning-and-q-learning","title":"Dyna-Q: Integrating Model learning and Q Learning","text":"<p>In the DP lesson, we have built a dynamics() function used in the policy_iteration() and other functions. The dynamics() function is an example of model learning. We have used this function to provide a model for the DP algorithms, assuming it is already available. Hence, we started each of them with the model-learning step. This section will integrate the model learning with the direct RL learning we have seen in other lessons (like TD and MC). We use the real experience the agent is going through to improve both our model l of the environment (model learning) and our value-function estimation (direct RL learning).</p> <pre><code>class DynaQ(MDP()):\n    def __init__(self, \u03b1=.1, m=10, **kw):\n        super().__init__(**kw)\n        self.\u03b1 = \u03b1\n        self.m = m          # how many steps to plan ahead\n        self.store = False  # no need to store experience, the Model will give us what we need\n        self.Model = np.zeros((self.env.nS, self.env.nA, 3)) # self.env.nR, self.env.nS))\n        self.stop_early = self.\u03c0isoptimal # stop experiment when policy is optimal, need to set Tstar to take effect\n\n    def init(self):\n        self.nUpdates = 0 # number of updates to optimal solution\n        super().init()\n\n\n    #--------------------------------------\ud83c\udf16 online learning --------------------------------------\n    def online(self, s, rn,sn, done, a,_):\n        self.Q[s,a] += self.\u03b1*(rn + (1- done)*self.\u03b3*self.Q[sn].max() - self.Q[s,a])\n        self.Model[s,a] = [rn, sn, done]\n        self.nUpdates +=1\n\n        sa = np.argwhere(self.Model[:,:,1]!=np.array(None))\n        for _ in range(self.m):    \n            # get s,a that has been visited before and randomly select a pair of them\n\n            ind = randint(sa.shape[0])\n            s, a = sa[ind]\n\n            rn,sn,done = self.Model[s,a]; sn=int(sn)        \n            self.Q[s,a] += self.\u03b1*(rn + (1- done)*self.\u03b3*self.Q[sn].max() - self.Q[s,a])\n            self.nUpdates +=1\n\n#             s = sn\n#             rn,sn,done = self.Model[sn,self.Q[sn].max()]\n</code></pre> <pre><code>dynaQ = DynaQ(env=maze(), episodes=2, seed=10, m=50, **demoQ()).interact()\n</code></pre> <p></p> <p>Let us see how many updates DynaQ had to do to achieve what it has achieved in the first 2 episodes!</p> <pre><code>dynaQ.nUpdates\n</code></pre> <pre><code>10659\n</code></pre> <p>For comparison let us do the same for Q learning, ( DynaQ is equivalent to Q learning fro m=0). This agent will take a bit of time sine we are training for 2 episodes.</p> <pre><code>qlearn = DynaQ(env=maze(), episodes=2, seed=1, m=0, **demoQ()).interact()\n</code></pre> <p></p> <pre><code>qlearn.nUpdates\n</code></pre> <pre><code>1011\n</code></pre> <p>As we can see the difference is huge between both, this difference shown on the performance. Let us study how the performance varies with n (the number of planning steps)</p>"},{"location":"unit3/lesson10/lesson10.html#the-effect-of-planning-steps-on-dynaq","title":"The effect of planning steps on DynaQ","text":"<pre><code>def mazePlanning(runs=30, algo=DynaQ, label='DynaQ', yticks=True):\n    if yticks: plt.yticks([14, 200, 400, 600, 800])\n    for m in [0, 5, 50]:\n        DynaQ0 = Runs(algorithm=algo(env=maze(), \u03b1=.1, \u03b3=.95, m=m, episodes=50), \n                      runs=runs, plotT=True).interact(label=label+' m = %d'%m)\nfig_8_2 = mazePlanning\n</code></pre> <pre><code>fig_8_2()\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|30/30\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|30/30\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|30/30\n</code></pre>"},{"location":"unit3/lesson10/lesson10.html#prioritized-sweeping","title":"Prioritized Sweeping","text":"<p>In DynaQ, the algorithm uniformly chooses n number of previously visited states and re-do their updates. This is ok when the environment is small. However, when the environment becomes big, the performance will dip, and the algorithm will need to do more updates per step to reach an optimal solution. This is where prioritising more important updates over less important ones optimises compute resource usage and achieves more in fewer steps. This is the idea of prioritized sweeping algorithms; it will sweep through different previous states' updates but prioritise those needing the most changes. It measures this priority by the absolute value of the TD error (yes, TD again!). The higher this error value is, the higher its corresponding priority is. It uses a queue to add states that need to update the most (as per their TD error) on the top of the queue. Then it will go through the queue one by one until its number of planning steps is done or until the queue is empty. The algorithm is shown below.</p> <pre><code>class Psweeping(MDP(MRP)):\n    def __init__(self, \u03b1=.1, m=5, \u03b8=1e-4, **kw):\n        super().__init__(**kw)\n        self.\u03b1 = \u03b1\n        self.m = m\n        self.\u03b8 = \u03b8\n\n        self.store = False # no need to store experience, the Model will give us what we need\n        self.Model = np.zeros((self.env.nS, self.env.nA, 3))\n        self.stop_early = self.\u03c0isoptimal # stop experiment when policy is optimal, needs to set Tstar to take effect\n\n\n    def init(self):\n        self.PQueue = [] # collections.deque()# native list is more efficent than a linked list\n        self.nUpdates = 0 # number of updates to optimal solution\n        super().init()\n\n    def insert(self, P,s,a): # P is the priority\n        found_lowerP = False # found an entry with a lower priority\n        for i, (P_old, s_old, _) in enumerate(self.PQueue[-2*self.m:]):\n            if s==s_old:\n                if P&gt;=P_old: self.PQueue.pop(i)\n                else:  found_lowerP = True\n                break\n        # insert the tuple in the right position according to P\n        if not found_lowerP: bisect.insort_left(self.PQueue, (P,s,a))\n\n    #--------------------------------------\ud83c\udf16 online learning --------------------------------------\n    def online(self, s, rn,sn, done, a,_):\n\n        P = abs(rn + (1- done)*self.\u03b3*self.Q[sn].max() - self.Q[s,a])\n        if P&gt;=self.\u03b8:  self.insert(P,s,a)\n        self.Model[s,a] = [rn, sn, done]\n\n        i=0\n        while self.PQueue and i&lt;=self.m:\n            _,s,a = self.PQueue.pop()\n            rn, sn, done = self.Model[s,a]; sn=int(sn)\n            self.Q[s,a] += self.\u03b1*(rn + (1- done)*self.\u03b3*self.Q[sn].max() - self.Q[s,a])\n            self.nUpdates +=1\n            i+=1\n\n            # to guarantee equivelncy with Qlearning when \u03b8=0, m=0\n            if self.m==0: break \n            # go backward to previous states and actions sp, ap\n            for sp,ap in np.argwhere(self.Model[:,:,1]==s):\n                r,_,done= self.Model[sp,ap] #_==s\n                P = abs(r + (1- done)*self.\u03b3*self.Q[s].max() - self.Q[sp,ap])\n                if P&gt;=self.\u03b8: self.insert(P,sp,ap)\n</code></pre> <p>Note that for the insert() function, we check first if state s is already in the queue, and if so, remove its entry if it has less priority than the new entry. Otherwise, do not add anything. </p> <p>PQueue is an ascending sorted queue according to P, not to s, and hence we cannot do a binary search for s, so we had to reside to linear search we tried to cut the search time by limiting the queue scope to 2n from the end of the queue.</p> <p>Let us ensure that Qlearning and prioritized sweeping are almost identical for \u03b8=0 and m=0.</p> <pre><code>psweep0 = Runs(algorithm=Psweeping(env=maze(), \u03b1=.1, \u03b3=.95, m=0, \u03b8=0, episodes=50),runs=10, plotT=True).interact(label='psweep n = %d'%0)\nqlearn = Runs(algorithm=Qlearn(env=maze(), \u03b1=.1, \u03b3=.95, episodes=50),runs=10, plotT=True).interact(label='Qlearn')\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|10/10\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|10/10\n</code></pre> <p></p> <pre><code>psweeping = Psweeping(env=maze(), episodes=2, seed=1, m=0, **demoQ()).interact()\n</code></pre> <p></p> <pre><code>psweeping.nUpdates\n</code></pre> <pre><code>3\n</code></pre> <p>Note how prioritised sweeping done only 3 updates and compare this to DynaQ.  Ok so now let us test with m=50</p> <pre><code>psweeping = Psweeping(env=maze(), episodes=2, seed=1, m=50, **demoQ()).interact()\n</code></pre> <p></p> <pre><code>psweeping.nUpdates\n</code></pre> <pre><code>2754\n</code></pre> <p>Compare the above 5k number of updates for prioritised sweeping with the 32k number of update for DynaQ for the same number of planning states m=50. The difference is huge.</p>"},{"location":"unit3/lesson10/lesson10.html#the-effect-of-planning-steps-on-prioritised-sweeping","title":"The effect of planning steps on Prioritised Sweeping","text":"<p>Ok let us conduct the same experiment as before and test to see how the performance changes with n.</p> <pre><code>mazePlanning(runs=10, algo=Psweeping, label='Prioritised Sweeping', yticks=False)\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|10/10\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|10/10\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|10/10\n</code></pre> <p></p>"},{"location":"unit3/lesson10/lesson10.html#prioritized-sweeping-on-different-maze-sizes","title":"Prioritized sweeping on different maze sizes","text":"<pre><code>def maze(rows=6, cols=9, **kw):\n    return Grid(gridsize=[rows,cols], s0=int((rows)/2)*cols, goals=[rows*cols-1], style='maze', **kw)\n</code></pre> <pre><code>class mazes:\n    def __init__(self, m=6, **kw):\n        gridsizes = [(6, 9), (9 , 12), (12, 18), (16, 26), (24, 34), (32, 50), (39, 81), (60, 104)][:m]\n        self.env = []\n        for rows,cols in gridsizes:\n            self.env.append(maze(rows,cols, **kw))\n\n    def __getitem__(self, i): return self.env[i]\n\n    def sizes(self):\n        sizes = [0]\n        for mz in  self.env: sizes.append(mz.nS_available())\n        return sizes\n\nprint(mazes().sizes())\n</code></pre> <pre><code>[0, 47, 92, 188, 372, 748, 1500]\n</code></pre> <pre><code>mazes()[0].render()\n</code></pre> <pre><code>psweeping = Psweeping(env=mazes(figsize=[19,3.5])[3], \u03b1=.5, episodes=50, seed=1, m=5, **demoQ()).interact()\n</code></pre> <p>Let us examine the number of steps required by prioritised sweeping and compare it to the number of updates</p> <pre><code>psweeping.t\n</code></pre> <pre><code>48\n</code></pre> <pre><code>psweeping.nUpdates\n</code></pre> <pre><code>36288\n</code></pre> <pre><code>%time dynaQ = DynaQ(env=mazes(figsize=[19,3.5])[3], \u03b1=.5, episodes=50, seed=1, \\\n                    m=5, animate=True, **demoQ()).interact()\n</code></pre> <pre><code>CPU times: user 43.1 s, sys: 7.77 s, total: 50.9 s\nWall time: 25.7 s\n</code></pre> <p></p> <pre><code>dynaQ.t\n</code></pre> <pre><code>63\n</code></pre> <pre><code>dynaQ.nUpdates\n</code></pre> <pre><code>240648\n</code></pre> <p>As we can see there is a big difference between the number of updates for each algorithms.</p> <p>After experimenting with the different mazes environment we can roughly come up with a reasonably flexible optimal number of steps for each environment which we stored in Tstar and is used in the below function.</p>"},{"location":"unit3/lesson10/lesson10.html#runs-and-comparison-for-planning-algorithms","title":"Runs and Comparison for Planning Algorithms","text":"<p>We need to write some new functions to perform several runs and comparisons while allowing the algorithm to stop before finishing all the episodes. The reason we cannot use our original Runs class is due to allowing for stopping before finishing all the episodes. We show this simple function below.</p> <pre><code>def PlanningRuns(algorithm, runs=1, envs=mazes(), Tstar = [17,23,38,55,75,110,300]):\n    nUpdates = np.ones((runs,len(envs.sizes())))*10\n    for run in range(runs):\n        for i, env in enumerate(envs):\n            nUpdates[run,i+1] = algorithm(env=env, \u03b1=.5, episodes=60, m=5, Tstar=Tstar[i], seed=(i+1)*(run+10)).interact().nUpdates\n    return nUpdates\n</code></pre> <pre><code>class PlanningCompare:\n\n    def __init__(self, algos=[DynaQ, Psweeping], runs=1, m=5):\n        self.envs = mazes(m=m)\n        self.algos = algos\n\n        self.nUpdates = []\n        for i, algo in enumerate(algos):\n            print('\\nMaze Planning with %s...........................'%algo.__name__)\n            self.nUpdates.append(PlanningRuns(algorithm=algo, runs=runs, envs=self.envs ))\n\n    def Plot(self):\n        plt.yscale('log')\n        sizes = self.envs.sizes()\n        plt.xticks(sizes)\n        for i, nUpdate in enumerate(self.nUpdates):\n            plt.plot(sizes, self.nUpdates[i].mean(0), label=self.algos[i].__name__)\n\n        plt.xlabel('Gridworld size (#states)')\n        plt.ylabel('Updates until optimal solution')\n        plt.legend()\n        plt.show()\n</code></pre> <pre><code>def example_8_4():\n    PlanningCompare().Plot()\n</code></pre> <pre><code>%time example_8_4()\n</code></pre> <pre><code>Maze Planning with DynaQ...........................\nexperience stopped at episode 5\nexperience stopped at episode 5\nexperience stopped at episode 11\n\nMaze Planning with Psweeping...........................\nexperience stopped at episode 1\nexperience stopped at episode 4\nexperience stopped at episode 15\nexperience stopped at episode 32\nexperience stopped at episode 20\n</code></pre> <p></p> <pre><code>CPU times: user 28.8 s, sys: 646 ms, total: 29.5 s\nWall time: 29.3 s\n</code></pre>"},{"location":"unit3/lesson10/lesson10.html#conclusion","title":"Conclusion","text":"<p>This lesson covered two major planning algorithms, namely the Dyna-Q and the Prioritised Sweeping. These have variants, but what we covered are quite dominant algorithms in the RL planning landscape. We saw that Dyna-Q is quite good at finding a complete solution with the cost of a higher number of the uniformly selected past update. Prioritised Sweeping, on the other hand, is selective in its update and prioritises those expected to need the most attention due to the latest update, and they propagate backwards towards previously visited states. Prioritised sweeping is faster and more promising. However, we need to tune an extra hyperparameter of the threshold \u03b8. In practice, a small value for \u03b8 seems to work fine, but we have to pay attention to the learning rate \u03b1 as it also dictates the rate at which the update propagates backwards. If \u03b1 is set to a small value, prioritised sweeping can suffer from a significant slowness in its performance.</p>"},{"location":"unit3/lesson10/lesson10.html#your-turn","title":"Your turn","text":"<ol> <li>consider what you would need to change in the prioritised sweeping in order to deal with a stochastic environment</li> </ol>"},{"location":"unit3/lesson11/lesson11.html","title":"Lesson 10: Mobile Robot Localization and SLAM","text":"<p>Lesson learning outcomes:</p> <p>By completing this lesson, you will be better able to:</p> <ul> <li>outline the issues and problems associated with a mobile robot localising its position within an environment</li> <li>discuss the sources of uncertainty in robot localization, particularly motion noise, sensor noise, sensor aliasing, and initial uncertainty</li> <li>apply your knowledge of robot localization to simulate a mobile robot navigating an environment</li> </ul> <p>In this lesson, you learn about the localization and some of the issues and problems associated with a mobile robot localising its position within an environment. We also discuss how a localization and mapping work in parallel.</p>"},{"location":"unit3/lesson11/lesson11.html#localization","title":"Localization","text":"<p>A mobile robot navigating in an environment needs to know its position within that environment. We call this robot localization and often refer to the robot's environment as the \"map\" when discussing robot localization. The localization task is not always straightforward for a robot because there are many sources of noise that confuse the robot about its location.</p> <p></p> <p>Localization tries to answer the question of how a mobile robot can know where it is in any given environment. Consider the mobile waiter robot in Figure 3.1 that needs to navigate around a restaurant environment. The waiter robot has the task of taking orders from customers and delivering their food from the kitchen when it is ready. To complete this task it needs to move around the tables, or obstacles, in this environment. For the robot to successfully move to a particular table or to the kitchen, it needs to know its current location (the start of moving) and its desired location (goal of moving).</p> <p>As the robot waiter navigates around the restaurant, it needs to know where it is, or localize itself. We can make solving the question of localization easier by assuming that when the restaurant owner first turns on the robot, they tell it where it is. This means that the waiter robot knows perfectly where it is at the beginning, because the restaurant owner set its start position. In the real world, there are factors that make it difficult to be certain of a robot's start position, but to introduce localization, we will assume that the restaurant owner is able to do that accurately in our restaurant example.</p>"},{"location":"unit3/lesson11/lesson11.html#dead-reckoning-or-odometry","title":"Dead reckoning or odometry","text":"<p>Next, the restaurant opens, and the owner sets the robot to work waiting on tables. The waiter robot starts to move to complete tasks. When it does this, it tries to keep track of its pose using dead reckoning.</p> <ul> <li>Pose is the location and rotation for our waiter robot.</li> <li>Dead reckoning (also called odometry) is the process the robot uses to calculate its current position, using a previously determined position and estimated speeds over an elapsed time.</li> </ul> <p>The waiter robot can use knowledge it has of its wheels in order to complete dead reckoning. It knows the orientation and size of its wheels, plus the speed at which they are rotating, and it is able to compute its own velocity at any point in time. It can integrate these velocities to calculate its current position as it continues to move in an environment.</p> <p>Example: \"Waiter robot calculates its new position\"</p> <p>Using its knowledge of its wheels (size, orientation, velocity of rotation):</p> <ul> <li>the waiter robot waiting at starting position, X, can calculate that if it moves in a forward direction for one metre at a known velocity, it can update its position in its internal environment map by one metre forward, X + 1.</li> </ul>"},{"location":"unit3/lesson11/lesson11.html#effector-noise-or-motion-uncertainty","title":"Effector noise or motion uncertainty","text":"<p>The problem with dead reckoning is that even if the restaurant owner commands the robot to move with a certain velocity, its actual motion will be different. This can be due to:</p> <ul> <li>imperfections in robot structure, for example, one wheel is not fully inflated and does not move as far as the robot expects;</li> <li>the environment, for example, there are bumps on the ground, or a surface which cause the wheels to slip or skid.</li> </ul> <p>These factors mean that when the robot completes the calculation to move forward one metre, it is likely to move less than one full metre forward. The robot will not be in the pose that its calculation suggests it will be located.</p> <p>Therefore, there will always be imperfections in the way a robot moves, even if these are very minor, because of multiple imperfections in the environment. These imperfections add up. If the robot only considers the rotation of its wheels, there will be a significant difference between the data the robot is using to calculate its location and what the restaurant owner observes about the position of their waiter robot. In the real world, there are always factors that mean a robot never moves exactly as its owner intended it to move.</p> <p>These factors are called effector noise or sometimes motion uncertainty, or actuator noise. The term effector noise relates to special sensors found on a robot that we call effectors. Effectors are similar to other sensors in that they receive information about the environment, but robots also use them to move in an environment.</p> <p>Errors in a robot's knowledge of its position tend to accumulate over time. In this short video, we explains why this happens.</p> <p>Link to the Video</p> <p>The video is approximately 3-minutes long.</p> <p>Download transcript (PDF).</p>"},{"location":"unit3/lesson11/lesson11.html#sensors-used-to-identify-environmental-features","title":"Sensors used to identify environmental features","text":"<p>So far, we have established the idea that even if the robot knows its starting pose perfectly, as it moves in the real world, its knowledge of its location becomes more and more uncertain. We can solve this uncertainty by adding sensors to a robot that enable it to gather data on environmental features to figure out where it exactly is.</p> <p>In this video, we explains the principles of using sensors so that a robot can estimate its own position as it moves within an environment. He uses the example of a single beam range sensor to explain why it is common to have uncertainty over the exact position of a robot, or localization uncertainty, and introduces the term sensor noise.</p> <p>Link to the Video</p> <p>The video is approximately 8-minutes long. Download transcript (PDF).</p> <p>\"Summary of localization uncertainty from the video\"</p> <p></p> <p>In the video, you saw that both effector noise and sensor noise result in localization uncertainty for a robot. We noted that:</p> <ul> <li> <p>we can use sensors to aid a robot to localize its position in an environment. For example, in Figure 3.2 above, we can predict that the robot will be somewhere in the grey region of the environment</p> </li> <li> <p>a single reading from a sensor does not usually provide an accurate enough localization for a robot. A robot needs to take many readings to be more certain of its location, and as it moves, it will need to continually take sensor readings to update its location</p> </li> <li> <p>in the real world, it will never be possible to narrow down a robot\u2019s position to a single point due to sensor noise.</p> </li> </ul> <p>Having considered sensor noise, you saw that there is another source of uncertainty, for example when a robot moves parallel to the surface of an object. We call this uncertainty sensor aliasing, when different robot poses result in the same sensor values.</p> <p>We observed a robot in the environment shown in Figure 3.3 below, and saw that when it reads that it is 100cm from the wall, it is actually somewhere in the grey region due to the combined uncertainty due to sensor noise and sensor aliasing.</p> <p></p>"},{"location":"unit3/lesson11/lesson11.html#localization-algorithms","title":"Localization algorithms","text":"<p>Localization algorithms enable a robot to estimate its pose, or localize, in a given map as accurately as possible in the face of all these uncertainties.</p> <p>Over the next three Lessons we will study how to use sensors and apply this to a simulated TurtleBot in the Activities that accompany some of the Lessons. We will integrate the information coming from the sensors with the information coming from the wheels to localize a robot in a given environment.  We are focusing on this because it is the most general solution to this problem of localization for your robot. It will use sensors and actuators and as it moves, you will integrate all this information.</p> <p>Here are other ways to make it easy for a robot to localize if you are able to structure the environment artificially.</p>"},{"location":"unit3/lesson11/lesson11.html#example-1-robot-system-in-a-factory","title":"Example 1: Robot system in a factory","text":"<p>In this example, where only robots inhabit a factory space, you can place markers in the environment. In Figure 3.4 is a typical distribution warehouse. You can see small white squares (similar to the Unique marker shown in Figure 3.5) at equal distances between each other on the floor surface of the factory. The robots have cameras that look down onto the floor. When a camera sees the marker, it can wirelessly ask the computer system where that marker is in the warehouse. Each marker has a unique position in the warehouse, so this system allows the robot to know exactly where it is within this engineered environment.</p> <p></p> <p>Figure 3.5 Unique marker with known absolute 2D position in the map. These markers are sometimes called \u00abAR marker\u00bb due to their popular use in Augmented Reality.</p>"},{"location":"unit3/lesson11/lesson11.html#beacon-based-localization-with-augmented-reality-markers","title":"Beacon-based localization with augmented reality markers","text":"<p>Flying robots can also use similar markers on the ground to the unique marker shown in Figure 3.5. These markers are sometimes called \u00abAR marker\u00bb due to their popular use in Augmented Reality. Each marker is different has the location information encoded into it. For flying robots using this method of beacon-based localization, the robot has a camera that it uses to localize itself using these markers in the environment.</p>"},{"location":"unit3/lesson11/lesson11.html#motion-capture-systems","title":"Motion capture systems","text":"<p>You can also arrange it the other way with a marker on the robot and cameras in the environment in a motion capture system. With cameras around the edge of an environment (circle shape in Figure 3.6 and pyramid shapes in Figure 3.7), you can place markers on the robots moving in this environment. This method facilitates having many robots in an environment such as Figure 3.6, because cameras in the environment can detect each robot's unique marker. You can estimate the location of each robot via its unique marker because you can accurately set the position of each camera in the environment.</p> <p>See how in Figure 3.7 we can also use this technology in augmented reality. With sensors placed on a person moving in the environment, you can build structures of a person\u2019s motion, giving this method the name motion capture systems.</p> <p>Cameras for a motion capture system usually have the following properties:</p> <ul> <li>High resolution (from VGA up to 16 Mpixels)</li> <li>Very high frame rate (several hundreds of Hz)</li> <li>Good for ground truth reference and multi-robot control strategies</li> <li>Popular brands:</li> <li>VICON (10kCHF per camera),</li> <li>OptiTrack (2kCHF per camera)</li> </ul> <p>Example \"Making a TurtleBot motion capture system\"</p> <p>Think about the Turtlebot you will use in this module. You can set up a motion capture system with cameras in the corners of the room and a visible marker on the Turtlebot. You will have an accurate position for your TurtleBot in the environment at any point in time. This requires you to engineer the environment, and consider the location of the camera so that the robot is visible at all times. If the robot is not visible to the camera, then you will not be able to locate the robot.</p>"},{"location":"unit3/lesson11/lesson11.html#different-approaches-to-localization","title":"Different Approaches to Localization","text":"<p>The first approach to localization is probabilistic approach. </p>"},{"location":"unit3/lesson11/lesson11.html#probabilistic-approach-to-localization","title":"Probabilistic approach to localization","text":"<p>The best way to address the localization problem is by viewing these issues mathematically as a probabilistic inference problem. In other words, we represent the robot\u2019s pose in the environment as a probability distribution, instead of a single value. We then update this probability distribution according to how the robot moves and what the robot sensors perceive.</p> <p></p> <p>When you place a robot in an environment and switch it on, it will not know where it is. It needs some method or algorithms determine its location using the methods we discussed in the last lesson, i.e.:</p> <ul> <li>odometry or dead reckoning</li> <li>localization based on external sensors, beacons or landmarks</li> </ul> <p>In this video, we explains a third option combining both these methods in a new method of Probabilistic Map Based Localization to determine our robot\u2019s starting location.</p> <p>Link to the video</p> <p>The video is approximately 15-minutes long.</p> <p>Download transcript (PDF).</p>"},{"location":"unit3/lesson11/lesson11.html#representing-uncertainty-using-probability-theory","title":"Representing uncertainty using probability theory","text":"<p>We use a probabilistic approach for mobile robot localization because measurement errors affect the data coming from a robot\u2019s sensors, meaning we can only compute the probability that the robot is in a given configuration.</p> <p>The key idea in probabilistic robotics is to represent uncertainty using probability theory: instead of giving a single best estimate of the current robot configuration, probabilistic robotics represents the robot configuration as a probability distribution over all possible robot poses.</p> <p>This probability distribution is called belief. It is represented by \\(bel\\).</p> <p>The probability distribution graph you saw in the video in Lesson step 5.1, and in Figure 5.2 below are a representation of this belief probability.</p> <p></p>"},{"location":"unit3/lesson11/lesson11.html#example-probability-distributions","title":"Example probability distributions","text":"<p>Here we introduce you to four examples of probability distributions:</p> <ul> <li>Uniform</li> <li>Multimodal</li> <li>Dirac</li> <li>Gaussian</li> </ul> <p>In the following examples, we consider a robot that is constrained to move along a straight rail i.e. the problem is one-dimensional.</p> <p>The robot configuration is the position \\(x\\) along the rail shown in Figure 5.3.</p> <p></p>"},{"location":"unit3/lesson11/lesson11.html#example-1-uniform-distribution","title":"Example 1: uniform distribution","text":"<p>We use this when the robot does not know where it is. It means that the information about the robot configuration is null. With uniform distribution, given a certain domain, the probability of every point is the same. Figure 5.4 below represents our one-dimensional environment of size \\(n\\).</p> <p>In this environment, with a uniform distribution, the height of the probability graph is</p> \\[ \\frac{1}{N} \\] <p>Because the sum of the area of the graph must add up to one.</p> <p></p> <p>Remember that in order to be a probability distribution a function \\(p(x)\\) must always satisfy the following constraint, i.e. equal one:</p> \\[ \\int_{-\\infty}^{+\\infty} p(x) d x=1 \\]"},{"location":"unit3/lesson11/lesson11.html#example-2-multimodal-distribution","title":"Example 2: Multimodal distribution","text":"<p>With multimodal distribution, there are certain points where we think the robot will be. In Figure 5.5 below, the robot is in the region around location \\(a\\) or \\(b\\).</p> <p></p>"},{"location":"unit3/lesson11/lesson11.html#example-3-dirac-distribution","title":"Example 3: Dirac distribution","text":"<p>Our next example is Dirac distribution. If our robot ever knew its location perfectly then we could put a probability of 1 to that position \\(a\\), where the robot currently is. i.e. the robot has a probability of 1.0 (i.e. 100%) to be at position \\(a\\).</p> <p>As shown in Figure 3.13, this would correspond to Dirac distribution. Note, this is represented as an upwards arrow in Figure 5.6 because it just goes infinitely high. The width of the arrow is an infinitely small point, with an infinitely high distribution at that point.</p> <p>The Dirac function \\(\\delta(x)\\) is defined as:</p> \\[ \\delta(x)=\\left\\{\\begin{array}{cc} \\infty &amp; \\text { if } x=0 \\\\ 0 &amp; \\text { otherwise } \\end{array} \\quad \\text { with } \\int_{-\\infty}^{+\\infty} \\delta(x) d x=1\\right. \\] <p>The Dirac function as infinity for \\(x\\) is equal to zero (the argument to the Dirac function is zero). It is zero at all other points on the distribution graph where the robot could be located. The Dirac function needs to integrate to one because it is still a probability distribution.</p> <p>We almost never have this with robot localization because we almost never perfectly know the robot's location.</p>"},{"location":"unit3/lesson11/lesson11.html#example-4-gaussian-distribution","title":"Example 4: Gaussian distribution","text":"<p>We frequently estimate robot poses in robotics using Gaussian distribution.  The Gaussian distribution has the shape of a bell and we define it using the following formula:</p> \\[ p(x)=\\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-\\frac{(x-\\mu)^{2}}{2 \\sigma^{2}}} \\] <p>We usually have a mean \\(\u03bc\\). This is shown in Figure 5.7 as the point along the \\(x\\) axis with the peak of the bell curve. We have the standard deviation represented by sigma \\(\u03c3\\). The variance, square root of \\(\u03c3\\), is how wide the distribution is, shown by the horizontal arrow in Figure 5.7.</p> <p>The Gaussian distribution is also called normal distribution and is usually abbreviated with \\(N(\u03bc ,\u03c3)\\).</p> <p></p> <p>The Gaussian distribution is also called normal distribution and this explains why we use a capital \\(N\\) and abbreviate it to \\(N(\u03bc ,\u03c3)\\).</p>"},{"location":"unit3/lesson11/lesson11.html#example","title":"Example","text":"<p>For example, if you have \\(N(2 ,5)\\).</p> <p>This means that the mean is located at two along the \\(x\\) axis and the standard deviation, or width of the bell curve either side of this is five.</p>"},{"location":"unit3/lesson11/lesson11.html#solution-to-the-probabilistic-localization-problem-action-and-perception-updates","title":"Solution to the probabilistic localization problem: action and perception updates","text":"<p>In this video, we brings together all you have learnt so far about probabilistic localization methods to introduce two key steps in robot localization:</p> <ul> <li>the action (or prediction) update step</li> <li>the perception (or measurement) update step</li> </ul> <p>The video finishes with the Bayes filter localization algorithm that solves the probabilistic localization problem. This is a longer video, so please feel free to pause it to support your learning needs.</p> <p>Link to the video</p> <p>The video is approximately 17-minutes long. You may wish to pause it in the middle.</p> <p>Download transcript (PDF).</p>"},{"location":"unit3/lesson11/lesson11.html#review-summary-of-mathematics-presented-in-the-video","title":"Review \"Summary of mathematics presented in the video\"","text":"<p>Here, we summarize the algorithms for the two key steps in robot localization from the video, because you will need to remember these for the rest of Unit 1.</p> <p>In robot localization, we distinguish two update steps:</p> <p>1. Action (or prediction) update: the robot moves and estimates its position through its proprioceptive sensors.</p> <p></p> <p>During this step, the robot uncertainty grows.</p> <p>2. Perception (or measurement) update: the robot makes an observation using its exteroceptive sensors and corrects its position by combining its belief before the observation with the probability of making   exactly that observation.</p> <p>During this step, the robot uncertainty shrinks.</p> <p></p> <p>Solving the probabilistic localization problem consists in solving separately the Action and Perception updates.</p> <p>How do we solve the Action and Perception updates?</p> <p>Action update uses the Theorem of Total probability</p> \\[ p(x)=\\int_{y} p(x \\mid y) p(y) d y \\] <p>Perception update uses the Bayes rule</p> \\[ p(x \\mid y)=\\frac{p(y \\mid x) p(x)}{p(y)} \\] <p>(because of the use of the Bayes rule, probabilistic localization is also called Bayesian localization).</p> <p>Bayers filter localization algorithm</p> <p>for all \\(x_{t}\\) do</p> \\[     \\overline{\\operatorname{bel}}\\left(x_{t}\\right)=\\int p\\left(x_{t} \\mid u_{t}, x_{t-1}\\right) \\operatorname{bel}\\left(x_{t-1}\\right) d x_{t-1} \\] <p>(action, or prediction, update)</p> \\[     \\operatorname{bel}\\left(x_{t}\\right)=\\eta p\\left(z_{t} \\mid x_{t}, M\\right) \\overline{\\operatorname{bel}}\\left(x_{t}\\right) \\] <p>(perception, or measurement, update)</p> <p>end for</p> <p>return \\(\\operatorname{bel}\\left(x_{t}\\right)\\)</p> <p>In this final video in Lesson 3, we returns to examples he used at the beginning of the lesson to demonstrate how the action and perception updates work in a real world scenario.</p> <p>The video is approximately 4-minutes long.</p> <p>Download transcript (PDF).</p> <p>Link to the Video</p>"},{"location":"unit3/lesson11/lesson11.html#introduction-to-slam","title":"Introduction to SLAM","text":"<p>For the path planning, there are different methods, namely,  global and local methods of path planning for autonomous mobile robots. A mobile robot uses these methods to simultaneously localize and generate a map (SLAM). </p> <p>In most simplest of the cases, it is assumed that the map of the environment is already available to the robot. A robot tries to localize itself in respect to something, i.e. it uses the features of the map to identify its pose. At times, a map is not available and then, it is necessary for a mobile robot to generate its own map of the environment, and use this map during localization.</p> <p>Now, we consider how a robot can generate such a map. As a starting point, consider the option of you, as a robot operator, wanting to use a robot to clean your house. You could start by drawing a map by hand. You can measure the walls and objects, say, in your house, and build the map manually. You can give this map to your robot and it will use it to identify its pose. The problem with this solution is accuracy. How confident are you that you have hand drawn this map accurately enough for your robot to avoid obstacles?</p> <p>For accurate localization, we must very accurately measure and include the position of all landmarks (e.g., walls, artificial beacons, etc.) that a robot uses for localising in a map. Completing this map building task is often difficult, costly and time consuming, especially for large environments. For example, a robot working in a large museum with over forty rooms will need to map every individual room.</p> <p>Another consideration is a dynamic environment (i.e. the position of obstacles is changing). We will need to regularly update, re-measure and redraw a map of such an environment. Going back to the cleaner robot working in your house, you would need to re-draw the map every time you left something on the floor, didn't have time to tidy, or changed the layout of a room. This would probably be more work than doing the cleaning yourself!</p> <p>A better alternative is to get a robot to do the task of map building itself: we call this automatic map building.</p> <p>Next, in this video, we have explained how a robot can automatically create a map of an environment using sensors.</p> <p>Link to the Vodeo</p> <p>The video is approximately 7-minutes long.</p> <p>Download transcript (PDF).</p>"},{"location":"unit3/lesson11/lesson11.html#occupancy-grid-mapping-algorithm","title":"Occupancy grid mapping algorithm","text":"<p>You are now ready to look at mathematical methods for automatic map building. One such method is Occupancy grid mapping algorithm. With the occupancy grid method, a robot represents a map of the environment as an evenly spaced field of cells. Each cell represents the presence or absence of an obstacle at that location in the environment.</p> <p>This method relies on two observations:</p> <ul> <li> <p>As the robot moves around an environment, it keeps a counter for every cell in the grid. You can move your robot in the environment for a long time, ideally visiting the same place over and over to account for noise. Then:</p> </li> <li> <p>if a laser beam returns a hit for a cell, then the algorithm increments the counter of that cell. For example, if all the cells start as zero and the robot moves around the environment, if the robot gets a hit, it changes to one. If the same cell gets a second hit, then the counter changes to two</p> </li> <li> <p>if a laser beam travels through a cell, then the algorithm decrements the counter for that cell</p> </li> <li> <p>When the robot has completed its survey around the environment, you can threshold the counter value in each cell, to determine whether a cell is occupied or empty. For example:</p> </li> <li> <p>if the counter value is greater than the threshold, particularly a cell with many hits, then you can make a reasonable assumption that it is an occupied cell</p> </li> <li>If the counter value is lower than the threshold, then you can assume it is likely to be a free cell</li> </ul>"},{"location":"unit3/lesson11/lesson11.html#the-threshold","title":"The threshold","text":"<p>The threshold is a parameter that you can adjust, based on how conservative you want to be about your robot bumping into an obstacle, or how unsure you are as to the accuracy of your map of the environment. A good threshold parameter has a value slightly lower than the initial counter value you start with.</p> <p>The threshold accounts for a tricky scenario that can happen: it is possible for a cell to still be at its initial counter value after a robot has completed its survey of the environment. In this scenario, because the robot did not travel through this cell, we do not know if the robot will 'hit' an obstacle when it later moves through this cell, or whether it is actually a free cell.</p> <p>One solution you might think will work, is to move the robot around the environment enough times to make sure every cell has either a hit or a through. In the real world, this is not possible for a number of reasons for this:</p> <ul> <li>there are always some corner cells that remain at their initial counter value</li> <li>the cell may be inside an obstacle, meaning you can never hit it, because you always hit the outer boundary of the obstacle</li> <li>or the cell may be in an area which your robot has not explored</li> </ul> <p>This means that the safest thing to do in this scenario is for you to mark as occupied, all cells still at their initial counter value at the end of the mapping.</p> <p>For example, if the initial counter value is zero, the free cell counter value is one, and the occupied counter value is two, then after using your threshold parameter, all the cells still showing as zero will be converted to two. As your robot continues to move around the environment, it will continually update these counters to increase the accuracy of the map. This method is also useful for a dynamic environment, so that, if an obstacle moves in an environment, the counter value for those cells will automatically start decreasing.</p> <p>Example \"Example: Your cleaning robot\"</p> <p>For example, if your cleaning robot is mapping your bedroom and you decide to move your bed from one corner into the middle of the wall. To start with, the counter had many hits in the corner of the room. Your robot rightly thinks there is an obstacle in the corner. Later, your robot gets many through in that same corner. It will gradually decrement the contours and, after a while, it realizes that the corner is now free for it to use.</p> <p>In this video, we first gives an example of a map built by a robot using its senses to complete automatic map building. Then he reviews the assumptions we made when we followed the process of occupancy grid mapping in the first video in this lesson.</p> <p>Link to the Video</p> <p>The video is approximately 4-minutes long.</p> <p>Download transcript (PDF).</p>"},{"location":"unit3/lesson11/lesson11.html#14-simultaneous-localization-and-mapping-slam","title":"1.4 Simultaneous localization and mapping (SLAM)","text":"<p>Localization and Mapping are like chicken and egg. Both require each other to exist.</p> <ul> <li> <p>If you have a good map, then you can perform good localization using the methods we discussed in Unit 1.</p> </li> <li> <p>If you have good localization (perhaps through artificial markers you put on your robot and a special camera in the environment), then you can construct a good map.</p> </li> </ul> <p>If you do not have a good map, and you do not have artificial markers for your robot, what can you do?</p> <p>For example, you place a Turtlebot in a new room at the start of a session. You do not have an environment map and you do not accurately know the location for your robot, what can you do at this starting point when you do not have any information?</p> <p>The solution is to try building the map and localising the robot at the same time.</p> <p>There are algorithms that try to build the map and localize simultaneously. We call them simultaneous localization and mapping, or SLAM, algorithms.</p> <p>In this video, we explained the idea of SLAM before we discuss SLAM algorithms in the next step of this lesson.</p> <p>Link to the Video</p> <p>The video is approximately 10-minutes long.</p> <p>Download transcript (PDF).</p>"},{"location":"unit3/lesson11/lesson11.html#slam-algorithms","title":"SLAM algorithms","text":"<p>In this lesson, we have introduced you to the high-level general idea of SLAM. There are different algorithms implementing this general SLAM idea.</p> <p>One algorithm is called Particle Filtering SLAM, which extends the Particle Filter Localization (Monte Carlo Localization) to SLAM.</p> <p>All these algorithms keep a belief (a probability distribution) about the robot pose and the structure of the map. This is different to what we learnt in Unit 1, where the belief was only about the robot\u2019s pose.</p> <p>For example, in Particle Filtering SLAM, each particle is not only a hypothesis for the pose for the robot, but is also a hypothesis about the structure of the map (e.g. occupancy of cells, or location of features).</p> <p>In this video, we showed you an example SLAM environment map created by Sebastian Thrun using Particle Filtering SLAM. Sebastian was the first director of the Google Car Project.</p> <p>Link to the Video</p> <p>The video is approximately 5-minutes long.</p> <p>Download transcript (PDF).</p>"},{"location":"unit3/lesson11/lesson11.html#autonomous-car-technology","title":"Autonomous car technology","text":"<p>These SLAM algorithms are at the heart of today\u2019s autonomous car technologies.</p> <p>You can read more about driverless cars in these press releases (all open in a new browser tab):</p> <p>BBC: How driverless cars will change our world.</p> <p>The Guardian: Uber riders in Pittsburgh can hail self-driving Ford Fusions in test program.</p> <p>BBC: US releases highway code for robots.</p>"},{"location":"unit3/lesson11/lesson11.html#further-reading","title":"Further Reading","text":"<p>For a detailed understanding of classical robotics localization algorithms and how to approach this problem further, see these two books</p> <ul> <li> <ol> <li>Introduction to Autonomous Mobile Robots (Siegwart et. al, 2011)</li> </ol> </li> <li> <ol> <li>Introduction to AI Robotics, 1<sup>st</sup> and 2<sup>nd</sup> Edition, Robin R Murphy (Murphy, 2000, 2019)</li> </ol> </li> </ul>"},{"location":"unit3/lesson11/lesson11.html#lesson-complete","title":"Lesson complete","text":"<p>Having completed this lesson, you should be able to recall there are four main sources of uncertainty for a robot\u2019s location:</p> <ul> <li>initial pose uncertainty because it is not easy to know a robot's start pose in the environment</li> <li>effector noise also called motion noise, motion uncertainty and actuator uncertainty</li> <li>sensor noise the actual errors that your sensor makes</li> <li>sensor aliasing when your environment looks similar from different positions</li> <li>explain why it is important for a robot to have a map of its environment so that it can navigate and avoid obstacles</li> <li>describe fundamental methods and algorithms that we use for a robot to localize its pose in an environment</li> <li>discuss the benefits and challenges of methods for creating a map of an environment for a robot</li> <li>outline the key benefits of Simultaneous localization and mapping, or SLAM</li> <li>recall applications for the SLAM algorithm, including its use in autonomous car technology</li> </ul>"},{"location":"unit3/lesson8/lesson8.html","title":"8. Temporal Difference","text":"<p>The notebook uses a library of functionality in RL that aims for simplicity and general insight into how algorithms work, these libraries are written from scratch using standard Python libraries (numpy, matplotlib etc.). Please note that you will need permission from the author to use the code for research, commercially or otherwise.</p>"},{"location":"unit3/lesson8/lesson8.html#introduction-to-bootstrapping","title":"Introduction to Bootstrapping","text":"<p>In this and subsequent units, we cover a set of RL algorithms that use bootstrapping, a powerful idea that allows us to create online updates that do not wait until the end of an episode to learn from the experience, live as it comes. We will continue on the tabular method, cover planning, and then move to function approximation methods. Along the way, we cover encoding techniques for state space traditionally used in RL, such as tile coding. On the function approximation, we will assume a linear model in this unit. We cover non-linear models from an application perspective in the subsequent unit. We are mainly concerned with regression not classification from a machine learning perceptive.</p> <p>The settings are still the same as that of an MDP. However, we assume that the state space is large and may not be practical to represent each state as an entry in a table. The states might also not manifest themselves clearly, and only we can obtain some observations about them. These observations result in a set of numerical, categorical or boolean features which we can then numerically deal with them as we did in earlier modules.</p> <p>Unit 3: Learning Outcomes By the end of this unit, you will be able to:  </p> <ol> <li>Assess the role of bootstrapping in RL and its impact on learning efficiency.  </li> <li>Explain n-step methods and the trade-offs associated with different values of n.  </li> <li>Compare n-step backup action-value-based control methods with direct policy estimation methods.  </li> <li>Evaluate how Temporal Difference (TD) methods obtain biased but low-variance estimates through environment interaction.  </li> <li>Analyze how actor-critic methods achieve biased but low-variance estimation through interaction with the environment.  </li> <li>Discuss the trade-offs between online and offline RL algorithms.  </li> <li>Design planning methods that incorporate model learning into RL.  </li> </ol>"},{"location":"unit3/lesson8/lesson8.html#lesson-7-tabular-methods-temporal-difference-learning","title":"Lesson 7-Tabular Methods: Temporal Difference Learning","text":"<p>Learning outcomes 1. understand the idea of bootstrapping and how it is being used in TD 2. understand the differences between MC and TD and appreciate their strengths and weaknesses 3. understand how to use the ideas of TD to extend it to a control method such as Sarsa and Q-learning</p> <p>Reading: You may refer to chapter 6 of the text book available online here. Please note that we explain the ideas of this topic from a practical perspective and not from a theoretical perspective which is already covered in the textbook.</p> <p>In this lesson, we cover the Temporal Difference learning method. TD is one of the fundamental ideas in RL. It uses bootstrapping to improve its predictions. The idea behind bootstrapping is to use (own estimation) to improve (own estimation) with an indication from the ground truth in the form of a reward. This sound surprising since we are not using a direct ground truth to revert to when we are improving the prediction. However, it turns out that there are theoretical guarantees that the method will converge to a solution that is usually close to optimal. The one constant stream of ground truth the agent keeps receiving is the rewards in each state. One of the major strengths of TD is that it can be used online without having to wait till the end of the episode as we did in the Monte Carlo methods. This also makes it extremely efficient and allows it to converge faster in practice *than MC. TD uses ideas similar to what we did in GPI: slightly improving the prediction and *not waiting until everything is clear (at the end of an episode). This idea is similar to what we did in stochastic mini-batch updates in ML. We will call it eagerness to learn. I.e., to grab whatever information is available and whenever it becomes available but at the same time keep accumulating a stock of this information to help us improve and sharpen our prediction. We will then move into designing control algorithms that depend on TD, we will tackle old and new algorithms, including Sarsa, Expected Sarsa, Q-learning and double Q-learning, and we will test them extensively using the infrastructure that we developed in the previous lesson. Finally, we conclude by studying a policy gradient algorithm for control, namely actor-critic, that depends on TD and REINFORCE.</p> <p>As usual, we will take a practical/pragmatic approach to cover the material and leave the theory to the book, which is well covered. Note that there are far more rigorous books that take special care for the mathematics guarantees behind the ideas of RL, which are not covered in our textbook Introduction to Reinforcement Learning but can be found in operation research books such as Neuro-Dynamic Programming.</p> <p>Plan As usual, in general there are two types of RL problems that we will attempt to design methods to deal with  1. Prediction problem For These problems we will design Policy Evaluation Methods that attempt to find the best estimate for the value-function given a policy.</p> <ol> <li>Control problems  For These problems we will design Value Iteration methods which utilise the idea of Generalised Policy Iteration. They attempt to find the best policy, via estimating an action-value function for a current policy then moving to a better and improved policy by choosing a greedy action often. We will then move to Policy Gradient methods that directly estimate a useful policy for the agent by maximising its value function.</li> </ol> <pre><code>%matplotlib inline\n</code></pre> <pre><code>from rl.rl import *\n</code></pre> <p>Ok, so we start by implementing the TD algorithm. Due to the way we structured our code and classes, it is relatively simple and straightforward to define any online and offline methods. TD is an online method that will be called in each step during an episode. We, therefore, can turn off the storage because we do not need it, but leaving it will not hurt the grid problems we are tackling. It will consume some memory and a few extra milliseconds of processing. For more difficult problems, we need to utilise the memory to train anyway, as we shall see in the Application unit.</p> <p>We also would need to pass a learning step as we did for the MC algorithm. A learning step dictates how much error percentage will be considered when we update the value function. Sometimes we could go all the way \u03b1=1 when the algorithm is tabular, and the problem is simple. For most of the problems and algorithms we tackle, however, this is not desirable, and we set \u03b1=.1 or less to ensure the algorithm performs well on the common states and is acceptable on less common states. MC, however, is particularly sensitive towards this \u03b1, and we often would need to set it to smaller values such as .01.</p> <pre><code>class TD(MRP):\n    # def stop_exp(self):\n\n    # ----------------------------- \ud83c\udf16 online learning ----------------------    \n    def online(self, s, rn,sn, done, *args): \n        self.V[s] += self.\u03b1*(rn + (1- done)*self.\u03b3*self.V[sn] - self.V[s])\n</code></pre> <p>Note how we multiplied the value \\(V[s_{t+1}]\\) by (1- done). This is to ensure that when the episode is finished (ex., the agent is at goal or has achieved the task), we want only the final reward \\(r_{t+1}\\) to participate in the update and not \\(V[s_{t+1}]\\). This multiplication will appear in all of the updates we use. This saves us from having to treat the goal states in a special way on the environment level (ex. we could have set the value \\(V[s_{t+1}]\\)=0 by checking if \\(s_{t+1}==goal\\) or by checking done in the environment or by treating done inside the s_() function when we use function approximation in later lessons). We felt that this would disguise this information, and it is always better to be explicit when possible.</p> <p>Note also that we didn't use a and an in the online() function because we are making predictions in TD (no control yet). In addition, we do not store the experience for this one-step online algorithm while we had to for MC, which is again one of the advantages of online methods.</p> <p>Let us test our brand new TD algorithm on the random walk prediction problem. Note that randwalk is the default environment for MRP anyway and hence no need to pass it.</p> <pre><code>TDwalk = TD(episodes=100, v0=.5, **demoV())\nTDwalk.interact(label='TD learning')\n</code></pre> <pre><code>&lt;__main__.TD at 0x1258e5a60&gt;\n</code></pre> <p></p> <pre><code>TDwalk.store\n</code></pre> <pre><code>False\n</code></pre> <p>Note that we did not need to store the episodes trajectories in a pure online method, hence these methods are usually more memory efficient that there offline counterpart! Note how TD performed far better and converged faster in fewer episodes than MC</p> <p>Of course we can call interact immediately  as follows.</p> <pre><code>TDwalk = TD(episodes=100, v0=.5, **demoV()).interact(label='TD learning')\n</code></pre> <p></p>"},{"location":"unit3/lesson8/lesson8.html#offline-td","title":"Offline TD","text":"<p>In this section, we develop an offline TD algorithm. This is not a common algorithm as it usually defies the reason for using TD. That is, we usually use TD because it is an online algorithm. Nevertheless, studying this algorithm allows us to appreciate the strengths and weaknesses of TD and to compare its performance with other offline algorithms, such as MC.</p> <pre><code>class TDf(MRP):\n\n    def init(self):\n        self.store = True\n\n    # ----------------------------- \ud83c\udf18 offline TD learning ----------------------------   \n    def offline(self):\n        #for t in range(self.t, -1, -1):\n        for t in range(self.t+1):\n            s = self.s[t]\n            sn = self.s[t+1]\n            rn = self.r[t+1]\n            done = self.done[t+1]\n\n            self.V[s] += self.\u03b1*(rn + (1- done)*self.\u03b3*self.V[sn]- self.V[s])\n</code></pre> <pre><code>TDwalk = TDf(\u03b1=.05, episodes=100, v0=.5, **demoV()).interact(label='TD learning')\n</code></pre> <p></p> <p>Note how we overrode the offline function in our MRP class that we covered in the previous lesson. The first three lines inside the for loop are to make the update format of the online and offline identical. We could have also made the algorithm go backwards, similar to MC. Each has its advantage and disadvantage, although for TD since it uses the temporal difference error, it usually makes little difference. You can uncomment the backward loop and try it yourself.</p>"},{"location":"unit3/lesson8/lesson8.html#conducting-trialsseveral-runs-of-experiments","title":"Conducting trials(several runs) of experiments","text":"<p>Let us now create a useful handy class that summarizes several runs for us to reach a reliable and unbiased conclusions when we compare algorithms performances.</p> <pre><code>class Runs: # experimental trials\n\n    def __init__(self, \n                 algorithm=None,\n                 runs=10, \n                 plotR=False, \n                 plotT=False, \n                 plotE=False,\n                 **kw): \n\n        self.algorithm = algorithm if algorithm is not None else MRP(**kw)\n        self.runs = runs\n        self.plotR = plotR\n        self.plotT = plotT\n        self.plotE = plotE\n\n\n    def header(self):\n        return 'runs|'          + self.algorithm.header()\n    def values(self, results):\n        return '%4d|'%self.runs + self.algorithm.values(results)\n\n    def init(self):\n        np.random.seed(1)# for binomial, choice and randint\n        random.seed(1)   # for using choices\n\n        self.Rs = np.zeros((self.runs, self.algorithm.episodes))\n        self.Ts = np.zeros((self.runs, self.algorithm.episodes))\n        self.Es = np.zeros((self.runs, self.algorithm.episodes))\n\n    def isplot(self):\n        return self.plotR or self.plotT or self.plotE\n\n    def interact(self, label='', frmt='-', **kw):\n        self.init()\n        runs,  algorithm = self.runs, self.algorithm\n        if not label: \n            label = 'no label passed'\n\n        start_time = time.time()\n        for self.run in range(runs):\n            run = self.run\n\n            # visual env in the last run, usually no need to visualise other runs\n            visual = algorithm.visual and run==runs-1  \n            label_ = ', run=%d'%(run+1)\n\n            algorithm = algorithm.interact(visual=visual, label=label_, seed=run, **kw)\n            self.Rs[run] = algorithm.Rs \n            self.Ts[run] = algorithm.Ts\n            self.Es[run] = algorithm.Es            \n            self.runstime = progress(self.run, runs, start_time, self.isplot())\n\n        if self.isplot(): self.plot(label, frmt)\n\n        return self\n\n    def plot(self, label='', frmt='-') :\n        Rs, Ts, Es, algorithm = self.Rs, self.Ts, self.Es, self.algorithm\n        label_ =' averaged over %d runs'%(self.runs)\n        if self.plotT: plt.plot(algorithm.eplist, Ts.mean(0), frmt, label=label); plt.xlabel('episodes,'+label_); plt.legend()\n        if self.plotR: plt.plot(algorithm.eplist, Rs.mean(0), frmt, label=label); plt.xlabel('episodes,'+label_); plt.legend()\n        if self.plotE: plt.plot(algorithm.eplist, Es.mean(0), frmt, label=label); plt.xlabel('episodes,'+label_); plt.legend()\n\n        return self\n\n# ============================ useful progress bar ======================================\n'''\n    useful progress bar function we can use tqdm but it does not play well sometime\n'''\ndef progress(i, I, start, show=True, color=0):\n    if show:\n        percent = int(100 * (i+1)//I)\n        print(f'\\r{percent}%|\\033[9{color}m{\"\u2588\" * int(percent*.9)}\\033[0m|{i+1}/{I}', \\\n              end='\\r' if i+1&lt;I else '\\n')\n\n    return int((time.time()- start)*1000)\n</code></pre> <p>Note how the class allows us to run several experiments efficiently. The main assumption is that the algorithms are inherited from an MRP class which applies for the majority of the classes that we will deal with in our units.</p> <p>Let us now see how we can use this new class to easily run experiments to study how an algorithm behaves. Below we show a function that compares TD with MC on different learning rates. You can read about this comparison and the associated figure in Example 6.2 of the book (hence the function's name). We will follow this trend of naming functions after their counterpart examples or figures in the book.</p> <pre><code>def TD_MC_randwalk(env=randwalk(), alg1=TDf, alg2=MC):\n    plt.xlim(0, 100)\n    plt.ylim(0, .25)\n    plt.title('Empirical RMS error, averaged over states')\n\n    for \u03b1 in [.05, .1, .15]:\n        TD\u03b1s = Runs(algorithm=alg1(env=env, \u03b1=\u03b1, v0=.5), runs=100, plotE=True).interact(label='TD \u03b1= %.2f'%\u03b1, frmt='-')\n\n    for \u03b1 in [.01, .02, .03, .04]:\n        MCs = Runs(algorithm=alg2(env=env, \u03b1=\u03b1, v0=.5), runs=100, plotE=True).interact(label='MC \u03b1= %.2f'%\u03b1, frmt='--')\n\ndef example_6_2(**kw): return TD_MC_randwalk(**kw)\n\nexample_6_2()\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n</code></pre> <p></p> <p>We have already imported MC to compare its performance with our newly defined offline TD. Remember that MC is also offline algorithm.</p> <p>Note that we could have simply defined  </p> <p>example_6_2=TD_MC_randwalk</p> <p>and then call it as</p> <p>example_6_2()</p> <p>but that would make importing it in other lessons harder.</p>"},{"location":"unit3/lesson8/lesson8.html#optimality-of-td","title":"Optimality of TD","text":"<p>In this section, we study the optimality of TD. We develop two algorithms, Batch TD and Batch MC. Both of these algorithms operate in a supervised learning fashion. We collect a set of episodes and then deal with them as mini-batches, and then we run a set of epochs that repeatedly present the so-far experience until the algorithm converges. We use TD and MC updates inside the algorithm to see which value each converges to. By doing so, we have levelled up the strength of both algorithms (both are offline and wait until the end of each episode to accommodate all past experiences after each episode), and we laid their performance on pure convergence terms.</p> <pre><code>class MRP_batch(MRP):\n\n    def __init__(self, **kw):\n        super().__init__(**kw)\n        self.store = True # store the full experience\n\n    # we will redfine the allocate to store the full experience instead of only latest episode\n    def allocate(self): \n        self.r = np.zeros((self.max_t, self.episodes))\n        self.s = np.ones ((self.max_t, self.episodes), dtype=np.uint32) *(self.env.nS+10)  \n        self.a = np.zeros((self.max_t, self.episodes), dtype=np.uint32)  # actions and states are indices        \n        self.done = np.zeros((self.max_t, self.episodes), dtype=bool)\n\n    def store_(self, s=None,a=None,rn=None,sn=None,an=None, done=None, t=0):\n        # store one trajectory(sarsa) in the rigth episode buffer\n        if s  is not None: self.s[t, self.ep] = s\n        if a  is not None: self.a[t, self.ep] = a\n        if rn is not None: self.r[t+1, self.ep] = rn\n        if sn is not None: self.s[t+1, self.ep] = sn\n        if an is not None: self.a[t+1, self.ep] = an\n        if done is not None: self.done[t+1, self.ep] = done\n\n    # returns the agent's trace from latest episode buffer\n    def trace(self):\n            return self.s[:self.t+1, self.ep]\n</code></pre> <p>Below we inherit the above class to allow us to conduct batch TD learning. This form of learning is usually not practical, but it is listed here for studying the behaviour of TD to gain insight into what kind of target it has and compare it with MC. The point is to prove that TD, in practice, indeed has a different goal than MC and is more efficient in converging to this target, which in turn, usually reduces the error more effectively than MC does.</p> <pre><code>class TD_batch(MRP_batch):\n    def __init__(self, \u03b1=.001, **kw):\n        super().__init__(\u03b1=\u03b1, **kw)\n\n    # -----------------------------------\ud83c\udf18 offline learning------------------------------------- \n    def offline(self):\n        # epochs\n        while True:\n            \u0394V = self.V*0\n            # each episode acts like a mini-batch in supervised learning\n            for ep in range(self.ep+1): \n                for t in range(self.Ts[ep]):#-1, -1, -1):\n                    s  = self.s[t, ep]\n                    sn = self.s[t+1, ep]\n                    rn = self.r[t+1, ep]\n                    done = self.done[t+1, ep]\n\n                    \u0394V[s] += rn + (1- done)*self.\u03b3*self.V[sn]- self.V[s]\n            \u0394V *= self.\u03b1\n            # exit the epochs loop if there is no more meaningful changes (method converged)\n            if np.abs(\u0394V).sum() &lt; 1e-3:  break #; print('exit')\n            self.V += \u0394V\n</code></pre> <pre><code>TDwalk_batch = TD_batch(episodes=100, v0=-1, **demoV()).interact()\n</code></pre> <p></p> <p>Note how the batch updates have much smoother and faster convergence per-episodes than a usual TD or MC. However, they have a much higher computational cost that makes them not suitable for practical problem.</p> <pre><code>class MC_batch(MRP_batch):\n    def __init__(self, \u03b1=.001, **kw):\n        super().__init__(\u03b1=\u03b1,**kw)\n\n    # -----------------------------------\ud83c\udf18 offline learning------------------------------------- \n    def offline(self):\n        # epochs\n        while True:\n            \u0394V = self.V*0\n            # each episode acts like a mini-batch in supervised learning\n            for ep in range(self.ep+1):\n                Gt = 0\n                for t in range(self.Ts[ep]-1, -1, -1):\n                    s  = self.s[t, ep]\n                    rn = self.r[t+1, ep]\n\n                    Gt = rn + self.\u03b3*Gt \n                    \u0394V[s] += Gt - self.V[s]\n\n            \u0394V *= self.\u03b1\n            # exit the epochs loop if there is no more meaningful changes (method converged)\n            if np.abs(\u0394V).sum() &lt; 1e-3: break #;print('exit')\n            self.V += \u0394V\n</code></pre> <pre><code>MCwalk_batch = MC_batch(episodes=100, v0=-1, **demoV()).interact()\n</code></pre> <p></p>"},{"location":"unit3/lesson8/lesson8.html#batch-runs","title":"Batch runs","text":"<p>Now it is time to run experiments to specify which algorithm is better. We follow the experiments conducted in figure 6.2 in the book. Note that we initialise to -1 this time to smoothen the resultant figure and remove any advantages the algorithms had when starting from .5 probabilities. This means that the algorithm would have to guess all the way from -1 to the probability of starting in a state s and ending up in the right terminal state. </p> <p>We start with 10 runs to show the full range that the algorithm will take in the early episodes, and then in the definition of figure_6_2( ), we restrict the figure's limit to show the interesting trend of each algorithm. Note that the algorithms could have been made more efficient by some further optimization which we left out for pedagogical reasons.</p> <pre><code>\u03b1=.001\nTDB = Runs(algorithm=TD_batch(v0=-1, \u03b1=\u03b1, episodes=100), runs=3, plotE=True).interact(label= 'Batch TD, \u03b1= %.3f'%\u03b1)\nMCB = Runs(algorithm=MC_batch(v0=-1, \u03b1=\u03b1, episodes=100), runs=3, plotE=True).interact(label='Batch MC, \u03b1= %.3f'%\u03b1)\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|3/3\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|3/3\n</code></pre> <p></p> <pre><code>def figure_6_2():\n    plt.gca().spines['right'].set_visible(False)\n    plt.gca().spines['top'].set_visible(False)\n    plt.xlim(0,100)\n    plt.ylim(0, .25)\n    plt.title('Batch Training')\n\n    \u03b1=.001\n    TDB = Runs(algorithm=TD_batch(v0=-1, \u03b1=\u03b1, episodes=100), runs=100, plotE=True).interact(label= 'Batch TD, \u03b1= %.3f'%\u03b1)\n    MCB = Runs(algorithm=MC_batch(v0=-1, \u03b1=\u03b1, episodes=100), runs=100, plotE=True).interact(label='Batch MC, \u03b1= %.3f'%\u03b1)\n</code></pre> <pre><code>figure_6_2()\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n</code></pre> <p></p>"},{"location":"unit3/lesson8/lesson8.html#td-control","title":"TD Control","text":"<p>In this section, we deal with TD control. We cover mainly two algorithms one is Sarsa which is an on-policy control algorithm (meaning the followed policy is the same as the policy we are learning about). The second main algorithm is the famous Q-learning algorithm which is an off-policy algorithm. In the case of Q-learning, the agent is acting according to an \u03b5-greedy algorithm while it is learning about a greedy algorithm.</p> <p>Similar to what we did earlier we will use the two dictionaries demoQ and demoR to make the calls more concise.</p>"},{"location":"unit3/lesson8/lesson8.html#sarsa-on-policy-control","title":"Sarsa on-policy control","text":"<pre><code>class Sarsa(MDP()):\n\n    def init(self): #\u03b1=.8\n        self.step = self.step_an # for Sarsa we want to decide the next action in time step t\n\n    # ----------------------------------------\ud83c\udf16 online learning ----------------------------------------\n    def online(self, s, rn,sn, done, a,an):\n        self.Q[s,a] += self.\u03b1*(rn + (1- done)*self.\u03b3*self.Q[sn,an] - self.Q[s,a])\n</code></pre> <p>Note that we do not store the experience for this one-step online algorithm while we had to for MC, and this is again one of the advantages of online methods.</p> <p>Let us now apply the Sarsa on a simple grid world environment. The goal is directly facing the start position. However, to make the problem more difficult for the algorithm we have deprioritised the right action and we place the order of the actions as follows: left, right, down and up. This simple change made the agent pick going left before going right and made the problem only a bit more difficult. Let us see how the Sarsa performs on it.</p> <pre><code>env2x3 = Grid(gridsize=[2, 3], reward='reward_', s0=0, goals=[5], figsize=[10,1])\nenv2x3.render(underhood='maxQ')\n</code></pre> <p></p> <pre><code>Qs1 = np.array([.1, .4  ,.0 , .3])\n(np.exp(Qs1)/np.exp(Qs1).sum()).round(2)\n</code></pre> <pre><code>array([0.22, 0.3 , 0.2 , 0.27])\n</code></pre> <pre><code>sarsa = Sarsa(env=env2x3, \u03b1=.1, \u03b3=.9, episodes=10, store=True, seed=0, **demoQ())\nsarsa.interact()\n</code></pre> <pre><code>&lt;__main__.Sarsa at 0x125c49f70&gt;\n</code></pre> <p></p> <pre><code>sarsa.s[:sarsa.t+2]\n</code></pre> <pre><code>array([0, 1, 4, 5], dtype=uint32)\n</code></pre> <pre><code>sarsa.Q\n</code></pre> <pre><code>array([[0.        , 0.05685457, 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.23751096],\n       [0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        ],\n       [0.        , 0.65132156, 0.        , 0.        ],\n       [0.        , 0.        , 0.        , 0.        ]])\n</code></pre> <pre><code>sarsa = Sarsa(env=grid(), \u03b1=.8, episodes=50, seed=10, **demoQ()).interact()\n</code></pre> <p></p> <pre><code>%time sarsa = Sarsa(env=grid(reward='reward_1'), q0=10, \u03b5=.4, \u03b1=.3, episodes=20000, seed=1, **demoQ()).interact()\n</code></pre> <pre><code>CPU times: user 4.85 s, sys: 416 ms, total: 5.26 s\nWall time: 3.89 s\n</code></pre> <p></p> <pre><code>sarsa = Sarsa(env=grid(reward='reward100'), \u03b1=.3, episodes=20, seed=1, **demoQ()).interact()\n</code></pre> <p></p> <pre><code>mc = MCC(env=grid(reward='reward100'), \u03b1=.3, episodes=20, seed=1, **demoQ()).interact()\n</code></pre> <p></p> <p>Note how Sarsa performed better and converged faster in fewer episodes than MCC</p> <pre><code>%time sarsa = Sarsa(env=grid(reward='reward100'),  \u03b1=.3, episodes=20, seed=1, plotT=True).interact(label='Sarsa')\n%time mc = MCC(env=grid(reward='reward100'), \u03b1=.3, episodes=20, seed=1, plotT=True).interact(label='MCControl')\n</code></pre> <pre><code>CPU times: user 60.4 ms, sys: 14.4 ms, total: 74.8 ms\nWall time: 25 ms\n</code></pre> <p></p> <p>Of course we change the seed the performance will change for both. Also if we change the learning rate \u03b1 the performance will vary (change the seed to 0 and run). This is why it is important to conduct several runs in order to obtain the performance of the algorithms on average.</p> <pre><code>def Maze(rows=6, cols=9, **kw):\n    return Grid(gridsize=[rows,cols], s0=int((rows)/2)*cols, goals=[rows*cols-1], style='maze', **kw)\n\ndef maze_large(**kw):\n    return Maze(rows=16, cols=26, figsize=[25,4],**kw)\n</code></pre> <pre><code>sarsa_large = Sarsa(env=maze_large(), \u03b1=.1, episodes=500, seed=0 , **demoQ()).interact()\n</code></pre> <p></p>"},{"location":"unit3/lesson8/lesson8.html#changing-actions-priorities","title":"Changing actions priorities","text":"<p>To change the action priority we can do the following:</p> <pre><code>env = grid(reward='reward100')\nenv._right, env._left, env._up , env._down=tuple(range(0,4))\nsarsa = Sarsa(env=env, episodes=20, seed=10, **demoQ()).interact()\n</code></pre> <p></p> <p>Note how the new dynamics made the solution a bit easier. This makes more difference specifically for the MCC because it depends on explore-start.</p> <pre><code>mc = MCC(env=env, \u03b1=.2, episodes=50, **demoQ()).interact()\n</code></pre> <p></p>"},{"location":"unit3/lesson8/lesson8.html#sarsa-on-windy-environment","title":"Sarsa on windy environment","text":"<p>In this section we show how Sarsa behaves on the windy environment that we have shown in lesson 2. The idea to show that TD is able of learning to deal with the upward wind in a manner that allows it to reach the goal effectively. This study can be seen in Example 6.5 in the book.</p> <pre><code>def Sarsa_windy():\n    return Sarsa(env=windy(reward='reward1'), \u03b1=.5, seed=1, **demoQ(), episodes=170).interact(label='TD on Windy')\n\nexample_6_5 = Sarsa_windy\n\ntrainedV = example_6_5()\n\nplt.subplot(133).plot(trainedV.Ts.cumsum(), range(trainedV.episodes),'-r')\nplt.show()\n</code></pre> <p></p>"},{"location":"unit3/lesson8/lesson8.html#q-learning-off-policy-control","title":"Q-learning off-policy control","text":"<p>Now we move to the Q-learning algorithm. Q-learning is one of the most successful algorithms in RL. Although it is an off-policy (not offline) algorithm, it usually performs better than the Sarsa. Q-learning also allowed for a control algorithm's first proof of convergence due to its simple update rules. </p> <p>Important Note that Q-learning does not require changing the step function because it does not require knowing the next action in advance (unlike Sarsa). Hence it uses a simple algorithmic schema that is almost identical to TD.</p> <pre><code>class Qlearn(MDP()):\n\n    #--------------------------------------\ud83c\udf16 online learning --------------------------------------\n    def online(self, s, rn,sn, done, a,_):\n        self.Q[s,a] += self.\u03b1*(rn + (1- done)*self.\u03b3*self.Q[sn].max() - self.Q[s,a])\n</code></pre> <p>As you can see, we did not use the action an in Qlearning() because we take the max of the action and assume that it is the one that the agent will pick (although this might not be the case, and hence it is an off-policy learning algorithm because we are learning about a fully greedy policy while the agent is acting according to an \u03b5greedy policy). Also note that we do not store the experience for this one-step online algorithm while we had to for MC, which is again one of the advantages of online methods.</p> <pre><code>qlearn = Qlearn(env=grid(), \u03b3=1, \u03b1=.8, episodes=40, seed=10, **demoQ()).interact()\n</code></pre> <p></p> <pre><code>qlearn = Qlearn(env=grid(reward='reward1'), \u03b1=.8, episodes=40, seed=10, **demoQ()).interact()\n</code></pre> <p></p>"},{"location":"unit3/lesson8/lesson8.html#sarsa-and-q-learning-on-a-cliff-edge","title":"Sarsa and Q-Learning on a Cliff Edge!","text":"<p>This section compares the performance of on-policy Sarsa and off-policy Q-learning algorithms to show how each act on a specific problem. The problem that we will tackle is a cliff-edge world. This is a grid world of 12x4, with a goal location on the far-right bottom corner and the start location on the far-left bottom corner. There are no obstacles. However, there is a cliff between the start and the goal locations on the bottom. If the agent trespasses on it, it falls off the cliff, receives a penalty of -100 and will be relocated back to the start location without starting a new episode. The agent receives a reward of -1 everywhere, including the goal location. We will use the sum of rewards metric to measure the performance of algorithms on this problem.</p> <pre><code>sarsa = Sarsa(env=cliffwalk(), \u03b1=.5, episodes=50, seed=1, **demoR()).interact()\n</code></pre> <p></p> <pre><code>sarsa = Qlearn(env=cliffwalk(), \u03b1=.5, episodes=50, seed=1, **demoR()).interact()\n</code></pre> <p></p> <pre><code>def Sarsa_Qlearn_cliffwalk(runs=200, \u03b1=.5, env=cliffwalk(), alg1=Sarsa, alg2=Qlearn):\n    plt.gca().spines['right'].set_visible(False)\n    plt.gca().spines['top'].set_visible(False)    \n    plt.yticks([-100, -75, -50, -25])\n    plt.ylim(-100, -10)\n\n\n    SarsaCliff = Runs(algorithm=alg1(env=env, \u03b1=\u03b1, episodes=500), runs=runs, plotR=True).interact(label='Sarsa')\n    QlearnCliff = Runs(algorithm=alg2(env=env, \u03b1=\u03b1, episodes=500), runs=runs, plotR=True).interact(label='Q-learning')\n    return SarsaCliff, QlearnCliff\n\ndef example_6_6(**kw): return Sarsa_Qlearn_cliffwalk(**kw)\n</code></pre> <pre><code>SarsaCliff, QlearnCliff = Sarsa_Qlearn_cliffwalk()\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|200/200\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|200/200\n</code></pre> <p></p> <pre><code>sarsa = Sarsa(env=maze(reward='reward1'), episodes=20, seed=10, **demoQ()).interact()\n</code></pre> <p></p>"},{"location":"unit3/lesson8/lesson8.html#expected-sarsa","title":"Expected Sarsa","text":"<p>In this section, we cover the expected Sarsa algorithm. This algorithm is very similar to the Q-learning algorithm and has the same schematic structure (unlike Sarsa, it does not require obtaining the next action in advance). It takes all the probabilities of the different actions and forms an expectation of the next action.</p> <pre><code>class XSarsa(MDP()):\n\n    # ------------------------------------- \ud83c\udf16 online learning --------------------------------------\n    def online(self, s, rn,sn, done, a,_):      \n        # obtain the \u03b5-greedy policy probabilities, then obtain the expecation via a dot product for efficiency\n        \u03c0 = self.\u03c0(sn)\n        v = self.Q[sn].dot(\u03c0)\n        self.Q[s,a] += self.\u03b1*(rn + (1- done)*self.\u03b3*v - self.Q[s,a])\n</code></pre> <p>Note that the policy is assumed to be \u03b5-greedy, if you want to deal with other policies then a different implementation is required</p> <pre><code>xsarsa = XSarsa(env=cliffwalk(), \u03b1=.5, episodes=50, seed=1, **demoR()).interact()\n</code></pre> <p></p>"},{"location":"unit3/lesson8/lesson8.html#double-q-learning","title":"Double Q-learning","text":"<pre><code>class DQlearn(MDP()):\n\n    def init(self):\n        self.Q1 = self.Q\n        self.Q2 = self.Q.copy()\n\n    # we need to override the way we calculate the aciton-value function in our \u03b5greedy policy\n    def Q_(self, s=None, a=None):\n            return self.Q1[s] + self.Q2[s] if s is not None else self.Q1 + self.Q2\n\n    # ----------------------------- \ud83c\udf16 online learning ----------------------\n    def online(self, s, rn,sn, done, a,_): \n        p = np.random.binomial(1, p=0.5)\n        if p:    self.Q1[s,a] += self.\u03b1*(rn + (1- done)*self.\u03b3*self.Q2[sn].max() - self.Q1[s,a])\n        else:    self.Q2[s,a] += self.\u03b1*(rn + (1- done)*self.\u03b3*self.Q1[sn].max() - self.Q2[s,a])\n</code></pre>"},{"location":"unit3/lesson8/lesson8.html#comparing-sarsa-expected-sarsa-q-learning-and-double-q-learning","title":"Comparing Sarsa, Expected Sarsa, Q-learning and Double Q-learning","text":"<p>Ok now we can compare all 4 algorithms on the different environments to see their performances. </p>"},{"location":"unit3/lesson8/lesson8.html#comparison-on-cliff-walking","title":"Comparison on cliff walking","text":"<pre><code>def XSarsaDQlearnCliff(runs=300, \u03b1=.5):\n    plt.gca().spines['right'].set_visible(False)\n    plt.gca().spines['top'].set_visible(False)    \n    plt.yticks([-100, -75, -50, -25])\n    plt.ylim(-100, -10)\n    env = cliffwalk()\n\n    XSarsaCliff = Runs(algorithm=XSarsa(env=env, \u03b1=\u03b1, episodes=500), runs=runs, plotR=True).interact(label='XSarsa')\n    DQlearnCliff = Runs(algorithm=DQlearn(env=env, \u03b1=\u03b1, episodes=500), runs=runs, plotR=True).interact(label='Double Q-learning')\n\n    return XSarsaCliff, DQlearnCliff\n</code></pre> <pre><code>SarsaCliff.plot(label='Sarsa', frmt='-')\nQlearnCliff.plot(label='Q-learning', frmt='-')\nXSarsaCliff, DQlearnCliff = XSarsaDQlearnCliff()\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|300/300\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|300/300\n</code></pre>"},{"location":"unit3/lesson8/lesson8.html#comparison-on-the-maze","title":"Comparison on the Maze","text":"<pre><code>def compareonMaze(runs=100, \u03b1=.5):\n\n    plt.gca().spines['right'].set_visible(False)\n    plt.gca().spines['top'].set_visible(False)\n\n    env=Grid(gridsize=[10,20], style='maze', s0=80, reward='reward1') # this is bit bigger than the defualt maze\n    env.render()\n\n    SarsaMaze = Runs(algorithm=Sarsa(env=env, \u03b1=\u03b1, episodes=30), runs=runs, plotT=True).interact(label='Sarsa')\n    XSarsaMaze = Runs(algorithm=XSarsa(env=env, \u03b1=\u03b1, episodes=30), runs=runs, plotT=True).interact(label='XSarsa')\n\n    QlearnMaze = Runs(algorithm=Qlearn(env=env, \u03b1=\u03b1, episodes=30), runs=runs, plotT=True).interact(label='Q-learning')\n    DQlearnMaze = Runs(algorithm=DQlearn(env=env, \u03b1=\u03b1, episodes=30), runs=runs, plotT=True).interact(label='Double Q-learning')\n\n    return SarsaMaze, XSarsaMaze, QlearnMaze, DQlearnMaze\n</code></pre> <pre><code>SarsaMaze, XSarsaMaze, QlearnMaze, DQlearnMaze = compareonMaze(\u03b1=.5)\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n</code></pre> <pre><code>SarsaMaze, XSarsaMaze, QlearnMaze, DQlearnMaze = compareonMaze(\u03b1=.7)\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n</code></pre>"},{"location":"unit3/lesson8/lesson8.html#actor-critic-td-for-policy-gradient-methods","title":"Actor-Critic: TD for Policy Gradient Methods","text":"<p>Earlier, we saw how REINFORCE could perform well in the grid environment. REINFORCE is a policy gradient method that attempts to directly estimate a policy instead of estimating an action-value function. This is done by using the value function as an objective function that we would want to maximise (instead of minimising an error function as in Sarsa or Q-learning).</p> <p>Like Monte Carlo, REINFORCE is an offline method that needs to wait until the end of an episode to estimate the value function. The question, then, is there an algorithm similar to REINFORCE but online? The method should be derived similarly to Sarsa and Q-learning, which depends on the next step estimate of the value function. The answer is yes, and the method is called Actor-critic, which does that exactly. The algorithm general unified update attempts to estimate its policy by directly maximising the returns with respect to a baseline (see section 13.4). When the algorithm replaces its returns with an estimate of the returns (section 13.5, the difference between the return estimate and the baseline becomes a TD error), the algorithm can be thought of as having two distinctive parts an actor and a critic. The actor maximises its start-state-value function, while the critic attempts to improve its estimates of the state-value function for all states. Both of them use the Temporal Difference (TD) error to improve their estimates, meaning they can work online. Like REINFORCE, the actor-critic uses a SoftMax policy to select an action according to the actor policy parameters. So, to maximise the value, the actor takes the derivative of the $ \\nabla \\log v(S_0)$. </p> <p>Actor-critic is one of the oldest RL algorithms, and it avoids several issues that arise from the use of \\(\\epsilon\\)-greedy policy. The most obvious one is that the policy changes the probability of selecting an action gradually and continuously when the parameters change, unlike \\(\\epsilon\\)-greedy, which can change the maximum value action abruptly due to a small change in the parameters. This also allows it to provide better convergence guarantees.</p> <pre><code>class Actor_Critic(PG()):\n\n    def step0(self):\n        self.\u03b3t = 1 # powers of \u03b3, must be reset at the start of each episode\n\n    def online(self, s, rn,sn, done, a,an): \n        \u03c0, \u03b3, \u03b3t, \u03b1, \u03c4, t = self.\u03c0, self.\u03b3, self.\u03b3t, self.\u03b1, self.\u03c4, self.t\n        \u03b4 = (1- done)*\u03b3*self.V[sn] + rn - self.V[s]  # TD error is based on the critic estimate\n\n        self.V[s]   += \u03b1*\u03b4                          # critic\n        self.Q[s,a] += \u03b1*\u03b4*(1- \u03c0(s,a))*\u03b3t/\u03c4         # actor\n        self.\u03b3t *= \u03b3\n</code></pre>"},{"location":"unit3/lesson8/lesson8.html#delayed-reward","title":"Delayed Reward","text":"<p>First let us establish the baseline performance</p> <pre><code>ac = Actor_Critic(env=grid(), \u03b1=1, \u03c4=1, \u03b3=1, episodes=100, seed=0 , **demoQ()).interact()\n</code></pre> <p></p> <p>Note that we set \u03b1=1 which is unusual for an RL algorithm and the method just worked. This is a testimony to the resilience and strength of actor-critic methods. Let us test this further.</p> <pre><code>ac = Actor_Critic(env=grid(), \u03b1=1, \u03c4=.3, \u03b3=1, episodes=100, seed=0 , **demoQ()).interact()\n</code></pre> <p></p> <p>Note how reducing the exploration factor \\(\\tau=.3\\) led to a much faster convergence.</p> <p>Let us now reduce the learning rate instead.</p> <pre><code>ac = Actor_Critic(env=grid(), \u03b1=.1, \u03c4=1, \u03b3=1, episodes=500, seed=0, **demoQ()).interact()\n</code></pre> <p></p> <p>Note how we had to increase the number of episodes to converge when we set \\(\\alpha=.1\\) instead of \\(\\alpha=1\\).</p> <pre><code>ac = Actor_Critic(env=grid(), \u03b1=.1, \u03c4=.1, \u03b3=1, episodes=100, seed=0, **demoQ()).interact()\n</code></pre> <p></p> <p>Note how reducing both \\(\\tau\\) and \\(\\alpha\\) helped reach convergence quickly but with a better exploration.</p> <p>Let us now reduce the discount factor \\(\\gamma\\)</p> <pre><code>ac = Actor_Critic(env=grid(), \u03b1=1, \u03c4=1, \u03b3=.99, episodes=100, seed=0, **demoQ()).interact()\n</code></pre> <p></p> <pre><code>ac = Actor_Critic(env=grid(), \u03b1=1, \u03c4=.97, \u03b3=.99, episodes=100, seed=0, **demoQ()).interact()\n</code></pre> <p></p> <pre><code>ac = Actor_Critic(env=grid(), \u03b1=.3, \u03c4=1, \u03b3=.99, episodes=100, seed=0, **demoQ()).interact()\n</code></pre> <p></p> <pre><code>ac = Actor_Critic(env=grid(), \u03b1=.8, \u03c4=.9, \u03b3=.99, episodes=100, seed=0, **demoQ()).interact()\n</code></pre> <p></p> <p>print(ac.Q)</p>"},{"location":"unit3/lesson8/lesson8.html#intermediate-reward","title":"Intermediate Reward","text":"<pre><code>ac = Actor_Critic(env=grid(reward='reward0'), \u03b1=1, \u03c4=1, \u03b3=1, episodes=100, seed=0 , **demoQ()).interact()\n</code></pre> <pre><code>ac = Actor_Critic(env=grid(reward='reward0'), \u03b1=1, \u03c4=.9, \u03b3=1, episodes=100, seed=0 , **demoQ()).interact()\n</code></pre> <pre><code>ac = Actor_Critic(env=grid(reward='reward0'), \u03b1=.7, \u03c4=1, \u03b3=.98, episodes=100, seed=0 , **demoQ()).interact()\n</code></pre> <pre><code>ac = Actor_Critic(env=grid(reward='reward0'), \u03b1=.1, \u03c4=1, \u03b3=1, episodes=100, seed=0 , **demoQ()).interact()\n</code></pre> <pre><code>ac = Actor_Critic(env=grid(reward='reward0'), \u03b1=.1, \u03c4=.3, \u03b3=1, episodes=100, seed=0 , **demoQ()).interact()\n</code></pre> <pre><code>ac = Actor_Critic(env=grid(reward='reward0'), \u03b1=.1, \u03c4=.3, \u03b3=.99, episodes=100, seed=0 , **demoQ()).interact()\n</code></pre> <pre><code>ac = Actor_Critic(env=maze(reward='reward0'), \u03b1=.1, \u03c4=1,  \u03b3=1, episodes=100, seed=0 , **demoQ()).interact()\n</code></pre> <pre><code>ac = Actor_Critic(env=maze(reward='reward0'), \u03b1=.1, \u03c4=1,  \u03b3=1, episodes=100, seed=0 , **demoQ()).interact()\n</code></pre> <pre><code>ac = Actor_Critic(env=maze(), \u03b1=.1, \u03c4=1,  \u03b3=1,episodes=100, seed=0 , **demoQ()).interact()\n</code></pre> <pre><code>ac_large = Actor_Critic(env=maze_large(), \u03b1=.1, \u03c4=1,  \u03b3=1,episodes=500, seed=0 , **demoQ()).interact()\n</code></pre> <pre><code>ac_large = Actor_Critic(env=maze_large(), \u03b1=.1, \u03c4=.3, \u03b3=1, episodes=500, seed=0 , **demoQ()).interact()\n</code></pre>"},{"location":"unit3/lesson8/lesson8.html#model-selection-methods-comparisons-class","title":"Model selection: methods comparisons class","text":"<p>Ok, the question is, which one of these algorithms would perform best regardless of the learning rate \u03b1? To be able to know, we would need to compare the performances on a set of \u03b1 values to see the full picture. To that end, we will finally develop a useful comparison class. It will allow us to compare algorithms with different hyperparameters similar to what we did in other machine learning modules. All that is required is to specify which hyperparameter we want to vary and then pass the values we want to test for in a dictionary.</p> <pre><code>import time\n\nclass Compare:\n\n    def __init__(self, \n                 algoruns=None,\n                 hyper={'\u03b1':np.round(np.arange(.1,1,.2),1)},\n                 plotR=False, \n                 plotT=False,\n                 plotE=False,\n                 print=False, \n                 **kw):\n\n        self.algoruns = algoruns if algoruns is not None else Runs(**kw)\n        self.hyper = hyper\n        self.plotR = plotR\n        self.plotT = plotT\n        self.plotE = plotE\n        self.print = print\n\n    def isFunction(self, hyperval): \n        return isinstance(hyperval, str) # not in ['\u03b1','\u03b3','\u03b5','\u03bb']\n\n    def compare(self, label='',frmt='-', **kw):\n\n        algoruns = self.algoruns\n        algorithm  = self.algoruns.algorithm\n        runs, episodes = algoruns.runs, algorithm.episodes\n\n        hypername = list(self.hyper.keys())[0]\n        hypervals = list(self.hyper.values())[0]\n        nhypervals = len(hypervals)\n\n        self.Rs = np.zeros((nhypervals, runs, episodes))\n        self.Ts = np.zeros((nhypervals, runs, episodes))\n        self.Es = np.zeros((nhypervals, runs, episodes))\n\n        # now call the algorithm for each parameters set\n        if self.print: print(algoruns.header())\n        start = time.time()\n        for h, hyperval in enumerate(hypervals):\n\n            if self.isFunction(hyperval):  \n                  label_ =   '%s'% hyperval; hyperval = getattr(algorithm, hyperval)\n            else: label_ = '%.4f'% hyperval\n            setattr(algorithm, hypername, hyperval)\n\n            algoruns.interact(label= '%s %s=%s'%(label, hypername, label_), **kw)\n\n            self.Rs[h] = algoruns.Rs\n            self.Ts[h] = algoruns.Ts\n            self.Es[h] = algoruns.Es\n\n            # take the mean over the trials\n            results = (algoruns.Rs.mean(), algoruns.Ts.mean(), algoruns.Es.mean())\n            if self.print: print(algoruns.values(results))\n            # for the progress bar we use a differernt color for compare\n            self.comparetime = progress(h, len(hypervals), start, color=2)\n\n        if self.print: print('comparison time = %.2f'% self.comparetime,'\\n')\n        if self.plotR or self.plotT or self.plotE: self.plot(label=label, frmt=frmt)\n\n        return self\n\n    def plot(self, label, frmt):\n        hypername = list(self.hyper.keys())[0]#'\u03b1'\n        hypervals = list(self.hyper.values())[0]\n        ishyperNum= not self.isFunction(hypervals[0])\n        hyperrng  = hypervals if ishyperNum else list(range(len(hypervals)))\n        # [.1, .2, .3, .4...,1], ['reward0', 'reward1', 'reward10']\n\n        compareGT = [self.plotR, self.plotT, self.plotE ]\n        labels    = ['Rewards', 'Steps', 'Errors']\n        cs        = ['r', 'b', 'g']\n\n        plt.gca().spines['right'].set_visible(False)\n        plt.gca().spines['top'].set_visible(False)\n\n        HyperMeansGT = [self.Rs.mean(axis=(1,2)), self.Ts.mean(axis=(1,2)), self.Es.mean(axis=(1,2))]\n        for h, HyperMeans in enumerate(HyperMeansGT):\n\n            # plot if it is required\n            if not compareGT[h]: continue\n\n            if label: plt.plot(hyperrng, HyperMeans, frmt, label=label)\n            else:     plt.plot(hyperrng, HyperMeans, cs[h]+'--', label=labels[h])\n            plt.xlabel(hypername, fontsize=14)\n            plt.legend()\n\n            # need to annotate if the hyper parameters are policy or rewards etc\n            if ishyperNum: continue  \n            bottom, top = plt.ylim()\n            for i, hval in enumerate(HyperMeans):\n                anot = str(hypervals[i]) +', %s'%hval\n                plt.annotate(anot, xy=(i,hval+.1))\n\n\n        return self\n</code></pre> <p>Now we can compare different \u03b1 values to specify which algorithm is dominant. This study can be seen in Figure 6.3 in the book. Here we do 10 runs because it takes longer to do more, but you are welcome to try to run it for 100 runs. Note that the asymptotic study will run for 1000. the idea here is to compare the performances of the above control algorithms and variants of Q-learning and Sarsa in a systematic manner. The domain is the cliff walking environment. We want to see which algorithms (Sarsa, expected Sarsa, Q-learning, double Q-learning) perform best regardless of the learning rate. Such comparison would give us a definitive answer on which algorithm is best for the given problem when we see a pattern of dominance for all learning rate values.</p> <pre><code>def figure_6_3(runs=10, Interim=True, Asymptotic=True, episodes=100,  label=''): #100\n    #plt.ylim(-150, -10)\n    plt.xlim(.1,1)\n    plt.title('Interim and Asymptotic performance')\n    \u03b1s = np.arange(.1,1.05,.05)\n\n\n    algors = [ XSarsa,   Sarsa,   Qlearn]#,      DQlearn]\n    labels = ['XSarsa', 'Sarsa', 'Qlearning']#, 'Double Q learning']\n    frmts  = ['x',      '^',     's']#,         'd']\n\n    env = cliffwalk()\n    Interim_, Asymptotic_ = [], []\n    # Interim perfromance......\n    if Interim:\n        for g, algo in enumerate(algors):\n            compare = Compare(algorithm=algo(env=env, episodes=episodes), runs=runs, hyper={'\u03b1':\u03b1s},\n                             plotR=True).compare(label=labels[g]+' Interim'+label, frmt=frmts[g]+'--')\n            Interim_.append(compare)\n\n    # Asymptotic perfromance......\n    if Asymptotic:\n        for g, algo in enumerate(algors):\n            compare = Compare(algorithm=algo(env=env, episodes=episodes*10), runs=runs, hyper={'\u03b1':\u03b1s}, \n                             plotR=True).compare(label=labels[g]+' Asymptotic'+label, frmt=frmts[g]+'-')\n            Asymptotic_.append(compare)\n\n    plt.gcf().set_size_inches(10, 7)\n    return Interim_, Asymptotic_\n\nInterim_, Asymptotic_ = figure_6_3()\n</code></pre> <pre><code>100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|19/19\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|19/19\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|19/19\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|19/19\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|19/19\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|19/19\n</code></pre> <p></p> <p>As we can see the expected Sarsa performed best in the interim and on the asymptote.</p>"},{"location":"unit3/lesson8/lesson8.html#conclusion","title":"Conclusion","text":"<p>In this lesson, we have further developed our understanding of important and prominent RL online algorithms that are widely used, all based on the value iteration idea. I.e., we keep improving our policy and refining our value-function iteratively in each step until convergence. All of our algorithms are based on the Temporal Difference method. TD uses bootstrapping in its update; instead of using a true return of a state, it uses the current reward + its own estimation of the return for the next state. It is quite surprising to see how well TD works in practice. TD has been proven to converge to a good solution under some basic conditions regarding the learning rate. In practice, however, we assign a fixed small learning rate that works just fine. It is desirable that the learning rate is not decayed when the environment\u2019s dynamics are expected to change. We have further used TD update in a few control algorithms. Most notable are the Sarsa and Q-learning. The first is an on-policy, while the latter is an off-policy control algorithm. We have compared all algorithms on different problems, studied their strengths and weaknesses, and how they are expected to behave on a certain problem.</p>"},{"location":"unit3/lesson8/lesson8.html#your-turn","title":"Your turn","text":"<ol> <li>Create a class that implements an offline TD algorithm. Take inspiration from the MC class.</li> <li>Change the policy in the XSarsa to softmax, you would need to add the probability function in the MDP that deals with the softmax and then </li> <li>apply Sarsa and Q-learning on a 8 action maze environment by assigning the env=maze8() and study the differences in the way the agent changes its policy and it behaviour.</li> </ol>"},{"location":"unit3/lesson8/lesson8.html#challenge","title":"Challenge","text":"<p>You can challenge yourself by trying to implement a dynamic environment that changes its obstacles at a specific episode</p>"},{"location":"unit3/lesson9/lesson9.html","title":"9. n-Step Methods","text":"<p>The notebook uses a library of functionality in RL that aims for simplicity and general insight into how algorithms work, these libraries are written from scratch using standard Python libraries (numpy, matplotlib etc.). Please note that you will need permission from the author to use the code for research, commercially or otherwise.</p>"},{"location":"unit3/lesson9/lesson9.html#lesson-8-tabular-methods-n-steps-bootstrapping-tabular-methods","title":"Lesson 8-Tabular Methods: n-steps Bootstrapping Tabular Methods","text":"<p>Learning outcomes 1. understand how to generalize a one-step methods to n-step methods 2. understand the trend associated with n-steps methods  3. understand that intermediate n values usually works the best  4. generalise n-step prediction methods to n-step control methods</p> <p>Reading: The accompanying reading of this lesson is chapter 7 of our text book available online here. Please note that we explain the ideas of this topic from a practical perspective and not from a theoretical perspective which is already covered in the textbook.</p> <p>In the previous lesson, we saw how a TD update rule defined in terms of the next reward and state as the target can effectively converge to useful state and action-value functions. It took the form:</p> <p>\\(V(S_{t}) = V(S_{t}) + \\alpha[R_{t+1} + \\gamma V(S_{t+1}) - V(S_{t}) ]\\)</p> <p>This is a one-step update because one reward available in the current step is used. It can be written as</p> <p>\\(G_{t} = R_{t+1} + \\gamma V(S_{t+1})\\)</p> <p>\\(V(S_{t}) = V(S_{t}) + \\alpha[ G_{t} - V(S_{t}) ]\\)</p> <p>where \\(G_{t}\\) is a one-step return.</p> <p>In this lesson, we study the effect of increasing the number of steps considered for the target. In particular, we study the effect of collecting rewards for n steps and substituting the one-step return with the n-step return written as the discounted sum of the n rewards plus the value function for the n-th step state.</p> <p>In other words, we define the n-step return as</p> <p>\\(G_{t:t+n} = R_{t+1} + \\gamma R_{t+2} + \\gamma^{2} R_{t+3} +...+ \\gamma^{n-1} R_{t+n} + \\gamma^{n}V(S_{t+n})\\)</p> <p>And the update rule that uses this n-step return is:</p> <p>\\(V(S_{t}) = V(S_{t}) + \\alpha[G_{t:t+n} - V(S_{t}) ]\\)</p> <p>When we want to make in which time step, we are conducting this update, we add a subscript for the V function that represents the time step at which the estimate is referring to </p> <p>\\(G_{t:t+n} = R_{t+1} + \\gamma R_{t+2} + \\gamma^{2} R_{t+3} +...+ \\gamma^{n-1} R_{t+n} + \\gamma^{n}V_{t+n-1}(S_{t+n})\\)</p> <p>\\(V_{t+n}(S_{t}) = V_{t+n-1}(S_{t}) + \\alpha[G_{t:t+n} - V_{t+n-1}(S_{t}) ]\\)</p> <pre><code>%matplotlib inline\n</code></pre> <pre><code>from rl.rl import *\n</code></pre>"},{"location":"unit3/lesson9/lesson9.html#adjusting-an-mrp-class","title":"Adjusting an MRP class","text":"<p>First, let us develop our MRP class to accommodate waiting for n-1 steps before obtaining the \\(G_{t:t+n}\\). In each step, we must also create a G function to obtain the \\(G_{t:t+n}\\). Finally, we would need to alter our stopping criteria to wait for extra n-1 steps at the end to ensure we update the latest n-1 state values since we are always lagging n-1 steps during the episode.</p> <pre><code>class MRP(MRP):\n\n    def __init__(self, n=1, **kw):\n        super().__init__(**kw)\n        self.n = n\n\n    #----------------------------------- \ud83d\udc3esteps as per the algorithm style --------------------------\n    def stop_ep(self, done):\n        return self.stop_(done) or (self.t+1 &gt;= self.max_t-1 and self.store)\n\n    def stop_(self,done):\n        if done:\n            self.skipstep +=1                     # holds the count for how many steps after ep is finished (will reach n-1)\n            if self.skipstep == self.n: \n                self.t = self.t+1 - self.skipstep # returns t to its original actual count of number of steps.  it is executed before self.t+=1 in interact and hence +1 is necessary\n                self.skipstep = 0\n                return True\n            return False\n\n        self.skipstep = 0\n        return False\n    #-----------------------------------\ud83d\udcb0 returns --------------------------------------------------\n    def G(self, \u03c41, \u03c4n):    # n-steps return, called during an episode   \n\n        #if self.\u03b3==1: return self.r[\u03c41:\u03c4n+1].sum() # this saves computation when no dsicount is applied\n        Gn = 0\n        for t in range(\u03c4n, \u03c41-1, -1): # yields \u03c4n-\u03c41= (\u03c4+n)-(\u03c4+1)= n-1 setps\n            Gn = self.\u03b3*Gn + self.r[t] \n\n        return Gn \n</code></pre>"},{"location":"unit3/lesson9/lesson9.html#online-tdn","title":"Online TDn","text":"<p>Now we write our TDn class. It is very similar to TD except that we replace the rn+V[t+1] by the \\(G_{t:t+n}\\). Also as long as the agent did do enough steps (n-1) we skip without doing the update.</p> <pre><code>class TDn(MRP):\n\n    def init(self):\n        self.store = True # there is a way to save storage by using t%(self.n+1) but we left it for clarity\n\n\n    # ----------------------------- \ud83c\udf16 online learning ----------------------    \n    def online(self,*args):\n        \u03c4 = self.t - (self.n-1);  n = self.n\n        if \u03c4&lt;0: return\n\n        # we take the min so that we do not exceed the episode limit (last step+1)\n        \u03c4n = \u03c4+n ; \u03c4n = min(\u03c4n, self.t+1 - self.skipstep)\n        \u03c41 = \u03c4+1\n\n        s\u03c4 = self.s[\u03c4 ]\n        sn = self.s[\u03c4n]\n        done = self.done[\u03c4n]\n\n        # n steps \u03c4+1,..., \u03c4+n inclusive of both ends\n        self.V[s\u03c4] += self.\u03b1*(self.G(\u03c41,\u03c4n) + (1- done)*self.\u03b3**n *self.V[sn] - self.V[s\u03c4])\n</code></pre> <p>Let us now apply TDn on our simple 5-steps random walk problem.</p> <pre><code>np.random.seed(0)\nTDnwalk = TDn(env=randwalk(), v0=0, \u03b1=.05, n=4, episodes=100, **demoV()).interact()\n</code></pre> <p></p> <pre><code>np.random.seed(0)\nTDnwalk = TDn(env=randwalk(), v0=0, \u03b1=.05, n=1, episodes=100, **demoV()).interact()\n</code></pre> <p></p> <pre><code>TDnwalk.t\n</code></pre> <pre><code>18\n</code></pre> <pre><code>TDnwalk.Ts\n</code></pre> <pre><code>array([ 3,  7,  9, 15, 11,  5,  3,  5, 13, 27,  9,  3, 11,  3, 11,  7,  7,\n        7, 13,  3,  7,  5, 15,  7,  5, 13,  3, 23, 13,  9,  3,  7,  9, 17,\n       13, 17,  9, 13,  9,  3,  3, 29,  3, 15,  7,  3,  3, 19,  9,  3, 15,\n        3,  3,  3, 19,  3, 15,  3,  3,  9,  5,  3,  3,  7,  3,  3,  3,  5,\n        3, 11,  7, 11,  3,  5,  9,  5,  3, 19,  5,  9,  5,  3,  5,  3,  5,\n        3,  3,  9, 11,  9,  7,  5,  9,  7,  9,  3,  3, 15,  7, 19],\n      dtype=uint32)\n</code></pre>"},{"location":"unit3/lesson9/lesson9.html#tdn-and-mc-runs-on-random-walk","title":"TDn and MC Runs on random walk","text":"<p>Let us now  see how a TDn for n=1 and n=5 as well as MC behaves on our usual 5-states random walk on average. To that end as usual we execute several runs.</p> <p>from MC import MC</p> <pre><code>def nstepTD_MC_randwalk(env=randwalk(), algorithm=TDn, alglabel='TD'):\n    plt.xlim(0, 100)\n    plt.ylim(0, .25)\n    plt.title('Empirical RMS error, averaged over states')\n    n=5\n\n    for \u03b1 in [.05, .1, .15]:\n        TD\u03b1s = Runs(algorithm=algorithm(env=env, n=1,\u03b1=\u03b1, v0=.5),  runs=100, plotE=True).interact(label='%s \u03b1= %.2f'%(alglabel,\u03b1), frmt='.-')\n\n    for \u03b1 in [.05, .1, .15]:\n        TD\u03b1s = Runs(algorithm=algorithm(env=env,n=n,\u03b1=\u03b1, v0=.5),  runs=100, plotE=True).interact(label= '%s \u03b1= %.2f n=%d'%(alglabel,\u03b1,n), frmt='-')\n\n    for \u03b1 in [.01, .02, .03, .04]:\n        MCs = Runs(algorithm=MC(env=env,\u03b1=\u03b1, v0=.5),  runs=100, plotE=True).interact(label='MC \u03b1= %.2f'%\u03b1, frmt='--')\n</code></pre> <p>Note that 5 states random walk environment env=randwalk() is the default environment for MRPs and s we did not need to explicitly pass it in the above.</p> <pre><code>nstepTD_MC_randwalk()\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n</code></pre> <p></p>"},{"location":"unit3/lesson9/lesson9.html#comparison-of-online-n-step-td-and-mc-for-different","title":"Comparison of online n-step TD and MC for different \u03b1","text":"<p>Ok, let us now study the effect of varying the hyperparameter n. n blends the horizon of all methods between bootstrapping algorithms and non-bootstrapping methods. To that end, we will apply TDn with different n values along with MC on a random walk prediction problem. This time we will use 19 states, and the goal to the left has a reward of -1, the goal on the right has a reward of 1, and all of the 19 intermediate states have a reward of 0.</p> <pre><code>def nstepTD_MC_randwalk_\u03b1compare(env=randwalk_(), algorithm=TDn, Vstar=None, runs=10, envlabel='19', \n                                 MCshow=True, alglabel='online TD'):\n\n    steps0 = list(np.arange(.001,.01,.001))\n    steps1 = list(np.arange(.011,.2,.025))\n    steps2 = list(np.arange(.25,1.,.05))\n\n    \u03b1s = np.round(steps0 +steps1 + steps2, 2)\n    #\u03b1s = np.arange(0,1.05,.1) # quick testing\n\n    plt.xlim(-.02, 1)\n    plt.ylim(.24, .56)\n    plt.title('n-steps %s RMS error averaged over %s states and first 10 episodes'%(alglabel,envlabel))\n    for n in [2**_ for _ in range(10)]:\n        Compare(algorithm=algorithm(env=env, v0=0, n=n, episodes=10, Vstar=Vstar), \n                              runs=runs, \n                              hyper={'\u03b1':\u03b1s}, \n                              plotE=True).compare(label='n=%d'%n)\n    if MCshow:\n        compare = Compare(algorithm=MC(env=env, v0=0, episodes=10), \n                                  runs=runs, \n                                  hyper={'\u03b1':\u03b1s}, \n                                  plotE=True).compare(label='MC \u2261 TDn(n=$\\\\infty$)', frmt='-.')\n</code></pre> <pre><code>figure_7_2 = nstepTD_MC_randwalk_\u03b1compare\nfigure_7_2()\n</code></pre> <pre><code>100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n</code></pre> <p></p> <p>Note how when n=\\(\\infty\\) TDn converges to an MC. Hence, we could see that TDn represents the full spectrum of algorithms that completely use bootstrapping (TD) and algorithms that do not use bootstrapping (MC). It nicely blends these algorithms and lets us control bootstrapping parametrically by choosing a suitable value for the n hyperparameter.</p>"},{"location":"unit3/lesson9/lesson9.html#offline-tdn","title":"Offline TDn","text":"<p>Let us now develop an offline TDn method. This method is exactly as its name suggests. We need to be mindful of going n-1 extra steps at the end because of the lag of n-1 steps at the start until the agent accumulates enough steps to obtain the \\(G_{t:t+n}\\).</p> <pre><code>class TDnf(MRP):\n\n    def init(self):\n        self.store = True # must store because it is offline\n\n    # ----------------------------- \ud83c\udf18 offline TD learning ----------------------------   \n    def offline(self):\n        n=self.n        \n        for t in range(self.t+n): # T+n to reach T+n-1\n            \u03c4  = t - (n-1)\n            if \u03c4&lt;0: continue\n\n            # we take the min so that we do not exceed the episode limit (last step+1)\n            \u03c41 = \u03c4+1\n            \u03c4n = \u03c4+n ; \u03c4n=min(\u03c4n, self.t+1)\n\n            s\u03c4 = self.s[\u03c4 ]\n            sn = self.s[\u03c4n]\n            done = self.done[\u03c4n]\n\n            # n steps \u03c4+1,..., \u03c4+n inclusive of both ends\n            self.V[s\u03c4] += self.\u03b1*(self.G(\u03c41,\u03c4n)+ (1- done)*self.\u03b3**n *self.V[sn] - self.V[s\u03c4])\n</code></pre> <pre><code>TDnwalk = TDnf(env=randwalk(), v0=0, \u03b1=.1, n=4, episodes=100, seed=0, **demoV()).interact()\n</code></pre> <p></p> <pre><code>TDnwalk.t\n</code></pre> <pre><code>18\n</code></pre> <pre><code>TDnwalk.Ts\n</code></pre> <pre><code>array([ 3,  7,  9, 15, 11,  5,  3,  5, 13, 27,  9,  3, 11,  3, 11,  7,  7,\n        7, 13,  3,  7,  5, 15,  7,  5, 13,  3, 23, 13,  9,  3,  7,  9, 17,\n       13, 17,  9, 13,  9,  3,  3, 29,  3, 15,  7,  3,  3, 19,  9,  3, 15,\n        3,  3,  3, 19,  3, 15,  3,  3,  9,  5,  3,  3,  7,  3,  3,  3,  5,\n        3, 11,  7, 11,  3,  5,  9,  5,  3, 19,  5,  9,  5,  3,  5,  3,  5,\n        3,  3,  9, 11,  9,  7,  5,  9,  7,  9,  3,  3, 15,  7, 19],\n      dtype=uint32)\n</code></pre> <pre><code>TDnwalk = TDnf(env=randwalk(), v0=0, \u03b1=.1, n=4, episodes=100, seed=0, **demoV()).interact()\n</code></pre> <p></p> <pre><code>TDnwalk.t\n</code></pre> <pre><code>18\n</code></pre> <pre><code>TDnwalk.Ts\n</code></pre> <pre><code>array([ 3,  7,  9, 15, 11,  5,  3,  5, 13, 27,  9,  3, 11,  3, 11,  7,  7,\n        7, 13,  3,  7,  5, 15,  7,  5, 13,  3, 23, 13,  9,  3,  7,  9, 17,\n       13, 17,  9, 13,  9,  3,  3, 29,  3, 15,  7,  3,  3, 19,  9,  3, 15,\n        3,  3,  3, 19,  3, 15,  3,  3,  9,  5,  3,  3,  7,  3,  3,  3,  5,\n        3, 11,  7, 11,  3,  5,  9,  5,  3, 19,  5,  9,  5,  3,  5,  3,  5,\n        3,  3,  9, 11,  9,  7,  5,  9,  7,  9,  3,  3, 15,  7, 19],\n      dtype=uint32)\n</code></pre>"},{"location":"unit3/lesson9/lesson9.html#tdf-and-tdnf-for-n1","title":"TDf and TDnf for n=1","text":"<pre><code>TDnwalk = TDf(env=randwalk_(), v0=0, \u03b1=.8, episodes=10, seed=0, **demoV()).interact()\n</code></pre> <pre><code>TDnwalk = TDnf(env=randwalk_(), v0=0, \u03b1=.8, n=1, episodes=10, seed=0, **demoV()).interact()\n</code></pre>"},{"location":"unit3/lesson9/lesson9.html#tdf-tdnf-and-mc-runs-on-random-walk","title":"TDf , TDnf and MC Runs on random walk","text":"<p>Let us now  see how a TDnf for n=1 and n=5 as well as MC behaves on our usual 5-states random walk on average. To that end as usual we execute several runs.</p> <pre><code>nstepTD_MC_randwalk(algorithm=TDnf, alglabel='TDf')\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n</code></pre> <p></p> <p>Let us now double check that both TDnf and TDf are identical for n=1.</p> <pre><code>from tqdm import trange, tqdm\n</code></pre> <pre><code>\u03b1s = np.arange(0,1.05,.1)\nn=1\ncompareTDf = Compare(algorithm=TDf(env=randwalk_(), v0=0, episodes=10), runs=2, hyper={'\u03b1':\u03b1s}, \n                     plotE=True).compare(label='TD offline')\n\ncompareTDnf = Compare(algorithm=TDnf(env=randwalk_(), v0=0, n=n, episodes=10), runs=2, hyper={'\u03b1':\u03b1s}, \n                      plotE=True).compare(label='TDn offline n=%d'%n)\n</code></pre> <pre><code>100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|11/11\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|11/11\n</code></pre> <p></p>"},{"location":"unit3/lesson9/lesson9.html#offline-tdnf-comparison","title":"Offline TDnf \u03b1 comparison","text":"<p>Let us now compare how offline n-step TD (TDnf) performs with different values for \u03b1 (learning-step hyper parameter.</p> <pre><code>nstepTD_MC_randwalk_\u03b1compare(algorithm=TDnf, alglabel='offline TD')\n</code></pre> <pre><code>100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n</code></pre> <p></p>"},{"location":"unit3/lesson9/lesson9.html#n-step-sarsa-on-policy-control","title":"n-step Sarsa on-policy Control","text":"<p>As you can see, we have imported the class factory MDP to make it inherit the new MRP class that we defined in this lesson (which contains functions to deal with multiple steps updates) without having to restate the definition of MDP again.</p> <pre><code>class Sarsan(MDP(MRP)):\n\n    def init(self):\n        self.store = True        # although online but we need to access *some* of earlier steps,\n        self.step = self.step_an # for Sarsa we want to decide the next action in time step t\n\n    # ----------------------------- \ud83c\udf16 online learning ----------------------    \n    def online(self,*args):\n        \u03c4 = self.t - (self.n-1);  n=self.n\n        if \u03c4&lt;0: return\n\n        # we take the min so that we do not exceed the episode limit (last step+1)\n        \u03c41 = \u03c4+1\n        \u03c4n = \u03c4+n ; \u03c4n=min(\u03c4n, self.t+1 - self.skipstep)\n\n        s\u03c4 = self.s[\u03c4];  a\u03c4 = self.a[\u03c4]\n        sn = self.s[\u03c4n]; an = self.a[\u03c4n]\n        done = self.done[\u03c4n]\n\n        # n steps \u03c4+1,..., \u03c4+n inclusive of both ends\n        self.Q[s\u03c4,a\u03c4] += self.\u03b1*(self.G(\u03c41,\u03c4n) + (1- done)*self.\u03b3**n *self.Q[sn,an] - self.Q[s\u03c4,a\u03c4])\n</code></pre> <pre><code>nsarsa = Sarsan(env=grid(), n=10, \u03b1=.4, episodes=50, seed=0,  **demoQ()).interact()\n</code></pre> <p></p> <pre><code>for seed in range(100):\n    nsarsa = Sarsan(env=grid(), \u03b1=.4, seed=seed, episodes=1).interact()\n    if len(nsarsa.env.trace) &lt;15: print(seed)\n</code></pre> <pre><code>16\n47\n</code></pre> <pre><code>def figure_7_4(n=5,seed=16): \n\n    # draw the path(trace) that the agent took to reach the goal\n    nsarsa = Sarsan(env=grid(), \u03b1=.4, seed=seed, episodes=1).interact()\n    nsarsa.env.render(underhood='trace', subplot=131, animate=False, label='path of agent')\n\n    # now draw the effect of learning to estimate the Q action-value function for n=1\n    nsarsa = Sarsan(env=grid(), \u03b1=.4, seed=seed, episodes=1, underhood='maxQ').interact() \n    nsarsa.render(subplot=132, animate=False, label='action-value increassed by 1-steps Sarsa\\n')\n\n    #n=5 # try 10\n    # now draw the effect of learning to estimate the Q action-value function for n=10\n    nsarsa = Sarsan(env=grid(), n=n, \u03b1=.4, seed=seed, episodes=1, underhood='maxQ').interact()    \n    nsarsa.render(subplot=133, animate=False, label='action-value increassed by %d-steps Sarsa\\n'%n)\n\nfigure_7_4(n=3)\n</code></pre> <p></p> <pre><code>sarsa = Sarsa(env=maze(reward='reward1'),  \u03b1=.2, episodes=100, seed=10, **demoQ()).interact()\n# sarsa = Sarsan(env=maze(reward='reward1'), n=1, \u03b1=.2, episodes=100, seed=10, **demoQ()).interact() # same as above\n</code></pre> <p></p> <pre><code>sarsa = Sarsan(env=maze(reward='reward1'), n=10,  \u03b1=.2, episodes=100, seed=10, **demoQ()).interact()\n</code></pre> <p></p> <p>Note how the agent reaches a better policy in less number of episodes, i.e. it converges faster when using n-step method. At the end it counted a extra n-1 steps to finish the set of n-1 updates needed at the end of the episode.</p>"},{"location":"unit3/lesson9/lesson9.html#n-step-q-learning","title":"n-step Q-learning","text":"<p>Can we implement an n-step Q-learning algorithm?</p> <p>If we think about this question superficially, from an implementation perspective, the answer seems simple. All we have to do is to replace the \\(\u03b3^n*Q[sn,an]\\) with \\(\u03b3^n*Q[sn].max()\\). However, this is not the correct way to implement the n-step off-policy Q-learning. </p> <p>Remember that Q-learning is an action-value learning method that uses TD learning off-policy, where we learn about a greedy policy while following a more exploratory policy, such as \u03b5-greedy. To be able to generalise the n-step TD update to an off-policy action-value method, we need to take into account the following. In each step of the n-steps that we are considering, we need our method to learn what the agent would have done if it had followed a \u03c0=greedy policy (instead of the b=\u03b5greedy). </p> <p>One way to compensate for this discrepancy between the behavioural policy b and the target policy \u03c0 would be to multiply each reward of the \\(G_{t:t+n}\\) with the importance sampling ratio. This ratio divides the probability of taking action a from policy \u03c0 by the probability of taking the same action according to policy b. Importance sampling methods suffer from high variance, and usually, they are not practical for control.</p> <p>Another approach is by using expectation instead of importance sampling. In this case, we use a similar idea to the expected Sarsa but in an off-policy context, where we alter the calculation of \\(G_{t:t+n}\\) in a way that sums over the different actions probabilities in each time step (except the first). This results in an algorithm called the Tree Backup Algorithm.</p> <p>Finally, we can blend the importance sampling with expectation using a hyperparameter \u03c3 which results in the Q(\u03c3) control algorithm. All of the above three approaches are outside the scope of our coverage, and there is a theory behind this choice. See section 7.3 of our textbook.</p>"},{"location":"unit3/lesson9/lesson9.html#conclusion","title":"Conclusion","text":"<p>In this lesson, we have covered the n-step TD algorithms for prediction and control. We have seen how the different values of n represent a trade-off between full bootstrapping, as in the one-step TD (n=1), and no bootstrapping, as in the Monte Carlo algorithm (n=T the number of steps in an episode). Intermediate n values give algorithms that lie within the two extremes of the spectrum of bootstrapping algorithms. We also find that the best value is usually an intermediate value &gt;1. As we have seen, creating such an algorithm is challenging and has its drawbacks in terms of implementation. In later lessons, we will see how to achieve similar results without waiting or counting steps. We will adopt a bootstrapping variation mechanism which turns the n-step countable mechanism into an infinite continuum of value by adopting the \\(\\lambda\\) hyperparameter that takes a real value instead of integers.</p>"},{"location":"unit3/lesson9/lesson9.html#your-turn","title":"Your turn","text":"<ol> <li>implement the incorrect n-step Q-learning and apply it on the maze problem and see how it performs.</li> <li>implement n-step Expected Sarsa and compare its performance with the n-step Sarsa.</li> </ol>"},{"location":"unit3/lesson9/lesson9.html#challenge","title":"Challenge","text":"<p>Try to implement the correct off-policy n-step Sarsa presented in page 149 of the textbook.</p>"},{"location":"unit4/lesson12/lesson12.html","title":"Lesson 12: Introduction to Function Approximation Methods","text":"<p>Unit 4: Learning Outcomes By the end of this unit, you will be able to: </p> <ol> <li>Apply RL techniques to control an agent in complex environment representations.  </li> <li>Compare the trade-offs of on-policy and off-policy learning algorithms.  </li> <li>Evaluate the convergence properties of RL algorithms in both tabular and function approximation settings, considering their practical limitations.  </li> </ol> <p>In this and subsequent lessons, we will learn about finding solutions for states represented by features (observations). In a real-world environment, it is hard and unrealistic to expect to be able to identify a state fully. Usually, we can only partially observe the state via a set of features that can help us to recognise or distinguish a state, but that does not mean that we have guarantees that the state is fully identifiable or that the features are unique for each different state. Nevertheless, we should be able to deal with these spaces. After all, RL is meant for real-world problems. We must come to terms with the issue that this partial observability can be dealt with effectively in most practical cases by using suitable features. Because the states that share inner properties usually tend to have similar features, by dealing with these states via these features, we usually succeed in generalising to other similar states. To that end, dealing with the features via a set of parameters is natural, as we did in several machine learning modules. Of course, we can bring along other non-parametric models, such as Gaussian processes. However, this falls outside the scope of our RL treatment.  In some cases, we might need to generalise the RL framework from the MDP\u2019s underlying assumption of full observability to Partially Observable Markov Decision Processes or POMDP. In most cases, this might be unnecessary, and we can get away without dealing with the intricacy of POMDP. Again, POMDP is outside the scope of our coverage. What is left is then to move from a tabular to a parametric representation by adjusting the update rules we have dealt with so far to the parametric representation instead of the tabular one. </p>"},{"location":"unit4/lesson12/lesson12.html#plan","title":"Plan","text":"<p>From a practical perspective, we start by generalising from a tabular to an equivalent vectorised form via a one-hot representation. In this representation, each vector component corresponds with a state in the state space, so the vector size is the same as the number of states, and we use a one-hot encoding. To encode (represent) a state, we turn on (set to 1) the corresponding component (all other features are 0\u2019s) and the update for the weights\u2019 parameters will be applied similar to what we did for the tabular. In fact, each component's weight is really the value function for the corresponding state. We should get results identical to those we obtained for the problems we tackled in the tabular form\u2014random walk, the maze, the cliff walking, etc. The benefit of the one-hot encoding is that it is a vectorised version of the tabular representation and constitutes the first step towards generalising to a linear parametric model. Once we are satisfied that our vectorised form is working, we can then come up with different representations for the problems that have a dimensionality different than the state space, usually smaller and more concise than a continuous state space dimension, which can be infinite (countable or uncountable) or intractable, which is one of the advantages of parametric models. </p> <p>What is left is to find a suitable representation of the problem at hand.  Consequently, we continue converging towards a more general representation by covering state aggregation. In state aggregation, we group a set of states and represent them all in one feature, and we use one-hot encoding again. This time, we turn a feature on whenever the agent is at any of the group of states corresponding to it.  We then move to other constructions, including coarse coding and tile coding, to deal with state representation. The book chapter covers these very well, and you are advised to read about them in section 9.5. Selecting a construction is usually a matter of trial and error as well as preference. We can use a model that helps us automatically find suitable features for the problem at hand. </p> <p>This is where neural networks can come to the rescue.  Neural networks can automatically find a suitable set of features internally through their hidden layers, as we know. Whether to use a deep neural network or a shallow one depends on the complexity of the problem and the complexity that we would want our state representation to have. Neural networks help extract helpful features in the early layers that will be used in later layers to extract a correct value function. In this case, we do not need to use tile or similar coding because the network automatically learns the feature representation. The main issue that we often face, which acted as a deterrence for researchers to use neural networks for a long time, is that there are far fewer guarantees of convergence for neural networks, while several strong guarantees exist for linear regression models. Nevertheless, this impasse was broken with the introduction of DQN, and in practice, RL algorithms have proven resilient to neural networks in general. The picture is different for off-policy methods, and fewer guarantees exist for the tabular, let alone the function approximation methods. </p> <p>Note that we are dealing mainly with regression from an ML perspective. This is because the value function is just a function that maps the state to an actual number, so the answer is continuous values (not discrete). It is just a value, so we face a regression problem. When we deal with function approximation, TD (r + V(s')-V(s)) and other methods do not take the gradient of the target V(s'); we only differentiate V(s). This is because we are bootstrapping, and in ML(supervised learning), the target is a fixed value that is not differentiable (while in RL, it is). Remember, for example, that the update rule for the Monte Carlo method involves (Gt-V(s)), which, calculating its gradient, involves differentiating V(s) only. Because of this, we call the methods that do not take the target's gradient a semi-gradient method. We then use the action-value function to establish a suitable policy as we did earlier (ex., e-greedy). </p> <p>Finally, we move to a different type of RL control algorithms that deal directly with the policy and attempts to learn a policy directly without going through the intermediate step of fitting a value function. These methods might use regression or classification models called policy-gradient methods because they differentiate the policy \\(\\pi\\) itself, not \\(Q\\). These methods are amongst the most promising methods in RL and have proven more resilience with more convergence guarantees than action-value functions methods. </p> <p>Ok, with all of that in mind, let us get started.</p>"},{"location":"unit4/lesson13/lesson13.html","title":"13. Linear Approximation for Prediction","text":"<p>The notebook uses a library of functionality in RL that aims for simplicity and general insight into how algorithms work, these libraries are written from scratch using standard Python libraries (numpy, matplotlib etc.). Please note that you will need permission from the author to use the code for research, commercially or otherwise.</p>"},{"location":"unit4/lesson13/lesson13.html#lesson-12-state-value-approximation-methods-for-prediction","title":"Lesson 12: State-Value Approximation Methods for Prediction","text":"<p>Learning outcomes</p> <ol> <li>understand the intractability of some real-world state space</li> <li>understand state space representation via a set of features and its advantages</li> <li>understand the properties of different function approximation models</li> <li>understand how to generalize tabular on-policy prediction methods to function approximation methods</li> </ol> <p>In this lesson, we deal with function approximation to represent the state. We will use different encoding regimes. One straightforward idea is to represent the whole space as a binary vector, where each entry represents a state. Another idea is to combine multiple entries/components from multiple vectors to represent the state and more. It is a good idea to start by reading section 9.5 of our book.</p> <p>Reading: The accompanying reading of this lesson is chapter 9 of our text book available online here. Please note that we explain the ideas of this topic from a practical perspective and not from a theoretical perspective which is already covered in the textbook.</p> <pre><code>%matplotlib inline\n</code></pre> <pre><code>from rl.rl import *\n</code></pre>"},{"location":"unit4/lesson13/lesson13.html#state-representation-one-hot-encoding","title":"State Representation: One-hot Encoding","text":""},{"location":"unit4/lesson13/lesson13.html#vectorising-grid-world-with-one-hot-encoding","title":"Vectorising Grid World with One-hot Encoding","text":"<p>In this section we aim to vectroise the grid world that we developed in the early lessons of a previous unit. The idea is to move gradually away from the tabular representation towards state-value function approximation where we approximate each state as a vector of features. Note that we where estimating the state-value function in the tabular form. So given that we assume limited states, it is possible, at least in theory to reach an exact value for all state-values or the action-values. When we deal with vector representation of a state, we depart from having finite number of states to the possibility of having infinite number of states in the world. Therefore, in this case we will be approximating the state representation and in turn we are estimating an approximate state-value and action-value functions and hence the name of our methods are Function Approximation Methods. </p> <p>To encode a state, we set to 1 the corresponding component while all other features are set to 0. </p> <pre><code>class vGrid(Grid):\n    def __init__(self, nF=None, **kw):\n        super().__init__( **kw)\n        # num of features to encode a state\n        self.nF = nF if nF is not None else self.nS \n        self.S = None\n\n    # vectorised state representation: one-hot encoding (1 component represents a state)\n    def s_(self):\n        \u03c6 = np.zeros(self.nF)\n        \u03c6[self.s] = 1 \n        return \u03c6\n\n    def S_(self):\n        if self.S is not None: return self.S\n        # S is a *matrix* that represents the full state space, this is only needed for Grid visualisation\n        sc = self.s  # store current state to be retrieved later\n        for self.s in range(self.nS): \n            self.S = np.c_[self.S, self.s_()] if self.s else self.s_()\n        self.s = sc \n        return self.S\n</code></pre> <p>Note that we need to pass 'vGrid' as the prefix for functions that deals with the different Grid types (such as maze and ranwalk ) to return a vGrid type instead of the usual Grid type in order to get the vectorised state representation that we defined above.</p> <pre><code>randwalk(vGrid).render()\nrandwalk(vGrid).s_()\n</code></pre> <p></p> <pre><code>array([0., 0., 0., 1., 0., 0., 0.])\n</code></pre> <pre><code>randwalk().render()\nrandwalk().s_()\n</code></pre> <p></p> <pre><code>3\n</code></pre> <p>Compare the return of the above calls, in the vGrid we obtained a vector that represents the current state while in the Grid we get the index of the current state. </p> <p>We can also define a vrandwalk to be a vectroised random walk grid as follows.</p> <pre><code>def vrandwalk(**kw):  return randwalk  (vGrid, **kw)\ndef vrandwalk_(**kw): return randwalk_ (vGrid, **kw)\ndef vgrid(**kw):      return grid      (vGrid, **kw)\ndef vmaze(**kw):      return maze      (vGrid, **kw)\ndef vcliffwalk(**kw): return cliffwalk (vGrid, **kw)\ndef vwindy(**kw):     return windy     (vGrid, **kw)\n</code></pre> <pre><code>vrandwalk().render()\nvrandwalk().s_()\n</code></pre> <p></p> <pre><code>array([0., 0., 0., 1., 0., 0., 0.])\n</code></pre>"},{"location":"unit4/lesson13/lesson13.html#prediction-with-function-approximation","title":"Prediction with Function Approximation","text":""},{"location":"unit4/lesson13/lesson13.html#mrp-with-linear-function-approximation","title":"MRP with Linear Function Approximation","text":"<p>Linear Feature Representation</p> <p>In a linear model we will devise, customary to linear regression models, a set of weights that correspond with each feature, so we have a weight vector that have the same size of the feature vector along with the bias, we will defer treating the bias to when it becomes necessary.</p> <p>Because we are dealing with linear models(regardless of the representation), the value function is given as</p> <p>\\(V(s) = w^\\top x\\)</p> <p>The update for the weights parameters will be applied similar to what we did for the tabular but this time we will use the dot product. </p> <p>In hot encoding, each component's weight is really the value function for the corresponding state. We should get identical results of those that we obtained for the problems that we tackled in the tabular form, ex. random walk, the maze, the cliff walking etc. </p> <pre><code>class vMRP(MRP):\n\n    # set up the weights, must be done whenever we train\n    def init(self):\n        self.w = np.ones(self.env.nF)*self.v0\n        self.V = self.V_ # this allows us to use a very similar syntax for our updates\n        self.S_= None\n\n    #-------------------------------------------buffer related-------------------------------------\n    # allocate a suitable buffer\n    def allocate(self): \n        super().allocate()\n        self.s = np.ones ((self.max_t, self.env.nF), dtype=np.uint32) *(self.env.nS+10)    \n\n    #---------------------------------------- retrieve Vs ------------------------------------------\n    def V_(self, s=None):\n        return self.w.dot(s) if s is not None else self.w.dot(self.env.S_()) \n\n    def \u0394V(self,s): # gradient: we should have used \u2207 but jupyter does not like it\n        return s\n</code></pre> <p>Note how we redefined the V as a function instead of as a array which allows us to use a very similar syntax for our updates, we will replace the squared brackets with rounded brackets and that's it!! thanks to the way we originally structured our MRP infrastructure. To appreciate this, let us see how we can redefine our offline MC algorithm along with the online TD update to deal with function approximation. Below we show how.</p>"},{"location":"unit4/lesson13/lesson13.html#gradient-mc-with-function-approximation","title":"Gradient MC with Function Approximation","text":"<pre><code>        -\u2207 Jt = -\u2207 1/2(\u03b4t^2) = \n        -\u2207 1/2(Gt - V(s))^2 = \n        -2(1/2)(Gt - V(s))*(\u2207 Gt - \u2207 V(s)) = \n        -1(Gt - V(s))*(0-s)=\n        (Gt - V(s))*s\n</code></pre> <pre><code>class MC(vMRP):\n    def __init__(self,  **kw):\n        super().__init__(**kw)\n        self.store = True \n\n    def init(self):\n        super().init() # this is needed to bring w to the scope of the child class\n        self.store = True \n\n    # ----------------------------- \ud83c\udf18 offline, MC learning: end-of-episode learning ----------------------    \n    def offline(self):\n        # obtain the return for the latest episode\n        Gt = 0\n        for t in range(self.t, -1, -1):\n            s = self.s[t]\n            rn = self.r[t+1]\n\n            Gt = self.\u03b3*Gt + rn\n            self.w += self.\u03b1*(Gt - self.V(s))*self.\u0394V(s)\n</code></pre> <p>This definition is almost identical to the tabular definition except for the following: 1. we update now a set of weights instead of one entry in a table. So in the left hand side we see w instead of V[s]. 2. we use V(.) instead of V[.] on the right hand side (we could have used operator overloading to keep using the [.] but it is a bit more involving)  3. we multiply by the gradient \u2207V(s) that is given by the MRP parent class.</p> <p>Note that s here is the state representation of the actual state, i.e. it is a vector of components.</p> <p>Note also that although the parent class deals with linear function approximation, the MC class does not assume that, it just needs the gradient of the V. So, as along as we make sure the parent MRP class does provide this function, we can define other types of MRP that use other types of function approximation like the tile coding or neural networks and we would not need to change the TD definition.  Please refer to section 9.3 of the book for more details.</p> <p>Let us now apply this vectorised from of the MC, or more precisely the gradient MC algorithm, on the random walk problem that use one-hot encoding. </p> <pre><code>mcwalk = MC(env=vrandwalk(), \u03b1=.01, episodes=20, v0=.5, seed=0, **demoV()).interact(label='offline MC learning')\n</code></pre> <p></p>"},{"location":"unit4/lesson13/lesson13.html#online-semi-gradient-td-with-linear-function-approximation","title":"Online Semi-Gradient TD with Linear Function Approximation","text":"<pre><code>class TD(vMRP):\n    # ----------------------------- \ud83c\udf16 online learning ----------------------    \n    def online(self, s, rn,sn, done, *args): \n        self.w += self.\u03b1*(rn + (1-done)*self.\u03b3*self.V(sn) - self.V(s))*self.\u0394V(s)\n</code></pre> <p>Again, note also that although the parent class deals with linear function approximation, the TD class does not assume that, it just needs the gradient of V. So, as along as we make sure the parent MRP class does provide this function, we can define other types of MRP that use other types of function approximation like the tile coding or neural networks and we would not need to change the TD definition. Therefore we can use a class factory to be able to change the parent class when we need to.</p> <p>We called this algorithmm a semi-gradient since we took the gradient of the value function V(s) and not the gradient of the target V(sn). The reason is to do be consistent with MC which is considered the theoretical baseline for TD. Nevertheless, there are algorithms that takes the gradient of both, although these are usually not as fast as semi-gradient TD, especially with linear function approximation, refer to section 9.3 of our book for more details.</p> <p>Let us now apply this vectorised from of the TD, or more precisely the semi-gradient TD algorithm. We call it semi-gradient because we to the gradient of one of the terms of TD error which is the estimation of the current state and we left the target estimation of the next state as is. Please refer to section 9.3 of the book.</p> <pre><code>TDwalk = TD(env=vrandwalk(), episodes=20, v0=.5, **demoV()).interact(label='TD learning')\n</code></pre> <p></p> <p>We need to pass the env explicitly because we have inherited the MRP and the new environment would have no took effect.</p>"},{"location":"unit4/lesson13/lesson13.html#offline-semi-gradient-td-with-function-approximation","title":"Offline Semi-Gradient TD with Function Approximation","text":"<pre><code>class TDf(vMRP):\n\n    def init(self):\n        super().init()\n        self.store = True\n\n    # ----------------------------- \ud83c\udf18 offline TD learning ----------------------------   \n    def offline(self):\n        for t in range(self.t, -1, -1):\n            s = self.s[t]\n            sn = self.s[t+1]\n            rn = self.r[t+1]\n            done = self.done[t+1]\n\n            self.w += self.\u03b1*(rn + (1-done)*self.\u03b3*self.V(sn) - self.V(s))*self.\u0394V(s)\n</code></pre> <pre><code>tdwalk = TDf(env=vrandwalk(), \u03b1=.05, episodes=50, v0=.5, **demoV()).interact(label='offline TD learning')\n</code></pre> <p>Let us rerun example 6.2 to double check that our algorithm is working well. This time we are using a vectorised grid and a linear model for TD and MC.</p> <pre><code># runing the book example 6.2 which compare TD and MC on randome walk, but this time we use vector representation\nexample_6_2(env=vrandwalk(), alg1=TDf, alg2=MC)\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|100/100\n</code></pre> <p></p>"},{"location":"unit4/lesson13/lesson13.html#one-hot-encoding-with-redundant-features-for-prediction","title":"One-hot-encoding with redundant features for prediction","text":"<p>As an auxiliary step towards generalising our classes to deal with any linear function approximation, below we create a random walk problem with 1000 redundant features to test whether our infrastructure classes are working.</p> <pre><code>vTDwalk = TD(env=vrandwalk_(nF=100), episodes=10, v0=.5, seed=10, **demoV()).interact(label='TD learning')\n</code></pre> <p></p>"},{"location":"unit4/lesson13/lesson13.html#state-representation-state-aggregation","title":"State Representation: State aggregation","text":"<p>\\(\\underbrace{\\{s_0\\}}_{F_0}, \\underbrace{\\{s_1, s_2, ..., s_{100}\\}}_{F_1}, \\underbrace{\\{s_{101}, s_{102}, ..., s_{200}\\}}_{F_2},...,  \\underbrace{\\{s_{901}, s_{902}, ..., s_{1000}\\}}_{F_{10}}, \\underbrace{\\{s_{1001}\\}}_{F_{11}},\\)</p> <p>For the state aggregation to work as intended, the goals must have a separate state representation from the groups; hence we dedicate the first and the last components for these two terminal states as we have shown above (or even put both of them in the same component). We then divide the non-goal states into 100, where each 100 are represented via a component and whenever the agent is at any of the 100 states the corresponding component will be on and all other components are off. </p> <p>Let us now move to a different state representation technique that generalizes the simple state-to-component one-hot encoding regime that we used earlier. We would like to move closer toward bridging continuous and discrete state space and so aggregation seems an obvious choice. In state aggregation, we group a set of states together and represent them all in one component(feature) and we use again one-hot encoding. This time one component represents a group of states instead of one state. Therefore, we turn a component(feature) on whenever the agent is at any of the group of states that corresponds to it. </p> <p>\\(F_i: \\underbrace{\\{s_0, s_1, ..., s_{99}\\}}_{F_0}, \\underbrace{\\{s_{100}, ..., s_{199}\\}}_{F_1},...,  \\underbrace{\\{s_{900}, ..., s_{999}\\}}_{F_9}\\)</p> <p>Therefore, if the agent is in state \\(s_{100}\\), in state \\(s_{199}\\) or in any state in between, the feature \\(F_1\\) will be on = 1 and the rest \\(F_0, F_2...F_9\\) are all \\(0\\)'s. The feature vector that represents the current state would be \\(F =[0,1,0,0,0,0,0,0,0,0]\\)</p> <p>This way also we treat the goal(terminal) states like other non-terminal states in terms of representation, and that is ok since we will leave the terminal state treatment to the learning algorithms. Recall that the return at time step \\(t\\) is the sum of expected future rewards from time step \\(t+1\\) to the end of an episode at time step \\(T\\) and is given by:  </p> <p>\\(G_t = R_{t+1} + R_{t+2} + ... + R_{T}\\)</p> <p>So the reward of current state does not participate in its return unlike the rewards of all future rewards that form the current state return. Hence the terminal state value (or expected return) is always set to 0 because the agent stays there and it will not obtain any future rewards from that state on. This is different than the terminal state reward itself given by the environment which may or may not be 0. Our algorithms do the following to treat terminal states: when the next state s' is a terminal state all of our algorithms will assign 0 to the terminal state's value estimation by multiplying it by (1-done), done is True on terminal state so (1-done)=0 at the terminal state. This alleviate us from having to designate a separate component to represent the terminal states and simplifies greatly our implementation.</p> <p>We call each group a tile, so a tile covers a group of states, think of a state in this context as the unit that we measure the tile's area with (hence the bridging of continuous and discrete state space). The above example has 10 tiles each of size 100 sates, while using tiles of size 200 mean that each covers or encompasses 200 states and will result in a feature vector of 5 components. </p> <p>Similar to vectors and matrices, tiles comes in different dimensions. When we deal with random walk problems we have 1-d tiles, while when we deal with a usual grid we will be dealing with 2-d tiles. The tile(group) size is stored in a variable called tilesize. Refer to example 9.2 in the book for more details. </p> <p>Below we provided an explanation of a precise process that we can use to achieve the above representation. </p> <pre><code>15//3\n</code></pre> <pre><code>5\n</code></pre> <pre><code>nS = 1002\nnS = 900\ngoals = [0, nS-1]\ntilesize = 200\n\n#------------calculating number of componenets(features)-----------\nnF =  -(-nS//tilesize)   # 1 in case of nS is not divisible by tilesize\nprint('number of groups = ', nF)\n\n#------------obtainng an index (feature)---------------------------\ns   =  100# goals[1] # goals[0]\nind =  s//tilesize\nprint('the goal\\'s index = ', ind)\n\n#------------assigning the feature---------------------------------\nw = np.zeros(nF)\nw[ind] = 1\nprint(w)\n</code></pre> <pre><code>number of groups =  5\nthe goal's index =  0\n[1. 0. 0. 0. 0.]\n</code></pre> <pre><code>#In python the operator // rounds up if we used a negative number\nprint(   1002//200 )\nprint(-(-1002//200))\n</code></pre> <pre><code>5\n6\n</code></pre> <pre><code>class aggGrid(vGrid):\n    def __init__(self, tilesize=1, **kw):\n        super().__init__(**kw)\n        self.tilesize = self.jump = tilesize\n        self.nF = -(-self.nS//self.tilesize)\n\n    def s_(self):\n        \u03c6 = np.zeros(self.nF) \n        \u03c6[self.s//self.tilesize] = 1 \n        return \u03c6\n</code></pre> <p>As we can see, we have encoded the states via our s_( ) function which uses one-hot encoding.  The index is specified via the aggregation which is achieved by using the //tilesize operation.</p>"},{"location":"unit4/lesson13/lesson13.html#1000-states-random-walk-with-jumps","title":"1000 states random walk with jumps","text":"<p>To shorten the time of transfer between the states and to adapt it to work well with state aggregation we inherited from the class vGrid which inherited from Grid, which in turn allows the agent to jump any number of states. vGrid also provide access to S_() function which is needed to obtain the V_ values of the random walk process.</p> <p>In the example below we choose to allow for a random jumps of up to 50 steps and we aggregate/group the states into 50 instead of 100 because this will result in around 20 feature similar to our 19-state random walk problem that we saw earlier in previous lessons (it is 19+2 states with terminal states).</p> <p>\\(\\underbrace{\\{s_0, s_1, ..., s_{49}\\}}_{F_0}, \\underbrace{\\{s_{50}, ..., s_{99}\\}}_{F_1},...,  \\underbrace{\\{s_{950}, ..., s_{999}\\}}_{F_{19}}\\)</p> <pre><code># assuming that vstar is a function that returns Vstar values\ndef aggrandwalk_(nS=1000, tilesize=50, vstar=None, **kw): \n    env = randwalk_(aggGrid, nS=nS, tilesize=tilesize, **kw)\n    if vstar is not None: env.Vstar = vstar(env) # vstar is a function\n    return env\n</code></pre> <pre><code>aggTDwalk = TD(env=aggrandwalk_(nS=12, tilesize=4, figsize=[40,.5], jump=4), \n                 episodes=100, v0=.5, seed=10, visual=True).interact(label='TD learning', pause=.5)\n</code></pre> <p></p> <p></p> <pre><code>env = aggrandwalk_(nS=23, tilesize=3, figsize=[40,.5])\nenv.render(pause=1)\nenv.jump=3\nenv.step(1)\nenv.render()\n</code></pre> <p></p> <p>Let us now apply our online TD on a 25 state aggregation problem with 5 groups.</p> <pre><code>aggTDwalk = TD(env=aggrandwalk_(nS=24+2, tilesize=4),\u03b1=.05, episodes=100, v0=.0, seed=10, **demoV()).interact(label='TD learning')\n</code></pre> <p></p>"},{"location":"unit4/lesson13/lesson13.html#n-step-td-with-linear-function-approximation","title":"n-step TD with linear function approximation","text":"<pre><code>class TDn(vMRP):\n\n    def init(self):\n        super().init()\n        self.store = True # there is a way to save storage by using t%(self.n+1) but we left it for clarity\n\n    # ----------------------------- \ud83c\udf16 online learning ----------------------    \n    def online(self,*args):\n        \u03c4 = self.t - (self.n-1);  n=self.n\n        if \u03c4&lt;0: return\n\n        # we take the min so that we do not exceed the episode limit (last step+1)\n        \u03c4n = \u03c4+n ; \u03c4n=min(\u03c4n, self.t+1 - self.skipstep)\n        \u03c41 = \u03c4+1\n\n        s\u03c4 = self.s[\u03c4 ]\n        sn = self.s[\u03c4n]\n        done = self.done[\u03c4n]\n\n        # n steps \u03c4+1,..., \u03c4+n inclusive of both ends\n        self.w += self.\u03b1*(self.G(\u03c41,\u03c4n)+ (1-done)*self.\u03b3**n *self.V(sn) - self.V(s\u03c4))*self.\u0394V(s\u03c4)\n</code></pre> <pre><code># try increase nS to 102+2 to see the effect\naggTDwalk = TDn(env=aggrandwalk_(nS=100, tilesize=10), \u03b1=.02, n=4, episodes=200, v0=.0, seed=10, **demoV()).interact(label='TD learning')\n</code></pre> <pre><code>aggTDwalk = TDn(env=aggrandwalk_(), \u03b1=.01, n=10, episodes=200, v0=.0, seed=0, **demoV()).interact(label='TD learning')\n</code></pre> <p>Note that we had to reduce the learning rate \u03b1 because the increased number of steps entails more updates and hence larger update magnitude.</p>"},{"location":"unit4/lesson13/lesson13.html#solving-the-1000-random-walk-via-dynamic-programmingdp","title":"Solving the 1000 Random Walk via Dynamic Programming(DP)","text":"<p>It might be hard to notice that the stairs looks a bit off (bottom steps start over the straight line and top ones appears under the line). The problem is in fact not in our TDn algorithm, instead it is in the Vstar solution (the straight line). Because we add the ability for the agent to jump, the old solution is not valid any more albeit very close.</p> <p>To arrive to a more accurate solution, we can hand in the problem to a dynamic programming algorithm to solve it for us. Below we show a solution for the 1000 random walk with jumps based on Dynamic Programming techniques that we covered in lesson 3. Particularly we use the policy evaluation method since we are dealing with prediction and the policy is stationary (agent moves either to the left or to the right with equal .5 probabilities.)</p> <p>DP will help us to see how far the initial guess (the one similar to 19-states but with 1000 states random walk without the jumps) from the actual solution of the 1000-states random walk with the jumps. </p> <pre><code>def DP(env=aggrandwalk_(), compare=False, \u03b8=1e-2):\n    \u03c0 = np.ones((env.nS, env.nA), dtype=np.uint32)*.5\n    Vstar = Policy_evaluation(env=env, \u03c0=\u03c0, V0=env.Vstar, \u03b8=\u03b8, show=False)\n    print('V* obtained')\n    if compare:\n        plt.plot(env.Vstar,   label='solution for 1000-random walk without jumps')\n        plt.plot(Vstar[1:-1], label='solution for 1000-random walk with    jumps')\n        plt.legend()\n    return Vstar\n</code></pre> <p>Let us compare the default straight line solution to the V* solution for the random walk problem when we employ the jumping procedure. This will take a couple of minutes so please wait for it.</p> <pre><code>aggVstar = DP(env=aggrandwalk_(), compare=True)\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,  2.14it/s]\n\n\nV* obtained\n</code></pre> <p></p> <p>This shows that the initial guess and the DP solution are close but not quite the same. Note that if we decrease \u03b8 the algorithm will get a more accurate estimation (the blue line will bend further) but it will take longer to run, the length of the run shows one of the issues of DP compared to more resilient and faster RL algorithms such as TD. Trye set  \u03b8=1e-3 to see how long it will take and share the length within the group discussion.</p>"},{"location":"unit4/lesson13/lesson13.html#solving-the-1000-random-walk-with-online-n-step-td-with-linear-function-approximation-and-state-aggregation","title":"Solving the 1000 Random Walk with Online n-step TD with linear function approximation and state aggregation","text":"<p>We can integrate finding a DP solution with the comparison as below, but since we have already found a solution in the previous steps we can simply also utilise the solution for comparison directly as we do in subsequent cells. Note that the level of accuracy, specified in \u03b8, can be adjusted. </p> <pre><code>%time nstepTD_MC_randwalk_\u03b1compare(env=aggrandwalk_(vstar=DP), \\\n                                   algorithm=TDn, runs=10, envlabel='1000', MCshow=False)\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,  2.30it/s]\n\n\nV* obtained\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\nCPU times: user 39min 16s, sys: 18min 26s, total: 57min 43s\nWall time: 8min 32s\n</code></pre> <p></p> <pre><code>%time nstepTD_MC_randwalk_\u03b1compare(env=aggrandwalk_(Vstar=aggVstar), \\\n                                   algorithm=TDn, runs=10, envlabel='1000', MCshow=False)\n</code></pre> <pre><code>100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\nCPU times: user 39min 59s, sys: 20min 33s, total: 1h 33s\nWall time: 8min 2s\n</code></pre> <p></p>"},{"location":"unit4/lesson13/lesson13.html#offline-n-step-td-with-linear-function-approximation","title":"Offline n-step TD with linear function approximation","text":"<pre><code>class TDnf(vMRP):\n\n    def init(self):\n        super().init()\n        self.store = True # offline method we need to store anyway\n\n    # ----------------------------- \ud83c\udf18 offline TD learning ----------------------------   \n    def offline(self):\n        n=self.n        \n        for t in range(self.t+n): # T+n to reach T+n-1\n            \u03c4  = t - (n-1)\n            if \u03c4&lt;0: continue\n\n            # we take the min so that we do not exceed the episode limit (last step+1)\n            \u03c41 = \u03c4+1\n            \u03c4n = \u03c4+n ; \u03c4n=min(\u03c4n, self.t+1)\n\n            s\u03c4 = self.s[\u03c4 ]\n            sn = self.s[\u03c4n]\n            done = self.done[\u03c4n]\n\n            # n steps \u03c4+1,..., \u03c4+n inclusive of both ends\n            self.w += self.\u03b1*(self.G(\u03c41,\u03c4n)+ (1-done)*self.\u03b3**n *self.V(sn) - self.V(s\u03c4))*self.\u0394V(s\u03c4)\n</code></pre>"},{"location":"unit4/lesson13/lesson13.html#solving-the-1000-with-offline-n-step-td-with-linear-function-approximation-and-state-aggregation","title":"Solving the 1000 with Offline n-step TD with linear function approximation and state aggregation","text":"<pre><code>%time nstepTD_MC_randwalk_\u03b1compare(env=aggrandwalk_(Vstar=aggVstar),\\\n                                   algorithm=TDnf, runs=10, alglabel='offline TD', envlabel='1000', MCshow=False)\n</code></pre> <pre><code>100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\n15%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|5/32\n\n/var/folders/5s/t5nyc5mx7gd0_0g56wdmpgmrq78pqs/T/ipykernel_13119/3845626547.py:23: RuntimeWarning: invalid value encountered in multiply\n  self.w += self.\u03b1*(self.G(\u03c41,\u03c4n)+ (1-done)*self.\u03b3**n *self.V(sn) - self.V(s\u03c4))*self.\u0394V(s\u03c4)\n\n\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|32/32\nCPU times: user 35min 31s, sys: 18min 28s, total: 54min\nWall time: 7min 13s\n</code></pre>"},{"location":"unit4/lesson13/lesson13.html#state-representation-tile-coding","title":"State Representation: Tile Coding","text":"<p>Tile coding is a powerful state representation, it takes the idea of state aggregation one step ahead. It is assumed that the state space is continuous and we aim to discretize it by partitioning it several times. In state aggregation that is what we have done. Effectively we have partitioned the state space into tiles. Here, we will do it for more than 1 time. </p> <p>We call each state partition that covers a specific area of the space as a tiling and we have \\(d\\) tilings. Each tiling has \\(n\\) tiles as we saw earlier in state aggregation. Each component of a tiling vector is called a tile.  The length of the tile encoding vector is \\(d\\times n\\).</p> <p>Therefore, instead of having one component on as in the state aggregation, we design a larger vector where we have more components that are turned on to help us differentiate further between the states. Note that the early grouping that we did in the previous section is actually a tile encoding with one tiling.</p> <p>When an agent is in state \\(s\\) we will have exactly \\(d\\) components turned on and the rest \\(d\\times n - d = d \\times (n-1)\\) are \\(0\\)s (while in state aggregation we had \\(1\\) component on and the rest of the \\(n-1\\) components were \\(0\\)). What is left is to understand how to construct and turn these \\(d\\) components on. For construction we simply look at the state space and cover it with a set of tiles (tiling) so that when an agent in a state, one of the tiles will be on. This can be simply a state partitioning. Then we lay another tiling, this time we shift (or offset/stride similar to what we do in CNN!) them so that there is some level of overlapping between the two tiling and so on. When we have two tilings, we will have two tiles on.</p> <p>\\(\\underbrace{\\{s_0, s_1, ..., s_{199}\\}}_{F_{0,0}}, \\underbrace{\\{s_{200}, ..., s_{399}\\}}_{F_{1,0}},...,  \\underbrace{\\{s_{800}, ..., s_{999}\\}}_{F_{4,0}}\\)</p> <p>\\(\\underbrace{\\{s_1, s_2, ..., s_{200}\\}}_{F_{0,1}}, \\underbrace{\\{s_{201}, ..., s_{400}\\}}_{F_{1,1}},...,  \\underbrace{\\{s_{801}, ..., s_{999}\\}}_{F_{4,1}}\\)</p> <p>\\(\\underbrace{\\{s_2, s_3, ..., s_{201}\\}}_{F_{0,2}}, \\underbrace{\\{s_{202}, ..., s_{401}\\}}_{F_{1,2}},...,  \\underbrace{\\{s_{802}, ..., s_{999}\\}}_{F_{4,2}}\\)</p> <p>So if the agent in stat \\(s_{200}\\) then its state tile coding representation would be:</p> <p>$F = [0, 1, 0, 0, 0, \\quad 1, 0, 0, 0, 0, \\quad 1, 0, 0, 0, 0] $</p> <p>Finally, if the tiling creates a strain on the memory requirement, we combine it with hashing. Hashing can save us lots of space and computation time. Note that when we use hashing we would need to construct a state vector that is of the size of the hashed features which is smaller than that of a \\(d\\times n\\) . Note that although we assumed that the state space is continuous, we can usually treat the state space as if it is continuous. See section 9.4.5 along with 9.4.4.</p> <pre><code>class tiledGrid(vGrid):\n    def __init__(self, ntilings, offset=4, tilesize=50, **kw):\n        super().__init__(**kw)\n        self.tilesize = self.jump = tilesize\n        self.ntilings = ntilings\n        self.offset = offset\n        self.ntiles = -(-self.nS//self.tilesize) \n        self.nF = self.ntiles*self.ntilings\n\n    def s_(self):\n        \u03c6 = np.zeros((self.ntilings, self.ntiles))\n\n        for tiling in range(self.ntilings):\n            ind = min((self.s + tiling*self.offset)//self.tilesize, self.ntiles-1)\n            \u03c6[tiling, ind] = 1\n\n        return \u03c6.flatten()\n</code></pre> <p>Let us now define a tiled random walk environment with 1000 states handy to be used in our next set of experiments. As usual it has a rewards of (-1,1) for the far left and far right states while it has a tile size of 200. We can use multiple tilings for it to cover its states. We will use our original guess as the optimal Vstar estimation for the state values,however we will allow Vstar to be assigned a more correct values based on DP.</p> <pre><code>def tiledrandwalk_(nS=1000, ntilings=1, tilesize=200, vstar=None, **kw):\n    env = randwalk_(tiledGrid, nS=nS, ntilings=ntilings, tilesize=tilesize,  **kw)\n    if vstar is not None: env.Vstar = vstar(env) \n    return env\n    #return randwalk_(tiledGrid, nS=nS, Vstar=Vstar, ntilings=ntilings, tilesize=tilesize,  **kw)\n</code></pre>"},{"location":"unit4/lesson13/lesson13.html#studying-the-effect-of-number-of-tilings","title":"Studying the Effect of Number of Tilings","text":"<p>Below we run the tiled random walk problem with different number of tilings to show there effect.</p> <pre><code>def TDtiledwalk(ntilings):\n    env=tiledrandwalk_(nS=20, tilesize=4, offset=1, ntilings=ntilings)\n    TD(env=env, \u03b1=.02, episodes=200, **demoV()).interact(label='TD learning, %d tilings'%ntilings)\n</code></pre> <pre><code>TDtiledwalk(ntilings=1)\n</code></pre> <p></p> <p>TDtiledwalk(ntilings=2)</p> <pre><code>TDtiledwalk(ntilings=3)\n</code></pre> <p></p> <p>TDtiledwalk(ntilings=4)</p> <pre><code>TDtiledwalk(ntilings=5)\n</code></pre> <p></p> <p>Note how the increased number of tilings enhanced the estimation and reduced the error and the values become smoother with less stair-style effect. So in fact ntilings has a smoothening effect on the value function and helps to improve the estimation of our algorithms. </p>"},{"location":"unit4/lesson13/lesson13.html#td-on-1000-tiled-coded-random-walk","title":"TD on 1000 Tiled Coded Random Walk","text":"<pre><code>TDwalk = TD(env=tiledrandwalk_(ntilings=8, Vstar=DP(tiledrandwalk_())),\u03b1=.005, episodes=200, **demoV()).interact(label='True Online TD(%.2f) learning')\n</code></pre> <p>When using function approximation, the objective function needs to be differentiable and we used the sum of squared error (SE) from which we obtained the gradient. This makes sense since it is easily differentiable objective function instead of the harder RMSE (root of the mean squared error). However we can still use our RMSE to measure the performance as we did in previous lesson that we have used previously. </p> <p>Below we compare between 1 tiling and 50 tilings for the tiled random walk problem to generate figure 9.10 of the book. We used Monte Carlo because it is a full gradient algorithm since the target does not involve a next step estimate, but we can use TD as well.</p> <p>First let us find the Vstar for a 200 jumps problem (the previous one was for 50 so we cannot use it).</p> <pre><code>def MCtiltingsRuns():\n    Vstar=DP(tiledrandwalk_(),\u03b8=1e-3)\n    for ntilings in [1, 50]:\n        env=tiledrandwalk_(ntilings=ntilings, tilesize=200, Vstar=Vstar)\n        \u03b1 =.001/ntilings\n        mcs = Runs(algorithm=MC(env=env,\u03b1=\u03b1, episodes=5000), v0=0, \n                   runs=30, plotE=True).interact(label='MC with %d tilings'%ntilings)\n    plt.ylim(0,.45)\n\nfigure_9_10=MCtiltingsRuns \n</code></pre> <pre><code>figure_9_10()\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:02&lt;00:00,  2.82s/it]\n\n\nV* obtained\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|30/30\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|30/30\n</code></pre> <p></p> <p>This figure takes a long time to produce due to the extensivity of the experiments. As we can see, adding more tilings further reduced the error of the value function estimation and hence enhanced the algorithm's performance.</p>"},{"location":"unit4/lesson13/lesson13.html#conclusion","title":"Conclusion","text":"<p>In this lesson we have covered the different aspects of using function approximation in the context of reinforcement learning methods We have seen how to generalize the ideas covered in previous lessons to parametric models via semi-gradient methods including Semi-gradient Sarsa.</p>"},{"location":"unit4/lesson13/lesson13.html#units-conclusion","title":"Unit's conclusion","text":"<p>This lesson concludes our unit where we have studied important formulation of RL which assumed that we use a parametric representation for our state space using function approximation instead of a table. In the next unit, we continue on that front to cover control algorithms that use function approximation and we will see a set of applications of RL in various domains.</p>"},{"location":"unit4/lesson13/lesson13.html#your-turn","title":"Your turn","text":"<ol> <li>Use tile coding on a grid world problem with Sarsa and see its effect.</li> <li>Sutton referred to idea of tile coding and that it is almost trivial to update the weights and rather than doing the dot product we can just pick the components that are active and update accordingly. Can you think of a way to implement such strategy for the state aggregation case? what kind of update we will end up with?</li> <li>When we varies ntilings in TDtiledwalk() function we more fluctuate in the error. Can you think of ways to counter this undesirable effect.</li> <li>In TDtiledwalk() try to reduce the learning rate \\(\\alpha\\) when we increase ntilings ex.: \\(\\alpha\\)=.02/ntilings. Adjust the code and see its effect.</li> <li>Prove the above state form of equation 13.9.</li> <li>Have a look at the following code and see if this implementation for aggGrid makes any difference to the random walk results obtained earlier. Note how this divides the non-terminal states by subtracting the 2 goal states form the count to define nF and then to obtain the hot-encoding it tests whether the state is a goal state and it always subtract 1.  $ i = (s-1)//200$ gives \\(F_i: \\underbrace{\\{s_1, s_2, ..., s_{200}\\}}_{F_0}, \\underbrace{\\{s_{201}, ..., s_{400}\\}}_{F_1},...,  \\underbrace{\\{s_{801}, ..., s_{1000}\\}}_{F_4}, \\underbrace{\\{s_{1001}, s_0\\}}_{F_5}\\). This implementation is more complicated but it will show us a more uniformed division for the non-terminal states. Note that you would need to add 2 also to the nS in randwalk_() so that you can visually see the desired effect of evenly dividing the non-terminal states equally.</li> </ol>"},{"location":"unit4/lesson14/lesson14.html","title":"14. Linear Approximation for Control","text":"<p>The notebook uses a library of functionality in RL that aims for simplicity and general insight into how algorithms work, these libraries are written from scratch using standard Python libraries (numpy, matplotlib etc.). Please note that you will need permission from the author to use the code for research, commercially or otherwise.</p>"},{"location":"unit4/lesson14/lesson14.html#lesson-13-action-value-approximation-and-policy-gradient-methods-for-control","title":"Lesson 13: Action-Value Approximation and Policy Gradient Methods for Control","text":"<p>Learning outcomes 1. understand how to generalize tabular control methods to function approximation methods 1. understand how to generalize tabular policy methods to policy approximation methods 1. understand how to using tile coding for a continuous control problem 1. understand the difference between additive and multiplicative representation 1. understand how to take advantage of binary representation to come up with a binary control algorithm-namely Binary Sarsa 1. understand the benefit of using hashing along the side with tile coding and the added benefit of using index hash table</p> <p>Reading: The accompanying reading of this lesson is chapters 10 and 12 of our text book available online here. Please note that we explain the ideas of this topic from a practical perspective and not from a theoretical perspective which is already covered in the textbook.</p> <p>In this lesson, we continue our coverage of algorithms that use function approximation, this time for control. We use tile coding again, but this time in the context of an underpowered car that needs to climb a mountain. In susequent lessons, we then move to a set of powerful techniques that utilise the idea of eligibility traces, which allow an algorithm to perform updates that mimic the effect of a set of n-steps updates without having to wait for n-steps! We then move to cover RL algorithms that are suitable for Robotics and Games, which utilise non-linear function approximation, particularly neural networks shallow and deep, such as DQN and DDQN.</p> <p>So far, we have not tackled any task that has a continuous state space; rather all the state spaces that we have come across were discrete. In this lesson, we will see how to apply the tile coding technique we covered in the previous lesson on a continuous control task. The task will be controlling an underpowered vehicle stuck between two hills. We want our car to successfully negotiate this terrain and reach the right hilltop. Along the way, we will develop a new binary algorithm, Binary Sarsa, that is suitable for dealing with binary encoding, and we will study its performance. We present three representations of the problem. The first just discretised the space and used a vector representation that corresponds with this discretisation. This representation is equivalent to using one tiling in tile coding. Then we develop this representation to use multiple tiling that offset each other to enrich the representation capability for our continuous space. We then try to reduce the extra overhead introduced by the tile coding using hashing. We show two types of hashing, one that uses raw hashing via the modulus % operator and one that uses an index hashing table, which guarantees correspondence with non-hashed tile coding when the table is large enough. Ok let us get started...</p> <pre><code>%matplotlib inline\n</code></pre> <pre><code>from rl.rlln import *\nfrom math import floor\n</code></pre> <p>Note that we imported the Grid and the other environments from the Function approximation lesson because we are dealing with vectorised environment form now on.</p>"},{"location":"unit4/lesson14/lesson14.html#control-with-function-approximation","title":"Control with Function approximation","text":""},{"location":"unit4/lesson14/lesson14.html#mdp-with-linear-function-approximation","title":"MDP with Linear Function Approximation","text":"<p>Below we show the implementation of the new MDP class. We have set it up in a way that allows us to maintain the same structure of the different updates that uses Q values. Note that we assign the new self.Q to the function self.Q_ which makes self.Q a function and then we can call self.Q(s,a) to obtain the Q values for state s and action a.</p> <pre><code>class vMDP(MDP(vMRP)):\n\n    def init(self):\n        super().init()\n        self.W = np.ones((self.env.nA, self.env.nF))*self.q0\n        self.Q = self.Q_\n\n    def Q_(self, s=None, a=None):\n        #print(s.shape)\n        W = self.W if a is None else self.W[a]\n        return W.dot(s) if s is not None else np.matmul(W, self.env.S_()).T \n\n    # we should have used \u2207 but python does not like it\n    def \u0394Q(self,s): \n        return s\n</code></pre> <p>Below we make sure that the classes hierarchy is correct by double-checking that the policy of an MDP object is \u03b5greedy.</p> <pre><code>vMDP().policy\n</code></pre> <pre><code>&lt;bound method MDP.&lt;locals&gt;.MDP.\u03b5greedy of &lt;__main__.vMDP object at 0x127e58b60&gt;&gt;\n</code></pre>"},{"location":"unit4/lesson14/lesson14.html#offline-mcc-with-any-function-approximation","title":"Offline MCC with Any Function Approximation","text":"<pre><code>class MCC(vMDP):\n\n    def init(self):\n        super().init()\n        self.store = True\n\n    # ---------------------------- \ud83c\udf18 offline, MC learning: end-of-episode learning-----------------------    \n    def offline(self):  \n        # obtain the return for the latest episode\n        Gt = 0\n        for t in range(self.t, -1, -1):\n            s = self.s[t]\n            a = self.a[t]\n            rn = self.r[t+1]\n\n            Gt = self.\u03b3*Gt + rn\n            self.W[a] += self.\u03b1*(Gt - self.Q(s,a))*self.\u0394Q(s)\n</code></pre> <pre><code>mc = MCC(env=vgrid(reward='reward100'), \u03b1=.5, episodes=20, seed=10, **demoQ()).interact()\n</code></pre>"},{"location":"unit4/lesson14/lesson14.html#online-sarsa-with-any-function-approximation","title":"Online Sarsa with Any Function Approximation","text":"<pre><code>class Sarsa(vMDP):\n\n    def init(self): #\u03b1=.8\n        super().init()\n        self.step = self.step_an # for Sarsa we want to decide the next action in time step t\n\n    # ----------------------------------------\ud83c\udf16 online learning ----------------------------------------\n    def online(self, s, rn,sn, done, a,an):\n        self.W[a] += self.\u03b1*(rn + (1-done)*self.\u03b3*self.Q(sn,an) - self.Q(s,a))*self.\u0394Q(s)\n</code></pre> <pre><code>sarsa = Sarsa(env=vgrid(reward='reward1'), \u03b1=.8, episodes=20, seed=10, **demoQ()).interact()\n</code></pre> <pre><code>sarsa.Q_(10,1)\n</code></pre> <pre><code>array([-32.256     , -33.9072    , -27.2128    , -24.5888    ,\n       -16.32      ,  -8.        ,  -8.        ,   0.        ,\n         0.        ,   0.        , -41.189888  , -33.6       ,\n       -27.4432    , -24.64      , -17.6       ,  -9.6       ,\n        -8.        ,  -8.        ,   0.        ,  -8.        ,\n       -39.7592576 , -35.354112  , -29.29706435, -19.989784  ,\n        -9.99998879,  -0.32      ,   0.        ,   0.        ,\n         0.        ,   0.        , -41.7846272 , -46.20985139,\n       -27.0091264 , -24.335872  , -16.144384  ,  10.        ,\n         0.        ,  -8.        ,  -8.        ,  -8.        ,\n       -39.0272    , -37.9008    , -24.256     , -18.88      ,\n       -11.586048  ,  -9.6       ,  -8.        ,   0.        ,\n         0.        ,   0.        , -33.28      , -32.83456   ,\n       -28.46976   , -24.97024   , -20.1728    , -11.2       ,\n        -8.        ,   0.        ,   0.        ,   0.        ,\n       -34.0992    , -26.6368    , -22.976     , -24.512     ,\n       -25.6128    , -28.16      ,  -8.        ,   0.        ,\n         0.        ,   0.        , -28.3648    , -32.4608    ,\n       -26.18624   , -27.0976    , -18.88      , -16.        ,\n       -14.4       ,   0.        ,   0.        ,   0.        ])\n</code></pre> <pre><code>def example_6_5():\n    return Sarsa(env=vwindy(reward='reward1'), \u03b1=.5, episodes=170, seed=100, **demoQ()).interact(label='TD on Windy')\n\ntrainedV = example_6_5()\n\nplt.subplot(133).plot(trainedV.Ts.cumsum(), range(trainedV.episodes),'-r', label='cumulative steps')\nplt.show()\n</code></pre>"},{"location":"unit4/lesson14/lesson14.html#q-learning-with-function-approximation","title":"Q-learning with Function Approximation","text":"<pre><code>class Qlearn(vMDP):\n# \ud83d\udd79\ufe0f \ud81a\udc44 \n    #--------------------------------------\ud83c\udf16 online learning --------------------------------------\n    def online(self, s, rn,sn, done, a,_):\n        self.W[a] += self.\u03b1*(rn + (1-done)*self.\u03b3*self.Q(sn).max() - self.Q(s,a))*self.\u0394Q(s)\n</code></pre> <pre><code>qlearn = Qlearn(env=vgrid(), \u03b1=.8, \u03b3=1, episodes=40, seed=10, **demoQ()).interact()\n</code></pre> <pre><code>qlearn = Qlearn(env=vgrid(reward='reward1'), \u03b3=1, \u03b1=.8, episodes=40, seed=10, **demoQ()).interact()\n</code></pre>"},{"location":"unit4/lesson14/lesson14.html#sarsa-and-q-learning-on-the-cliff","title":"Sarsa and Q-Learning on the Cliff!","text":"<pre><code>sarsa = Sarsa(env=vcliffwalk(), \u03b1=.5, episodes=500, seed=10, **demoR()).interact()\n</code></pre> <pre><code>sarsa = Qlearn(env=vcliffwalk(), \u03b1=.5, episodes=500, seed=10, **demoR()).interact()\n</code></pre> <pre><code>SarsaCliff, QlearnCliff = example_6_6(runs=20, env=vcliffwalk(), alg1=Sarsa, alg2=Qlearn)# runs=500\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n</code></pre> <pre><code>class XSarsa(vMDP):\n\n    # ------------------------------------- \ud83c\udf16 online learning --------------------------------------\n    def online(self, s, rn,sn, done, a,_):      \n        # obtain the \u03b5-greedy policy probabilities, then obtain the expecation via a dot product for efficiency\n        \u03c0 = self.\u03c0(sn)\n        v = self.Q(sn).dot(\u03c0)\n        self.W[a] += self.\u03b1*(rn + (1-done)*self.\u03b3*v - self.Q(s,a))*self.\u0394Q(s)\n</code></pre> <pre><code>xsarsa = XSarsa(env=vmaze(), \u03b1=.5, \u03b3=1, episodes=100, seed=1, **demoQ()).interact()\n</code></pre> <pre><code>xsarsa = XSarsa(env=vcliffwalk(), \u03b1=.5, \u03b3=1, episodes=100, seed=1, plotT=True).interact()\n</code></pre>"},{"location":"unit4/lesson14/lesson14.html#one-hot-encoding-with-redundant-features-for-control","title":"One-hot-encoding with redundant features for control","text":"<pre><code>sarsa = Sarsa(env=vmaze(nF=160, reward='reward1'), episodes=20, seed=10, **demoQ()).interact()\n</code></pre> <pre><code>sarsa.store=True\nsarsa.interact(seed=10)\n</code></pre> <pre><code>&lt;__main__.Sarsa at 0x129fabd70&gt;\n</code></pre> <pre><code>sarsa.s[0]\n</code></pre> <pre><code>array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0], dtype=uint32)\n</code></pre> <p>Note how we have almost 100 extra redundant features that will never be turned on. The point of doing such thing is to double check that our infrastructure is capable of accommodating different features sizes and is not tied to a feature representation that has the same size of the state space.</p>"},{"location":"unit4/lesson14/lesson14.html#policy-gradient-methods-with-function-approximation","title":"Policy Gradient Methods with Function Approximation","text":"<p>We turn out attention now to develop an actor-critic algorithm that uses function approximation. Note that as we said earlier the actor part uses control updates similar to Sarsa or Q learning while the critic part uses TD prediction updates. Hence, writing our algorithm is straightforward because we have already developed the infra-structure for both prediction and control.</p> <p>Note that if you have not read the policy gradient methods in lesson 4 and 5. It is now time to go back and read the corresponding sections. The ideas of policy gradient method are covered in chapter 13 of our book.</p> <pre><code>class Actor_Critic(PG(vMDP)):\n\n    def step0(self):\n        self.\u03b3t = 1 # powers of \u03b3\n\n    # -------------------------------------- \ud83c\udf16 online learning ------------------------------\n    def online(self, s, rn,sn, done, a,_): \n        \u03c0, \u03b3, \u03b3t, \u03b1, \u03c4, t, \u0394V, \u0394Q = self.\u03c0, self.\u03b3, self.\u03b3t, self.\u03b1, self.\u03c4, self.t, self.\u0394V, self.\u0394Q\n\n        \u03b4 = (1- done)*\u03b3*self.V(sn) + rn - self.V(s)    # TD error is based on the critic estimate\n\n        self.w    += \u03b1*\u03b4*\u0394V(s)                         # critic v\n        self.W[a] += \u03b1*\u03b4*\u0394Q(s)*(1 - \u03c0(s,a))*\u03b3t/\u03c4       # actor  \n        self.\u03b3t *= \u03b3  \n</code></pre> <p>Note that since we use a separate \\(\\theta_a\\) for each action, where we have that </p> <p>\\(Q(s,a) = \\phi^\\top \\theta_a \\qquad\\) \\(\\pi(a|s,\\theta_a) = \\frac{e^{\\phi^\\top \\theta_a}}{\\sum_{b}{e^{\\phi^\\top \\theta_b}}}\\)</p> <p>then equation 13.9 becomes as follows</p> <p>\\(\\nabla_{\\theta_a} ln \\pi(a|s,\\theta_a) = \\phi (1-  \\pi(a))\\)</p> <p>In the book authors assumes that we are using a concatenated vector \\(\\theta\\) for all actions where each n weights represents one of the actions.</p> <pre><code>ac = Actor_Critic(env=vgrid(), \u03b1=1, \u03c4=.1, \u03b3=1, episodes=100, seed=0 , **demoQ()).interact()\n</code></pre> <p></p> <p>In the below (disabled cell) we can see that the agent arrives at a good policy but it takes long to reach the goal. The reason is because the action-values differences is not high enough to converge to a close to greedy policy. Enable the cell by pressing escape then y and run the cell to see the lengthy run despite an apparent and sound policy shown by the arrows.</p> <p>ac = Actor_Critic(env=vmaze(), \u03b1=.1, \u03b3=.98, episodes=30, seed=0 , **demoQ()).interact()</p> <p>Below we solve the problem by decaying the exploration rate \u03c4 exponentially.</p> <pre><code>ac = Actor_Critic(env=vmaze(), \u03b1=.1, \u03c4min=.05, d\u03c4=.5,  \u03b3=1, episodes=30, seed=0 , **demoQ()).interact()\n</code></pre> <p></p> <pre><code>ac.\u03c4\n</code></pre> <pre><code>0.05\n</code></pre> <p>we can also decay linearly \u03c4 as follows</p> <pre><code>ac = Actor_Critic(env=vmaze(), \u03b1=.1, \u03c4min=.01, T\u03c4=500,  \u03b3=1, episodes=30, seed=0 , **demoQ()).interact()\n</code></pre> <p></p> <pre><code>ac.\u03c4\n</code></pre> <pre><code>0.01\n</code></pre> <p>However we can achieve a better results by using a combination of intermediate reward and higher learning rate. Let us start by setting a different reward.</p> <pre><code>ac = Actor_Critic(env=vmaze(reward='reward0'), \u03b1=.1, \u03c4=1,  \u03b3=1, episodes=30, seed=0 , **demoQ()).interact()\n</code></pre> <p></p> <p>Now let us increase the learning rate.</p> <pre><code>ac = Actor_Critic(env=vmaze(), \u03b1=.8, \u03c4=1,  \u03b3=1, episodes=30, seed=0 , **demoQ()).interact()\n</code></pre> <p></p> <p>Now let us do both</p> <pre><code>ac = Actor_Critic(env=vmaze(reward='reward0'), \u03b1=.8, \u03c4=1,  \u03b3=1, episodes=30, seed=0 , **demoQ()).interact()\n</code></pre> <p></p> <p>When we increase exploration for this setting we get </p>"},{"location":"unit4/lesson14/lesson14.html#mountain-car-problem","title":"Mountain Car Problem","text":"<p>We tackle the mountain car problem. This is problem is continuous and attention is needed for the way we represent the states. It has a boundaries for the x position as [-1.2,  .5]. In order to deal with it we need to discretize the X space by dividing it into a number of intervals each has a size of \u03c9.</p>"},{"location":"unit4/lesson14/lesson14.html#continuous-problem-and-state-space-discretisation","title":"Continuous Problem and State Space Discretisation","text":"<p>Below we show how we can discretise the problem by rescaling its range. We show the boundaries as well as some intermediate x positions. Note that we have always nS-2 as the max of the discretized s where the goal location is.</p> <pre><code>def ind(x=.1, printX=False):\n    X0, Xn  = -1.2, .5 \n    ntiles = 16\n    nS = ntiles +1\n    X = np.linspace(X0, Xn, ntiles)\n    scX = (ntiles)/(Xn - X0) # scaled range\n\n    s = int(scX*(x - X0)) \n\n    if printX: print(np.round(X,2))\n    print('at x=%.2f we have, s=%d:'%(x,s))\n    print()\n\nind(x=-1.2, printX=True)  # boundary case: far left\nind(x=.5)                 # boundary case: far right\nind(x=.5-.001)             # boundary case: far right\nind(x=.1)                 # mid state\n</code></pre> <pre><code>[-1.2  -1.09 -0.97 -0.86 -0.75 -0.63 -0.52 -0.41 -0.29 -0.18 -0.07  0.05\n  0.16  0.27  0.39  0.5 ]\nat x=-1.20 we have, s=0:\n\nat x=0.50 we have, s=16:\n\nat x=0.50 we have, s=15:\n\nat x=0.10 we have, s=12:\n</code></pre> <p>Note how the index for the far end ==ntiles which means that the number of states is actually ntiles+1.</p>"},{"location":"unit4/lesson14/lesson14.html#additive-state-space","title":"Additive State Space","text":"<p>We have two continuous spaces that specify the position and the velocity state of our car so we need to find a way to combine the two discretised spaces. We start by developing a form of binary encoding that is additive. We will call the set of partitions\u2019 tiles to help the consistency of the cover. But bear in mind that additive spaces are not called tilings. This will help us gradually move towards tile coding to appreciate its work and why. Below is the code for animating and dealing with this problem. We have used an additive state space. So, we have concatenated the discrete representation of the position (nS) with the discrete representation for the velocity(ntiles) to make up one vector for representing the position and the velocity of the car where the vector has a size of nS+ntiles.</p> <p>Ok let us get started...</p> <pre><code>print(floor(9.5/3.1))\nprint(9.5//3.1)\n</code></pre> <pre><code>3\n3.0\n</code></pre> <pre><code>class MountainCar:\n    def __init__(self, ntiles=8,  **kw):   # ntiles: number of tiles \n        # constants                          \n        self.X0,  self.Xn  = -1.2, .5       # position range\n        self.Xv0, self.Xvn = -.07, .07      # velocity range\n        self.\u03b7 = 3                          # we rescale by 3 to get the wavy valley/hill\n\n        # for render()\n        self.X  = np.linspace(self.X0,  self.Xn, 100)     # car's position\n        self.Xv = np.linspace(self.Xv0, self.Xvn, 100)    # car's speed\n        self.Y  = np.sin(self.X*self.\u03b7)\n\n        # for state encoding (indexes)\n        self.ntiles  = ntiles\n        # number of states is nS*nSd but number of features is nS+nSd with an econdoing power of 2^(nS+nSd)&gt;&gt;nS*nSd!\n        self.nF = self.nS = 2*(self.ntiles+1)\n\n        self.nA = 3\n        # for compatability\n        self.Vstar = None\n        self.nsubplots=3\n\n        # reset\n        self.x  = -.6 + rand()*(-.4+.6)\n        self.xv = 0\n\n        # figure setup\n        self.figsize0 = (12, 2) # figsize0 is used for compatibility\n\n\n    # get the descritized position and velocity\n    def s(self, tilings=1):\n        return floor(tilings*self.ntiles*(self.x  - self.X0 )/(self.Xn  - self.X0 ))\n\n    def sv(self, tilings=1):\n        return floor(tilings*self.ntiles*(self.xv - self.Xv0)/(self.Xvn - self.Xv0))\n\n    def reset(self):\n        #self.goals = self.nF -1 # to make sure that it is updated if we update nF\n        self.x  = -.6 + rand()*(-.4+.6)\n        self.xv = 0\n        return self.s_()\n\n\n    def s_(self):       \n        \u03c6 = np.zeros(self.nF) \n        \u03c6[self.s()] = 1 \n        \u03c6[self.sv() + self.ntiles + 1] = 1 \n        return \u03c6\n\n    # for compatibility\n    def S_(self):\n        return np.eye(self.nF)\n\n    def isatgoal(self):\n        return self.x==self.Xn\n\n    def step(self,a):\n        a-=1       # to map from 0,1,2 to -1,0,+1\n        self.xv += .001*a - .0025*np.cos(self.\u03b7*self.x); self.xv = max(min(self.xv, self.Xvn), self.Xv0)\n        self.x  += self.xv;                              self.x  = max(min(self.x,  self.Xn ), self.X0 )\n\n        # reset speed to 0 when reaching far left\n        if self.x&lt;=self.X0:  self.xv = 0\n\n        return self.s_(), -1.0, self.isatgoal(), {}\n\n\n    def render(self,  visible=True, pause=0, subplot=131, animate=True, **kw):\n        if not visible: return\n\n        self.ax0 = plt.subplot(subplot)\n        plt.gcf().set_size_inches(self.figsize0[0], self.figsize0[1])\n\n        car = '\\\u014d\u0361\u2261o\u02de\u0336' # fastemoji\n        bbox = {'fc': '1','pad': -5}\n\n        X = self.X\n        Y = self.Y\n        \u03b7 = self.\u03b7\n\n        plt.plot(X+.1,Y, 'k')\n        plt.plot(X[-1]+.1,Y[-1]-.05,'sg')\n        plt.text(X[-1],Y[-1]+.2,'Goal', color='g', size=14)\n        plt.title('Mountain Car', size=20)\n        plt.axis(\"off\")\n\n        # plot the mountain car \n        # take the derivative of the terrain to know the rotation of the car to make it more realistic\n        rotation = np.arctan(np.cos(self.x*\u03b7))*90  \n        plt.text(self.x, np.sin(self.x*\u03b7)+.05, car, va='center', rotation=rotation,  size=13, fontweight='bold', bbox=bbox)\n\n        if animate: clear_output(wait=True); plt.show(); time.sleep(pause)\n</code></pre> <pre><code>mcar = MountainCar()\nmcar.render()\n</code></pre> <p></p> <pre><code>mcar = MountainCar()\nmcar.x  = mcar.X0\nmcar.xd = 0\n\nwhile not mcar.isatgoal():\n    mcar.step(2)\n    mcar.render()\n    #print(np.sin(mcar.x))\n</code></pre> <p></p> <pre><code>mcar.xd\n</code></pre> <pre><code>0\n</code></pre>"},{"location":"unit4/lesson14/lesson14.html#studying-the-behaviour-of-the-car","title":"Studying the behaviour of the car","text":"<p>Let us see how the car behaves if we do not accelerated forward throttle=0, the car would depend on it mass and the inclination of the terrain and it would keep oscillating backwards and forward.</p> <pre><code>mcar.x  = mcar.X0\nmcar.xd = 0\n\nfor _ in range(200):\n    mcar.step(1)\n    mcar.render()\n</code></pre> <p></p> <p>The above was for starting on top of the left hill, but this is not the starting position of the car. The car always start at the bottom of the valley. Let us see how the car behaves if we just accelerated forward throttle=+1 when we start at the bottom of the valley.</p> <pre><code>mcar.reset()\nfor _ in range(200):\n    mcar.step(2)\n    mcar.render()\n</code></pre> <p></p> <p>Let us see how the car behaves when we start at the top of the right hill and when we just accelerated backwards throttle=-1.</p> <pre><code>mcar.x  = mcar.X[-2]\nmcar.xd = -1\n\nfor _ in range(200):\n    mcar.step(0)\n    mcar.render()\n</code></pre> <p></p>"},{"location":"unit4/lesson14/lesson14.html#optimal-solution","title":"Optimal solution","text":"<p>Obviously the optimal policy would be to accelerate forward and then backward to gain momentum to be able to reach the top since the car has not enough engine power to reach it by simple forward acceleration.</p> <pre><code>mcar.reset()\n\n# swing forward and backward to gain momentum and then go full speed on\nswing=24 # try 22 or less to see what happen\n# 1. accelerate forward  \nfor _ in range(swing):\n    mcar.step(2)\n    mcar.render()\n\n# 2. accelerate backward to take advantage from the momentum\nfor _ in range(swing):\n    mcar.step(0)\n    mcar.render()\n\n# 3. now full throttle forward to reach the goal\nfor _ in range(100):\n    mcar.step(2)\n    mcar.render()\n</code></pre> <p></p>"},{"location":"unit4/lesson14/lesson14.html#optimal-policy","title":"Optimal Policy","text":"<p>As we can see, the car must intelligently swing itself forward and then backwards before reaching the top. This kind of problem is hard to find a policy for using traditional control algorithms because things need to get worse (away from the goals) before getting better. The credit assignment plays on the long run rather than on immediate improvements.</p> <p>Note that in all what will come below, we set the learning rate to be \u03b1=()/8 because the default number of tiles is 8. Please differentiate between this and the number of tilings which we will vary, but it will also be set to 8.</p> <p>Ok let us start training</p>"},{"location":"unit4/lesson14/lesson14.html#short-number-of-episodes","title":"Short number of episodes","text":"<pre><code>sarsa = Sarsa(env=MountainCar(), \u03b1=.5/8, \u03b5=0.1, episodes=50, seed=1, **demoQ()).interact()\n</code></pre> <pre><code>qlearn = Qlearn(env=MountainCar(), \u03b1=.5/8, \u03b5=0.1, episodes=50, seed=1, **demoQ()).interact()\n</code></pre> <p>Longer episodes</p> <pre><code>sarsa = Sarsa(env=MountainCar(), \u03b1=.5/8,  \u03b5=0.1, episodes=500, seed=1, **demoQ()).interact()\n</code></pre> <p></p> <pre><code>qlearn = Qlearn(env=MountainCar(), \u03b1=.5/8, \u03b5=0.1, episodes=500, seed=1, **demoQ()).interact()\n</code></pre> <p></p> <p>Note that there is uncontrollable element of randomness due to the random starting position of the car that is part of the hardness of the problem. So each time you run even with the same seed you will get a different learning curve.</p>"},{"location":"unit4/lesson14/lesson14.html#exploration-by-optimistic-initialisation","title":"Exploration by optimistic initialisation","text":"<p>We use a reward scheme of -1 in every step with an initial value function of 0. These initial values are optimistic, given that the agent will be punished for every step before reaching the goal. Such settings naturally encourage the agent to explore the early episodes because it will be disappointed by the reward that it gets, so we do not need to adopt an exploratory policy explicitly, and we can set \u03b5=0. Let us see how the Sarsa act on such a purely greedy policy with optimistic initialisation to encourage exploration.</p> <pre><code>sarsa = Sarsa(env=MountainCar(), \u03b1=.1/8,   \u03b5=0,  episodes=500, seed=1, **demoQ()).interact()\n</code></pre> <p></p>"},{"location":"unit4/lesson14/lesson14.html#sarsa-for-binary-encoding","title":"Sarsa for Binary Encoding","text":"<p>Let us make our Sarsa implementation more efficient for a binary encoding where each component in the state vector is either 0 or 1. This simplifies the implementation of the dot product to be turned into summation. below we show the implementation.</p> <pre><code>s = np.array([0, 2 ,0, 1, 0])\n# s = np.array([0, 0 , 0, 0, 0])\nW = np.array([[.1,.2 ,.3, .4, .5], [1,2 ,3, 4, 5]])\nS = np.where(s!=0)[0]\nprint(len(S))\nfor i in S:\n    print(i)\n\nprint(W[0,np.where(s!=0)[0]].sum(0).round(2))\nprint(W[1,np.where(s!=0)[0]].sum(0).round(2))\n\nprint(W[:,np.where(s!=0)[0]].sum(1))\n</code></pre> <pre><code>2\n1\n3\n0.6\n6.0\n[0.6 6. ]\n</code></pre> <pre><code>class SarsaB(vMDP):  # Binary Sarsa that deals with binary state representation (later will have multiple tilings so we would have more than one hot encoding but each tilings is a one-hot encoding)\n\n    def init(self):\n        super().init() # \u03b1=.8\n        self.store = False       \n        self.step = self.step_an # for Sarsa we want to decide the next action in time step t\n\n    def Q_(self, s=None, a=None):\n        if s is None: return np.matmul(self.W, self.S).T \n        s_ = np.where(s!=0)[0] # set of indexes that represents states because we use a binary representation\n        Q = self.W[:,s_].sum(1) if a is None else self.W[a,s_].sum(0)\n        return Q if Q.size else 0\n\n    # ----------------------------------------\ud83c\udf16 online learning ----------------------------------------\n    def online(self, s, rn,sn, done, a,an):\n        s_ = np.where(s!=0)[0]\n        self.W[a,s_] += self.\u03b1*(rn + (1-done)*self.\u03b3*self.Q(sn,an) - self.Q(s,a))\n</code></pre>"},{"location":"unit4/lesson14/lesson14.html#runs-on-mountain-car","title":"Runs on Mountain Car","text":"<p>Let us now see the Sarsa behaviour on several runs</p> <pre><code>plt.ylim(100,1000)\n%time sarsaRuns = Runs(algorithm=Sarsa(env=MountainCar(),\u03b1=.1/8, episodes=500, \u03b5=0),\\\n                       runs=10, seed=1, plotT=True).interact(label='Sarsa on Mountain Car')\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|10/10\nCPU times: user 32.7 s, sys: 77.5 ms, total: 32.8 s\nWall time: 32.8 s\n</code></pre> <p></p> <pre><code>plt.ylim(100,1000)\n%time sarsaRuns = Runs(algorithm=SarsaB(env=MountainCar(), \u03b1=.1/8, episodes=500, \u03b5=0),\\\n                       runs=10, seed=1, plotT=True).interact(label='Sarsa on Mountain Car')\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|10/10\nCPU times: user 1min 8s, sys: 843 ms, total: 1min 9s\nWall time: 1min 9s\n</code></pre> <p></p> <p>As we can see both runs plots are identical as expected. However, Sarsa is faster than the Binary Sarsa. The reason is as usual the dot product is highly optimised and unless we are dealing with a very large state space the usual Sarsa is more efficient.</p>"},{"location":"unit4/lesson14/lesson14.html#binary-mountain-car-class-and-binary-sarsa","title":"Binary Mountain Car Class and Binary Sarsa","text":"<p>If we guarantee that we send indices instead of a feature vector we might be able to reduce the overhead. Let us see how.</p> <pre><code>class SarsaB(vMDP):  # Binary Sarsa that deals with binary state representation (later will have multiple tilings so we would have more than one hot encoding but each tilings is a one-hot encoding)\n\n    def init(self):\n        super().init() # \u03b1=.8\n        self.step = self.step_an # for Sarsa we want to decide the next action in time step t\n\n    def Q_(self, s=None, a=None):\n        if s is None: return np.matmul(self.W, self.S).T \n        Q = self.W[:,s].sum(1) if a is None else self.W[a,s].sum(0)\n        return Q if Q.size else 0\n\n    # ----------------------------------------\ud83c\udf16 online learning ----------------------------------------\n    def online(self, s, rn,sn, done, a,an):\n        self.W[a,s] += self.\u03b1*(rn + (1-done)*self.\u03b3*self.Q(sn,an) - self.Q(s,a))\n</code></pre> <p>As we can see we got rid of the s_ = np.where(s!=0)[0] statement in both the Q_() and online() functions. This statements obtain the indexes of the features that are on. To guarantee that s contain the indexes we need to adjust the environment as follows.</p> <pre><code>class MountainCarB(MountainCar):\n    def __init__(self, **kw):\n        super().__init__(**kw)\n\n    def s_(self):\n        return [self.s(), self.sv()+ self.ntiles + 1]\n</code></pre> <p>Now let us run the same experiments on the new two classes MountainCarB and SarsaB.</p> <pre><code>plt.ylim(100,1000)\n%time sarsaRuns = Runs(algorithm=SarsaB(env=MountainCarB(), \u03b1=.1/8, episodes=500, \u03b5=0),\\\n                       runs=10, seed=1, plotT=True).interact(label='Sarsa on Mountain Car')\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|10/10\nCPU times: user 1min, sys: 550 ms, total: 1min 1s\nWall time: 1min 1s\n</code></pre> <p></p>"},{"location":"unit4/lesson14/lesson14.html#mountain-car-runs","title":"Mountain Car Runs","text":"<p>Let us now develop a function to conduct several runs on the mountain car problem to see how the different state representations perform with different learning rates.</p> <pre><code>def MountainCarRuns(runs=20, algo=Sarsa, env=MountainCar(), label='', \u03b5=0):\n    for \u03b1 in [.1, .2, .5]:\n        sarsaRuns = Runs(algorithm=algo(env=env, \u03b1=\u03b1/8, episodes=500, \u03b5=\u03b5),\n                         runs=runs, seed=1, plotT=True).interact(label='\u03b1=%.2f/8'%\u03b1)\n    plt.ylim((10**2,10**3))\n    plt.yscale('log')\n    plt.title('Semi Gradient ' + algo.__name__  +' on Mountain Car '+label)\n\nfig_10_2 = MountainCarRuns\n</code></pre> <pre><code>%time MountainCarRuns()\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\nCPU times: user 2min 49s, sys: 470 ms, total: 2min 49s\nWall time: 2min 50s\n</code></pre> <p></p> <p>As we can see, the algorithm shows some signs of underperformance and instability. This is a common problem when we have less representational capabilities in our state space due to the additivity nature of our space.</p> <p>The adopted additive representation is ok, but it has its limitations. The main issue is that the total number of possible states is \\(ntiles \\times ntiles\\), but the dimensionality of our vector is far less than that it is ntiles+ntiles. It is not a one-hot encoding, so each component does not correspond to one state. Instead, two components correspond to one state. These two issues combined reduced the capability of our algorithms to generalise to nearby states.</p>"},{"location":"unit4/lesson14/lesson14.html#multiplicative-state-space-tile-coding-for-mountain-car","title":"Multiplicative State Space: Tile Coding for Mountain Car","text":"<p>In this section, we alter the state representation to have multiplicative space with a vector of size \\(ntiles \\times ntiles\\) to have a state dimensionality that matches the discrete space dimensionality. In effect, we return to one-hot encoding with each component corresponding with one state (of position and velocity). </p> <p>Hence, we will cover the space with a grid of 2-d instead of the two 1-d arrays of position and velocity we concatenated earlier. This will make the space dimension \\(ntiles \\times ntiles\\). The benefit is that we would have one hot encoding instead of the 2-hot encoding. Usually, in RL, one-hot encoding outperforms other encoding and is tried and tested in many applications. </p> <p>When we use multiple tilings, the space is \\(ntiles \\times ntiles \\times ntilings\\) where ntilings is the number of tilings. When we move to multiple tilings, we may have abandoned the one-hot encoding. However, we have made the state vector's total representation power multiple times what the discretised space is. This redundancy is crucial and works best for RL. </p> <p>This is similar to the idea of increasing the dimensionality in kernel methods which allowed us to gain more power in state representation, and then we found computationally convenient ways of reducing the computing demands via the kernel trick. Here, we also increase the space dimensionality via adopting multiple tilings, but then we reduce its computing demands by the hashing trick. So, we hash the bigger multiplicative state space into a fixed table which improves the efficiency of our algorithms and guarantees bounded memory and time complexities.</p> <p>Below we show how to change the representation of our space to a 2-d which greatly increases its dimensionality due to multiplication. But first, we start off by showing the effect of the rescaling, and then we show the implementation.</p> <pre><code>ntilings = ntiles = 5\n\nX0, Xn  = -1.2, .5\nscaling = (ntilings*ntiles)/(Xn-X0)\n\nx = X0                # left boundary state\nx = X0 + (Xn- X0)/2   # mid state\n# x = Xn                # right boundary state\n\ntiling = 0  # any value between 0..ntilings-1\nprint(int((x-X0)*scaling + tiling )//ntilings)\n</code></pre> <pre><code>2\n</code></pre> <p>Let us see also how the tilings and tiles indexes works together.</p> <pre><code>inds = []\n# ntilings = ntiles = 3\nds_s = 1  # displacement for s\nds_sv = 3 # displacement for sv\nfor tiling in range(ntilings):\n    s =  int((x-X0)*scaling + ds_s*tiling )//ntilings\n    sv = int((x-X0)*scaling + ds_sv*tiling )//ntilings\n    inds.append((tiling,s,sv))\nprint(inds)\n</code></pre> <pre><code>[(0, 2, 2), (1, 2, 3), (2, 2, 3), (3, 3, 4), (4, 3, 4)]\n</code></pre> <p>ok let us construct a feature vector and turn it corresponding indices</p> <pre><code>\u03c6 = np.zeros((ntilings, ntiles+ds_s, ntiles+ds_s+ds_sv))\nfor ind in inds: \n    \u03c6[ind]=1\n\n# we will print one tiling but feel free to print them all\ntiling=2 # any value between 0..ntilings-1\nprint(\u03c6[tiling])\n# print(\u03c6)\n</code></pre> <pre><code>[[0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n</code></pre> <p>As we can see the corresponding indices have been turned on. </p> <p>We can now flatten the feature vector \u03c6.</p> <pre><code>\u03c6.flatten()\n</code></pre> <pre><code>array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n</code></pre> <p>Note how we have exactly ntilings features turned on and the rest are 0. </p> <p>Ok so now we are ready to develop our tiled Mountain Car class.</p> <pre><code>class tiledMountainCar(MountainCar):\n    def __init__(self, ntilings=1, **kw): # ntilings: is number of tiles\n        super().__init__(**kw)\n        self.ntilings = ntilings\n        self.dim = (self.ntilings, self.ntiles+1, self.ntiles+3) # the redundancy to mach the displacements of position(x) and velocity(xv)\n        self.nF = self.dim[0]*self.dim[1]*self.dim[2]\n\n\n    def inds(self):\n        s_tiling = self.s(self.ntilings)\n        sv_tiling = self.sv(self.ntilings)\n\n        inds = []\n        for tiling in range(self.ntilings):\n            s  = (s_tiling  + 1*tiling )//self.ntilings\n            sv = (sv_tiling + 3*tiling )//self.ntilings\n            inds.append((tiling,s,sv))\n\n        return inds\n\n    def s_(self):\n        \u03c6 = np.zeros(self.dim)\n        for ind in self.inds(): \u03c6[ind]=1\n        return \u03c6.flatten()\n</code></pre> <p>As we can see, the implementation is simple we rescale x and xv, add the current tiling and divide the result by the number of tilings. In effect, this creates multiple space partitions, each shifted by (1*\u03c9/n)*i for the car position x by (3*\u03c9v/n)*i for the car velocity xv. </p> <p>We have rescaled the entire position, and velocity ranges into ntiles \\(\\times\\) ntilings. You can think of the number of tiles as centimetres and the nilings as millimeters on a ruler. The ruler may be 16 tiles (centimetres) long. Each tile is divided into 10 tilings(millimetres), but we have 10 rulers (the number of tilings is the same as the number of pieces each tile is divided into). Then when we want to know the encoding, we measure on the ruler where the input x is to obtain its tile. The first ruler starts at the start of the x range. Then we pick another ruler and offset its starting position by 1 millimetre (1 tiling), and we check the measure of our x on the second ruler to obtain its tile on the second ruler (on the second tiling), and we repeat the same process for all 10 rulers. </p> <p>We get a set of 10 active tiles on the 10 rulers, these will be our indexes that will be turned on in our state vector, and the rest are 0s. Our state vector size is ntiles x ntiles x n. The 1 and 3 are displacements to make the offset asymmetrical, which helps the generalisation (see figure 9.11 in the book for more details).</p> <pre><code>def SarsaOnMountainCar(ntilings, env=tiledMountainCar):\n    sarsa = Sarsa(env=env(ntilings=ntilings), \u03b1=.5/ntilings, episodes=500, seed=1, \u03b5=0, plotT=True).interact(label='ntilings=%d'%ntilings)\n    plt.gcf().set_size_inches(20,4)\n    plt.ylim(100,1000)\n</code></pre> <pre><code>for n in trange(5):\n    SarsaOnMountainCar(ntilings=2**n)\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:15&lt;00:00,  3.00s/it]\n</code></pre> <p></p> <p>Note the difference between the max steps and the max steps when we did not use tile coding. We will demonstrate the  advantage of tile coding further in a more extensive experiments.</p>"},{"location":"unit4/lesson14/lesson14.html#tiled-coded-mountain-car-runs","title":"Tiled coded mountain car runs","text":"<p>Let us see how the new tile coding behaves with respect to different learning rates, to do we simply call the SarsaMCar() function but we pass the tiledMountainCar environment with n=8.</p> <pre><code>MountainCarRuns(env=tiledMountainCar(ntilings=8), \u03b5=0, label='with 8 tilings')\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n</code></pre> <p></p> <p>Note that since we set  \u03b5=0 and used optimistically initialisation, these settings made Sarsa closer to Q-learning in the sense that it is learning about a purely greedy policy (of course Q-learning is acting usually according to an exploratory policy  \u03b5-greedy). Let us see how Q-learning behaves if we also set \u03b5=0.</p> <pre><code>MountainCarRuns(env=tiledMountainCar(ntilings=8), \u03b5=0.1, label='with 8 tilings')\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n</code></pre> <p></p> <pre><code>MountainCarRuns(algo=Qlearn, env=tiledMountainCar(ntilings=8), label='with 8 tilings')\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n</code></pre> <p></p> <p>Indeed as we can see .\u215d seems to be performing best for a additive one tile coding (the above class implemented one-hot coding which is a special case of tile coding where n=1). In the next section we implement a more generic state representation that allows for multiple tilings.</p>"},{"location":"unit4/lesson14/lesson14.html#binary-tiled-mountain-car","title":"Binary Tiled Mountain Car","text":"<pre><code>class tiledMountainCarB(tiledMountainCar):\n    def __init__(self, **kw): #ntilings: is number of tiles\n        super().__init__(**kw)\n\n    def s_(self):\n        inds=[]\n        s_tiling = self.s(self.ntilings)\n        sv_tiling = self.sv(self.ntilings)\n\n        for tiling in range(self.ntilings):\n            s  = (s_tiling  + 1*tiling )//self.ntilings\n            sv = (sv_tiling + 3*tiling )//self.ntilings\n            ind = (tiling*self.dim[1] + s)*self.dim[2] + sv\n            inds.append(ind)\n\n        return inds\n</code></pre> <pre><code>MountainCarRuns(algo=SarsaB, env=tiledMountainCarB(ntilings=8), label='Binary Mountain Car with 8 tilings')\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n</code></pre> <p>As we can see again the performance is the same with an extra cost overhead in terms of time.</p>"},{"location":"unit4/lesson14/lesson14.html#multiplicative-state-space-hashed-tile-coding-for-mountain-car","title":"Multiplicative State Space: Hashed Tile Coding for Mountain Car","text":"<p>This section draws on the idea of hashing a tile coding representation to reduce its dimensionality to make it more efficient. To solve the issues of high dimensionality when using multiple tilings, we can resort to the idea of hashing, which will help us to maintain a high resolution but reduce the space dimensionality. It does that by fixing the state space dimensionality regardless of the discretised space. </p> <p>It takes advantage of the fact that not all states will be visited as often, and when the agent starts exploring, the earlier states will be guaranteed to have unique encoding. Only later on, for less frequent states when the hashing table runs out of spaces, will there be some colliding (i.e. two states would have the same encoding). This violates a uniqueness of representation which is an explicit condition for several algorithms or similarity of representation for close states. However, such an effect is minimal compared to the benefits of redundancy of the original discretised space. The disadvantage can be ignored given that we made our table large enough.</p> <p>Below we show a basic implementation of hashing. The idea is very simple; we hash the index of the feature that we were turning on in the tiledMountainCar class. In other words, we take the two indexes of position and velocity, along with the tiling index, and we hash the three indexes as a tuple. Then we apply the modulus % with the hash_size (same as the number of features) to obtain the index of the feature that we want it to be active. Note that the hashing function gives us some number that represents the tuple. We will get the same number each time we pass the same tuple. That is all that is being provided by the built-in hashing function. However, some different tuples might have the same modulus, which will cause collision even when the table size is larger than the state space. We will try to avoid this collision issue later. At least, we will minimise it so that it occurs only when the state space exceeds the table size.</p> <pre><code>class hashedtiledMountainCar(tiledMountainCar):\n    def __init__(self, hash_size=1024,**kw): \n        super().__init__(**kw)\n        self.nF = hash_size # fixed size that does not vary with the ntilings* ntiles\n\n    def s_(self):\n        \u03c6 = np.zeros(self.nF)\n        for ind in self.inds():\n            \u03c6[hash(ind)%self.nF]=1\n        return \u03c6\n</code></pre> <pre><code>%time sarsa = Sarsa(env=hashedtiledMountainCar(ntilings=8, ntiles=8), \u03b1=.3/8, episodes=500, seed=1, \u03b5=0, plotT=True).interact(label='hased tiled')\n%time sarsa = Sarsa(env=      tiledMountainCar(ntilings=8, ntiles=8), \u03b1=.3/8, episodes=500, seed=1, \u03b5=0, plotT=True).interact(label='tiled')\n\nplt.gcf().set_size_inches(20,4)\nplt.ylim(100,1000)\n</code></pre> <pre><code>CPU times: user 3.03 s, sys: 8.76 ms, total: 3.04 s\nWall time: 3.05 s\n\n\n\n\n\n(100.0, 1000.0)\n</code></pre> <p></p> <p>Note that there are some differences in the results of the last two steps graphs due to the issue of collisions that was discussed above.</p> <pre><code>%time MountainCarRuns(env=hashedtiledMountainCar(ntilings=8, ntiles=8, hash_size=8*8*8), label='with hashed 8*8*8 tiles')\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\nCPU times: user 3min 26s, sys: 518 ms, total: 3min 26s\nWall time: 3min 27s\n</code></pre> <p></p> <p>Let us reduce the hash table size (i.e. the number of features) used to represent a state. Once we have less representation power we expect deterioration in performance. Note that we still are dealing with 8x8x8 tiled coding we are reducing the capability of the hash table to accommodate all of the discretised states with no collision.</p> <pre><code>%time MountainCarRuns(env=hashedtiledMountainCar(ntilings=8, ntiles=8, hash_size=8*8*4), label='with hashed 8*8*8 tiles')\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\nCPU times: user 4min 4s, sys: 821 ms, total: 4min 5s\nWall time: 4min 6s\n</code></pre> <p></p> <p>As we can see the performance of the model deteriorated a bit when we reduced the hash table size due to collision.</p>"},{"location":"unit4/lesson14/lesson14.html#index-hashed-table-iht","title":"Index Hashed Table (IHT)","text":"<p>Directly using the hash function on upcoming tuples of indexes is fine, but it loses an opportunity to store nearby states in nearby locations and to guarantee consistency in allowing for collision. A collision occurs when two tuples have the same index values. This means that two states will have the same representation, and the uniqueness of representation is violated. This can be a last resort for a very large state space, but it is better not to for small to medium spaces. </p> <p>The index hash table allows us to store the tuples as values (in a dictionary) and their indices as keys. Thanks to the dictionary structure in Python, we can retrieve the index of a tuple at any point with O(1) cost to do. What is left is then to only resort to hashing a key when we cannot store it anymore in the table, and in this case, we need to live with collision. Below we show a simplified and more concise implementation. For more details, refer to hash tile coding, described in our textbook. We built our code differently to avoid storing the indices explicitly in a table for more efficiency. The standard software has extra capabilities we do not need here (ex. adding the action index is unnecessary since we do not amalgamate all the weights of different actions in one large weight vector).</p> <pre><code>class IHTtiledMountainCar(tiledMountainCar):\n    def __init__(self, iht_size=1024, **kw): # by default we have 8*8*8 (position tiles * velocity tiles * tilings)\n        super().__init__(**kw)\n        self.nF = iht_size\n\n    def s_(self):\n        \u03c6 = np.zeros(self.nF)\n        inds = np.where(super().s_()!=0)[0]\n        \u03c6[inds%self.nF]=1\n        return \u03c6\n</code></pre> <pre><code>10*11*8\n</code></pre> <pre><code>880\n</code></pre> <pre><code>%time sarsa = Sarsa(env=IHTtiledMountainCar(ntilings=8, ntiles=8), \\\n                \u03b1=.3/8, episodes=500, seed=1, \u03b5=0, plotT=True).interact(label='IHT tilings')\n\n%time sarsa = Sarsa(env=tiledMountainCar(ntilings=8, ntiles=8), \\\n                \u03b1=.3/8, episodes=500, seed=1, \u03b5=0, plotT=True).interact(label='tilings')\n\n# %time sarsa = Sarsa(env=hashedtiledMountainCar(ntilings=8, ntiles=8), \u03b1=.3/8, episodes=500, seed=1, \u03b5=0, plotT=True).interact(label='hashed tilings')\n\nplt.ylim(100, 1000)\n</code></pre> <pre><code>CPU times: user 2.93 s, sys: 3.14 ms, total: 2.93 s\nWall time: 2.93 s\n\n\n\n\n\n(100.0, 1000.0)\n</code></pre> <p></p> <p>Note how the two graphs are identical, although we have used an index hashing table (IHT) in the second while we did not in the first. This is because IHT maps the indices of the visited states, and unless the number of states exceeds the IHT size, the results will be identical to those with no hashing. This is the major advantage of IHT over just hashing. The downside is that it can be computationally more expensive to use IHT, so the equivalence is with an overhead cost. However, in our implementation, we have avoided this overhead completely!</p> <pre><code>MountainCarRuns(runs=10, env=IHTtiledMountainCar(ntilings=8,ntiles=8), label='with index hashed table for 8*8*8 tiles')\nMountainCarRuns(runs=10, env=   tiledMountainCar(ntilings=8,ntiles=8), label='with 8*8*8 tiles')\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|10/10\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|10/10\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|10/10\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|10/10\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|10/10\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|10/10\n</code></pre> <p></p>"},{"location":"unit4/lesson14/lesson14.html#collisions-in-a-hash-representation","title":"Collisions in a Hash Representation","text":"<p>Note that the actual state dimension is tiles \\(\\times\\) tiles  \\(\\times\\) number of tilings: since we have the 8 tiles for the positions and 8 for the velocity (8 by 8 grid) and 8 tilings:</p> <pre><code>8*8*8\n</code></pre> <pre><code>512\n</code></pre> <p>So we could get away with 512 without collision in the hash table. Collision occurs whenever we use the same key for more than one state. We scarify some accuracy due this collision. So unless it is necessary we try to avoid it by being generous in the memory allocation for the hash table (same as our feature vector). We may allow  some collision for larger space on the base that important states will be visited more often and hence their representation will overshadow other less frequent states.</p>"},{"location":"unit4/lesson14/lesson14.html#tilings-comparison","title":"Tilings Comparison","text":"<p>In this section we compare between the performance of Sarsa on different tilings to see the effect of changing the power of state representation.</p> <pre><code>def MountainCarTilings(runs=20, \u03b1=.3, algo=Sarsa, env=tiledMountainCar):\n\n    plt.title('Sarsa on mountain car: comparison of different tilings with \u03b1=%.2f/8'%\u03b1)\n    for ntilings in [2, 4, 8, 16, 32]:\n        sarsaRuns = Runs(algorithm=algo(env=env(ntiles=8, ntilings=ntilings),\u03b1=\u03b1/ntilings,episodes=500, \u03b5=0), \n                         runs=runs, seed=1, plotT=True).interact(label='%d tilings'%ntilings)\n    plt.ylim((10**2,10**3))\n    plt.yscale('log')\n    plt.show()\n</code></pre> <pre><code>MountainCarTilings()\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n</code></pre> <p></p>"},{"location":"unit4/lesson14/lesson14.html#comparing-n-step-sarsa-on-mountain-car-with-different","title":"Comparing n-Step Sarsa on Mountain Car with different  \u03b1","text":"<p>Let us compare systematically how different number of steps plays with the learning rate \u03b1 for the mountain car problem. We will use hashed tilings with 8 tiles and 8 tilings. The goal is to confirm whether the behaviour of n-step Sarsa for control is compatible with the behaviour of n-step TD for prediction. </p> <p>Below we show the implementation of n-step Sarsa with function approximation. It is identical to the tabular form except we use the function Q(sn,an) instead of Q[sn,an].</p> <pre><code>class Sarsan(vMDP):\n\n    def init(self):\n        super().init()\n        self.store = True        # although online but we need to access *some* of earlier steps,\n        self.step = self.step_an # for Sarsa we want to decide the next action in time step t\n\n    # ----------------------------- \ud83c\udf16 online learning ----------------------    \n    def online(self, *args):\n        \u03c4 = self.t - (self.n-1);  n=self.n\n        if \u03c4&lt;0: return\n\n        # we take the min so that we do not exceed the episode limit (last step+1)\n        \u03c41 = \u03c4+1\n        \u03c4n = \u03c4+n ; \u03c4n=min(\u03c4n, self.t+1 - self.skipstep)\n\n        s\u03c4 = self.s[\u03c4];  a\u03c4 = self.a[\u03c4]\n        sn = self.s[\u03c4n]; an = self.a[\u03c4n]\n        done = self.done[\u03c4n]\n\n        # n steps \u03c4+1,..., \u03c4+n inclusive of both ends\n        self.W[a\u03c4] += self.\u03b1*(self.G(\u03c41,\u03c4n) + (1-done)*self.\u03b3**n *self.Q(sn,an) - self.Q(s\u03c4,a\u03c4))*self.\u0394Q(s\u03c4)\n</code></pre> <p>Below we see that the an intermediate n (number of steps in n-step Sarsa) acts best in similar to the effect of the number of steps on random walk and that control algorithms with intermediate bootstrapping usually performs best.</p> <pre><code>def MountainCarTiledRuns_n(runs=20, algo=Sarsan, env=tiledMountainCar):\n\n    plt.title(algo.__name__+' on mountain car: comparison of n-steps with the same 8x8x8 tilings')\n    for n, \u03b1 in zip([1, 8], [.5, .3]):\n        sarsaRuns = Runs(algorithm=algo(env=env(ntiles=8, ntilings=8), n=n, \u03b1=\u03b1/8,episodes=500, \u03b5=0), \n                         runs=runs, seed=1, plotT=True).interact(label='%d step-Sarsa, \u03b1=%.2f/8'%(n,\u03b1))\n    plt.ylim((10**2,10**3))\n    plt.yscale('log')\n    plt.show()\nfigure_10_3_n = MountainCarTiledRuns_n\n</code></pre> <pre><code>%time figure_10_3_n(env=tiledMountainCar)\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n</code></pre> <p></p> <pre><code>CPU times: user 2min 1s, sys: 231 ms, total: 2min 1s\nWall time: 2min 1s\n</code></pre> <pre><code>%time figure_10_3_n(env=hashedtiledMountainCar)\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n</code></pre> <p></p> <pre><code>CPU times: user 2min 10s, sys: 249 ms, total: 2min 11s\nWall time: 2min 11s\n</code></pre> <pre><code>%time figure_10_3_n(env=IHTtiledMountainCar)\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n</code></pre> <p></p> <pre><code>CPU times: user 2min 28s, sys: 270 ms, total: 2min 28s\nWall time: 2min 28s\n</code></pre> <p>As we can see the IHT performed exactly the same as the tiledMountainCar representation since we are using a large enough table_size, it gave a boost to the one-step Sarsa to become closer to the 8-step Sarsa.</p>"},{"location":"unit4/lesson14/lesson14.html#model-selection-for-n_step-sarsa","title":"Model Selection for n_step Sarsa","text":"<p>In this section we conduct an extensive set of experiments on Mountain Car with different learning rates in a similar manner to the study that we conducted for TD with random walk problem.</p> <pre><code>def MountainCarTiledCompare_n(runs=5, ntilings=8,  env=IHTtiledMountainCar): # 10\n\n    xsticks = np.array([0, .5 , 1, 1.5, 2, 2.3])/ntilings\n    plt.xticks(ticks=xsticks, labels=xsticks*ntilings)\n    plt.yticks([220, 240, 260, 280, 300])\n    plt.ylim(210, 300)\n    plt.title('Steps per episode averaged over first 50 episodes')\n\n    for n in range(5):\n        if n==0: \u03b1s = np.arange(.4,  1.8,  .1)\n        if n==1: \u03b1s = np.arange(.2,  1.8,  .1)\n        if n==2: \u03b1s = np.arange(.1,  1.8,  .1)\n        if n==3: \u03b1s = np.arange(.1,  1.2,  .07)\n        if n==4: \u03b1s = np.arange(.1,  1.0,  .07)\n\n        Compare(algorithm=Sarsan(env=env(ntiles=8, ntilings=ntilings), n=2**n, episodes=50, \u03b5=0), runs=runs, \n                                  hyper={'\u03b1':\u03b1s/ntilings}, \n                                  plotT=True).compare(label='%d-step Sarsa'%2**n)\n    plt.xlabel(r'$\\alpha \\times 8$ since we used 8 tiles for each tilings')\n    plt.show()\n\n\nfigure_10_4_n = MountainCarTiledCompare_n\n</code></pre> <p>Note that we always divided the learning rates \u03b1s by ntilings to match the amount of changes for different number of tilings so that we end up with an update magnitude that is compatible with a one-hot encoding.</p> <pre><code>%time MountainCarTiledCompare_n()\n</code></pre> <pre><code>100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|14/14\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|16/16\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|17/17\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|16/16\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|13/13\n</code></pre> <p></p> <pre><code>CPU times: user 4min 51s, sys: 588 ms, total: 4min 52s\nWall time: 4min 52s\n</code></pre> <p>Note that we have not used hashing and hence our range is different from the one stated in the book. Nevertheless, the pattern is maintained.</p>"},{"location":"unit4/lesson14/lesson14.html#conclusion","title":"Conclusion","text":"<p>In this lesson, you saw how to deal with a continuous state space using function approximation and how to apply previous concepts to a more difficult control problem. We saw how to apply the tile coding technique we covered in the previous lesson on a continuous control Mountain car task. Along the way, we developed a new binary algorithm, Binary Sarsa, that is suitable for dealing with binary encoding, and we will study its performance. We present three representations of the problem. The first just discretised the space and used a vector representation that corresponds with this discretisation. This representation is equivalent to using one tiling in tile coding. Then we developed this representation to use multiple tiling that offset each other to enrich the representation capability for our continuous space. We then reduced the extra overhead introduced by the tile coding using hashing. We show that hashing is a powerful technique, and we studied a simple and efficient implementation of it. We concluded by updating our Sarsa(n) algorithm and applying it to the mountain car to compare its performance with different n. In the next lesson, we will show a new set of algorithms that achieve a similar or better performance without waiting for n steps.</p>"},{"location":"unit4/lesson14/lesson14.html#your-turn","title":"Your turn","text":"<ol> <li>run the MountainCarTilings but this time fixed the divider on 8 (instead of dividing by ntilings), observe the results and compare them to the original experiments, post what you can infer in the group discussion.</li> </ol>"},{"location":"unit4/lesson14/lesson14.html#challenge","title":"Challenge","text":"<ol> <li>Implement a state rep that directly use the software library provided by the book here.</li> <li>Can you think of a way to make the Mountain Car class works for the tabular case(assume that we have one tiling )</li> </ol>"},{"location":"unit5/lesson15/lesson15.html","title":"15. Linear Approximation with Eligibility Traces(prediction and control)","text":"<p>Unit 5: Learning Outcomes By the end of this unit, you will be able to:  </p> <ol> <li>Predict the value function for a policy using function approximation techniques.  </li> <li>Explain eligibility traces and the trade-offs associated with their depth.  </li> <li>Implement control methods that infer an agent\u2019s policy from an action-value function with function approximation.  </li> <li>Apply RL techniques to control a robotic system.  </li> </ol> <p>In this unit we continue our coverage of function approximation to try to achieve muti-step update effect by performing one update only via the nifty ideas of elegibility traces. We then move into utilising nonlinear function approximation to perfrom prediction and control in RL settings.</p>"},{"location":"unit5/lesson15/lesson15.html#lesson-14-eligibility-traces-for-approximate-prediction-and-control","title":"Lesson 14: Eligibility Traces for Approximate Prediction and Control","text":"<p>Learning outcomes</p> <ol> <li>understand the basic idea of the forward offline \u03bb-return algorithm and its average all possible n-step updates</li> <li>understand the basic idea of a backward implementation of \u03bb-return in an online algorithm via eligibility traces </li> <li>understand how eligibility traces achieve a trade-off between full bootstrapping TD(0) and no bootstrapping MC=TD(1)</li> <li>appreciate that eligibility traces are done on the component level when dealing with function approximation</li> </ol> <p>Reading: The accompanying reading of this lesson is chapters 10 and 12 of our text book available online here. Please note that we explain the ideas of this topic from a practical perspective and not from a theoretical perspective which is already covered in the textbook.</p> <p>This lesson tackles eligibility traces as a new algorithmic mechanism to achieve more in our online updates. This mechanism allows us to efficiently implement RL algorithms that perform n-step bootstrapped learning without waiting for or accessing the previous n steps. Instead, we maintain a set of parameters updated regularly online each time an agent visits a state. Then this vector representing recently visited states will be sued to update the weights instead of the simple state vector. Eligibility traces are a powerful trick often used in classical RL. Ok, let us get started...</p> <pre><code>%matplotlib inline\n</code></pre> <pre><code>from rl.rlln import *\n</code></pre> <p>Note that we imported the Grid and the other environments from the Function approximation lesson because we are dealing with vectorised environment form now on.</p>"},{"location":"unit5/lesson15/lesson15.html#prediction-with-eligibility-traces","title":"Prediction with Eligibility Traces","text":""},{"location":"unit5/lesson15/lesson15.html#td","title":"TD(\u03bb)","text":"<p>TD(\u03bb) uses eligibility traces which allow us to implement the \u03bb-return algorithm more efficiently in a straightforward and online manner! Refer to section 12.2. This is an online algorithm. Surprisingly, we can obtain the average of an offline set of all n-step TD updates. The equivalence is only guaranteed for the offline case. Nevertheless, the online case works just fine. The next section will tackle the true online, which addresses this shortcoming.  The accumulating eligibility trace takes the form. </p> <p>\\(z_{t} = \\gamma\\lambda z_{t-1} + \u2207\\hat{v}(S_t, w_t)\\)</p> <p>while for the update, we replace the gradient with the eligibility vector. Note that this type of eligibility trace has the same gradient requirement regarding storage and processing time.</p> <p>\\(\\delta_{t} = R_{t+1} + \\gamma \\hat{v}(S_{t+1},w_t) - \\hat{v}(S_t,w_t)\\)</p> <p>\\(w_{t+1} = w_{t} + \\alpha\\delta_{t}z_{t}\\)</p> <p>The eligibility traces need to be initialised at the start of each episode, and we achieve that by using our step0 function.</p> <pre><code>class TD\u03bb(vMRP):\n    def __init__(self, \u03bb=.5, **kw):\n        super().__init__(**kw)\n        self.\u03bb = \u03bb\n\n    def step0(self):\n        self.z = self.w*0\n\n    # ----------------------------- \ud83c\udf16 online learning ----------------------    \n    def online(self, s, rn,sn, done, *args): \n        \u03b1, \u03b3, \u03bb = self.\u03b1, self.\u03b3, self.\u03bb\n        self.z = \u03bb*\u03b3*self.z + self.\u0394V(s)\n        self.w += \u03b1*(rn + (1-done)*\u03b3*self.V(sn) - self.V(s))*self.z\n</code></pre> <pre><code>TD\u03bbwalk = TD\u03bb(env=vrandwalk(), \u03bb=.9, \u03b1=.03, episodes=100, v0=.5, **demoV()).interact(label='TD learning')\n</code></pre> <p></p> <pre><code>TDwalk = TD(env=vrandwalk(), \u03b1=.03, episodes=100,  v0=.5, **demoV()).interact(label='TD learning')\n</code></pre> <p></p> <p>As we can see TD(\u03bb) can converge faster than TD as it compresses several TD updates in each of its updates. This leads to the jittery behaviour that usually makes us reduce the learning rate but we can achieve more updates in less episodes in general. The other downside is that we have to tune the hyper parameter \u03bb which can take more effort. </p>"},{"location":"unit5/lesson15/lesson15.html#true-online-td-and-dutch-traces","title":"True Online TD(\u03bb) and Dutch Traces","text":"<p>We implement the True online TD(\u03bb) algorithm in this section. This algorithm guarantees an equivalence between the online and the offline cases, and it was hailed as an important step towards implementing a multi-step prediction algorithm that is both efficient and sound. in particular, it guarantees an equivalence between the offline \u03bb-return algorithm (which takes the average of all possible n-step updates) and the true online TD(\u03bb)! The justification or proof is quite involved. Please refer to sections 12.5 and 12.6 of the book. you can also have a look at the original true TD paper and subsequent paper the latter is packed with useful information.</p> <p>The drawback is that it can only be applied on a linear approximation and not on any general one as TD(\u03bb), but it is a step in the right direction until researchers find a way to do it also for the nonlinear function approximator as well (such as neural networks). </p> <p>We need to handle an eligibility trace variable (Dutch trace), the size of which is identical to the weights. The update rule is given in equation 12.11, and its justification/deduction can be read from section 12.5. which shows the formula for Dutch traces.</p> <p>Dutch traces are available when we use linear function approximation. They perform better than the classical accumulated eligibility traces. They take the form of </p> <p>\\(z_{t} = \\gamma\\lambda z_{t-1} + (1-\\alpha\\gamma\\lambda z_{t-1}^\\top x_{t})x_{t}\\)</p> <p>The update takes the form:</p> <p>\\(\\delta_{t} = R_{t+1} + \\gamma \\hat{v}(S_{t+1},w_t) - \\hat{v}(S_t,w_t)\\)</p> <p>\\(w_{t+1} = w_{t} + \\alpha\\delta_{t}z_{t} + \\alpha( w_{t}^\\top x_{t} - w_{t-1}^\\top x_{t})(z_{t} - x_{t})\\)</p> <p>When we implement the true online TD(\u03bb) algorithm, we save computation by saving the result of \\(vo=w_{t-1}^\\top x_{t}\\) instead of storing \\(w_{t-1}\\).</p> <pre><code>class trueTD\u03bb(vMRP):\n    def __init__(self, \u03bb=.5, **kw):\n        super().__init__(**kw)\n        self.\u03bb = \u03bb\n\n    def step0(self):\n        self.z = self.w*0\n        self.vo = 0\n    # ----------------------------- \ud83c\udf16 online learning ----------------------    \n    def online(self, s, rn,sn, done, *args): \n        \u03b1, \u03b3, \u03bb = self.\u03b1, self.\u03b3, self.\u03bb\n\n        self.v = self.V(s)\n        self.vn= self.V(sn)*(1-done)\n        \u03b4 = rn + \u03b3*self.vn - self.v\n        self.z = \u03bb*\u03b3*self.z + (1-\u03b1*\u03bb*\u03b3*self.z.dot(s))*s\n\n        self.w += \u03b1*(\u03b4 + self.v - self.vo )*self.z - \u03b1*(self.v - self.vo)*s\n        self.vo = self.vn\n</code></pre> <pre><code>trueTD\u03bbwalk = trueTD\u03bb(env=vrandwalk(), \u03b1=.03, \u03bb=.8, episodes=100, v0=.5, **demoV()).interact(label='TD learning')\n</code></pre> <p></p> <pre><code>trueTD\u03bbwalk = trueTD\u03bb(env=vrandwalk(), \u03b1=.03, \u03bb=.4, episodes=100, v0=.5, **demoV()).interact(label='TD learning')\n</code></pre> <p></p> <pre><code>TDwalk = TD(env=vrandwalk(), episodes=100, \u03b1=.03,  v0=.5, **demoV()).interact(label='TD learning')\n</code></pre> <p></p> <p>As we can see true online TD(\u03bb) can converge faster than TD. Again, similar to TD(\u03bb) this leads to the jittery behaviour that usually entails reducing the learning rate, but we can achieve more updates in less episodes in general. The other downside is that we have to tune the hyper parameter \u03bb which can take more effort. This leads to our next set of experiments in order to study how the algorithms reacts to both \u03bb and \u03b1 and whether there is a sweet spot that one can aim for.</p>"},{"location":"unit5/lesson15/lesson15.html#model-selection-for-truetd-tuning-and","title":"Model selection for trueTD\u03bb: Tuning \u03bb and \u03b1","text":"<p>We will test our true online TD on the random walk with function approximation. Due to the dot product, some of the learning rate \u03b1 might be too much for the algorithm when using high \u03bb values, and they will cause overflow. This is not a problem as we are just testing the limits of the algorithm, but to avoid doing so, we added a conditional statement to limit the values of \u03b1 for high \u03bb values.</p> <pre><code>def TD\u03bb_MC_Walk_\u03b1compare(algorithm=TD\u03bb, label='TD(\u03bb)', runs=10):\n\n    steps0 = list(np.arange(.001,.01,.001))\n    steps1 = list(np.arange(.011,.2,.02))\n    steps2 = list(np.arange(.25,1.,.05))\n\n    \u03b1s = np.round(steps0 +steps1 + steps2, 2)\n    #\u03b1s = np.arange(0,1.05,.1) # quick testing\n\n    plt.xlim(-.02, 1)\n    plt.ylim(.24, .56)\n    plt.title('%s RMS error Average over 19 states and first 10 episodes'%label)\n    for \u03bb in [0, .1, .4, .8, .9, .95, .975, .99, 1]:\n        end=34 if \u03bb&lt;.975 else (-3 if \u03bb&lt;.99 else -10)\n        compare = Compare(algorithm=algorithm(env=vrandwalk_(), v0=0, \u03bb=\u03bb, episodes=10), \n                                  runs=runs, \n                                  hyper={'\u03b1':\u03b1s[:end]}, \n                                  plotE=True).compare(label='\u03bb=%.3f'%\u03bb)\n\n    if algorithm==trueTD\u03bb:\n        compare = Compare(algorithm=MC(env=vrandwalk_(), v0=0, episodes=10), \n                                  runs=runs, \n                                  hyper={'\u03b1':\u03b1s}, \n                                  plotE=True).compare(label='MC \u2261 TD(\u03bb=1)', frmt='-.')\n</code></pre> <p>Ok let us now apply the TD(\u03bb) on our usual random walk to see if it behaves as expected. We are using the default vectroised Grid which is equivalent to the tabular case. </p> <pre><code>TD\u03bb_MC_Walk_\u03b1compare(TD\u03bb, runs=5)\n</code></pre> <pre><code>100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|34/34\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|34/34\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|34/34\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|34/34\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|34/34\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|34/34\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|31/31\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|24/24\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|24/24\n</code></pre> <p></p> <p>Ok that seems good and is follows to the usual pattern of n-step algorithm. Note that we have executed for a couple of runs which explains the overshooting of the figure boundaries. Let us now contrast TD(\u03bb) and true online TD(\u03bb) on the same random walk problem. </p> <pre><code>def TD\u03bb_vs_trueonlineTD\u03bb(runs):\n    plt.figure(figsize=[15,4]).tight_layout()\n\n    plt.subplot(121); TD\u03bb_MC_Walk_\u03b1compare(TD\u03bb, label='TD(\u03bb)', runs=runs)\n    plt.subplot(122); TD\u03bb_MC_Walk_\u03b1compare(trueTD\u03bb, label='true online TD(\u03bb)', runs=runs)\n</code></pre> <p>This time we will run both for longer runs.</p> <pre><code>TD\u03bb_vs_trueonlineTD\u03bb(runs=5)\n</code></pre> <pre><code>100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|34/34\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|34/34\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|34/34\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|34/34\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|34/34\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|34/34\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|31/31\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|24/24\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|24/24\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|34/34\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|34/34\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|34/34\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|34/34\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|34/34\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|34/34\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|31/31\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|24/24\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|24/24\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|34/34\n</code></pre> <p></p>"},{"location":"unit5/lesson15/lesson15.html#notes-on-the-convergence-of-td-with-function-approximation","title":"Notes on the convergence of TD with Function Approximation","text":"<p>As we can see, the true online has more stretched curves indicating that they are more stable with a wider range of learning rates. This is due to the equivalence between the online and offline cases, which guarantees convergence and more stability than TD(\u03bb). </p> <p>Note that TD(\u03bb)is proven to converge for the offline case, not the online case. True online TD(\u03bb) also performs slightly better than TD(\u03bb). Note, of course, that when \u03bb=0, we go back to our usual TD algorithm. Finally, note that TD(0) with linear function approximation is proven to converge under the usual conditions of stochastic approximation theory 2.7 the learning rate \u03b1 ($\\sum_{i=1}^{\\infty}\\alpha_i = \\infty $ and $\\sum_{i=1}<sup>{\\infty}\\alpha_i</sup>2 &lt;\\infty $), see page 206 Proof of Convergence of Linear TD(0) in the book.</p>"},{"location":"unit5/lesson15/lesson15.html#control-with-eligibility-traces","title":"Control with Eligibility Traces","text":"<p>We move next to the control realm and write a Sarsa(\u03bb) with an approximate value function. The idea is similar to Sarsa(0) that we have written in the previous lesson (we called is just Sarsa there). We need, however, this time to take care of an extra variable that holds the eligibility traces, which has a size identical to the weights, so each action has its own eligibility trace. </p> <p>The eligibility traces need to be initialised at the start of each episode, and we achieve that by using our step0 function. The update is per equation 12.5 in the book, but we must apply it for the eligibility trace corresponding to the current action.</p> <p>Note that the book assumes that we concatenate all the eligibility traces by encoding a large state representation that assigns 0 to all components that do not correspond with the current action. We felt that this was vague and made the implementation unclear.</p>"},{"location":"unit5/lesson15/lesson15.html#sarsa","title":"Sarsa(\u03bb)","text":"<pre><code>class Sarsa\u03bb(vMDP):\n    def __init__(self, \u03bb=.5, **kw):\n        super().__init__(**kw)\n        self.\u03bb = \u03bb\n        self.step = self.step_an # for Sarsa we want to decide the next action in time step t\n\n    def step0(self):\n        self.Z = self.W*0\n    # ----------------------------------------\ud83c\udf16 online learning ----------------------------------------\n    def online(self, s, rn,sn, done, a,an):\n        self.Z[a] = self.\u03bb*self.\u03b3*self.Z[a] + self.\u0394Q(s)\n        self.W[a] += self.\u03b1*(rn + (1-done)*self.\u03b3*self.Q(sn,an)- self.Q(s,a))*self.Z[a]\n</code></pre> <pre><code>sarsa = Sarsa(env=vgrid(), \u03b1=.05, episodes=20, seed=10, **demoQ()).interact()\n</code></pre> <pre><code>sarsa.underhood = 'maxQ'\nsarsa.render()\n</code></pre> <pre><code>sarsa.Q_(10,1)\n</code></pre> <pre><code>array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 2.50000000e-02, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 6.00209380e-05, 6.10730839e-03,\n       5.14226101e-02, 3.25250643e-01, 1.58015574e+00, 5.59873331e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00])\n</code></pre> <pre><code>sarsa = Sarsa(env=vgrid(reward='reward1'), \u03b1=0.5, episodes=200, seed=0, **demoQ()).interact()\n</code></pre> <pre><code>sarsa\u03bb = Sarsa\u03bb(env=vgrid(reward='reward1'), episodes=20, seed=10, **demoQ()).interact()\n</code></pre> <pre><code>sarsa\u03bb = Sarsa\u03bb(env=vmaze(reward='reward1'), \u03bb=.1, episodes=200, seed=10, **demoQ()).interact()\n</code></pre> <pre><code>sarsa\u03bb = Sarsa\u03bb(env=vmaze(reward='reward1'), \u03bb=.5, episodes=20, seed=10, **demoQ()).interact()\n</code></pre> <pre><code>sarsa\u03bb = Sarsa\u03bb(env=vmaze(reward='reward1'), \u03bb=.9, episodes=20, seed=10, **demoQ()).interact()\n</code></pre> <p>Note how some of the cells arrows that represent the policy is being updated without being visited in the current time step this is due to the effect of eligibility traces accumulating states in previous steps that are eligible for updating.</p>"},{"location":"unit5/lesson15/lesson15.html#true-online-sarsa","title":"True Online Sarsa(\u03bb)","text":"<p>We implement the True online Sarsa(\u03bb) algorithm in this section. This algorithm guarantees an equivalence between the online and the offline cases, and it is hailed as an important step towards implementing a sound multi-step control algorithm that is both efficient and sound. The main drawback that we have is that it can only be applied to a linear approximation, not to any general one as Sarsa(\u03bb), but it is a step in the right direction until researchers find a way to do it also for the nonlinear functions as well (such as neural networks). We need to handle an eligibility trace variable, the size of which is identical to the weights, so each action has its own eligibility trace. The update rule is given in equation 12.11, and its justification/deduction can be read from section 12.5. Note that in our implementation. </p> <ul> <li>qo is an old action-value estimation for the current state</li> <li>qn is the action-value estimation for the next    state</li> <li>q  is the action-value estimation for the current state</li> </ul> <pre><code>class trueSarsa\u03bb(vMDP):\n    def __init__(self, \u03bb=.5, **kw):\n        super().__init__(**kw)\n        self.\u03bb = \u03bb\n        self.step = self.step_an # for Sarsa we want to decide the next action in time step t\n\n    def step0(self):\n        self.Z = self.W*0\n        self.qo = 0\n    # ----------------------------------------\ud83c\udf16 online learning ----------------------------------------\n    def online(self, s, rn,sn, done, a,an):\n\n        \u03b1, \u03b3, \u03bb = self.\u03b1, self.\u03b3, self.\u03bb\n\n        self.q = self.Q(s,a)\n        self.qn= self.Q(sn,an)*(1-done)\n        \u03b4 = rn + \u03b3*self.qn - self.q\n        self.Z[a] = \u03bb*\u03b3*self.Z[a] + (1-\u03b1*\u03bb*\u03b3*self.Z[a].dot(s))*s\n\n        self.W[a] += \u03b1*(\u03b4 + self.q - self.qo )*self.Z[a] - \u03b1*(self.q - self.qo)*s\n        self.qo = self.qn\n</code></pre> <pre><code>truesarsa\u03bb = trueSarsa\u03bb(env=vmaze(reward='reward1'), \u03bb=.99, \u03b1=.1, episodes=20, seed=20, **demoQ()).interact()\n</code></pre> <p></p> <pre><code>sarsa\u03bb = Sarsa\u03bb(env=vmaze(reward='reward1'), \u03bb=.99,  \u03b1=.1, episodes=20, seed=20, **demoQ()).interact()\n</code></pre> <p></p> <p>Note how true online Sarsa(\u03bb) performs a bit better than Sarsa(\u03bb) which in turn performs better than Sarsa(0) which has been covered in previous lessons.</p> <pre><code>sarsa = Sarsa(env=vmaze(reward='reward1'), episodes=20, seed=20, **demoQ()).interact()\n</code></pre> <p></p> <p>However, this can be compensated by increasing \u03b1 from .1 (the default value) to a higher value, such as .8 or .9. Note that \u03bb also affects the best value for \u03b1; often, we cannot set both to high values as we saw in the hyperparameter study that we performed on the random walk problem. </p> <pre><code>sarsa = Sarsa(env=vmaze(reward='reward1'), \u03b1=.9, episodes=20, seed=20, **demoQ()).interact()\n</code></pre> <p></p> <p>Let us now apply the MountainCarTiledRuns on the trueSarsa\u03bb to see how it behaves with different \u03bb values.</p> <pre><code>def MountainCarTiledRuns_\u03bb(runs=20, algo=trueSarsa\u03bb, env=tiledMountainCar):\n\n    plt.title(algo.__name__+' on mountain car: comparison of \u03bb and \u03b1 with the same 8x8x8 tilings')\n\n    for \u03bb, \u03b1 in zip([0, .8], [.5, .3]):\n        sarsaRuns = Runs(algorithm=algo(env=env(ntiles=8, ntilings=8), \u03bb=\u03bb, \u03b1=\u03b1/8,episodes=500, \u03b5=0), \n                         runs=runs, seed=1, plotT=True).interact(label='\u03bb=%.2f, \u03b1=%.2f/8'%(\u03b1,\u03b1))\n    plt.ylim((10**2,10**3))\n    plt.yscale('log')\n    plt.show()\n\nfigure_10_3_\u03bb = MountainCarTiledRuns_\u03bb\n</code></pre> <pre><code>%time figure_10_3_\u03bb(env=tiledMountainCar, algo=trueSarsa\u03bb)\n</code></pre> <pre><code>100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n100%|\u001b[90m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|20/20\n</code></pre> <p></p> <pre><code>CPU times: user 2min 22s, sys: 527 ms, total: 2min 23s\nWall time: 2min 23s\n</code></pre> <p>As we can see both n-step TD and true online TD(\u03bb)  have perform very similarly which confirms our starting premise, which is to design an algorithm that can perform n-step TD updates but which does not requires the steps delay.</p> <pre><code>def MountainCarTiledCompare_\u03bb(runs=3,  algo=trueSarsa\u03bb, env=IHTtiledMountainCar): # 10\n\n    xsticks = np.array([0, .5 , 1, 1.5, 2, 2.3])/8\n    plt.xticks(ticks=xsticks, labels=xsticks*8)\n    plt.yticks([180, 200, 220, 240, 260, 280, 300])\n    plt.ylim(170, 300)\n    plt.title('Steps per episode averaged over first 50 episodes for '+algo.__name__)\n\n    for \u03bb in [0, .68, .84, .92]:#, .96, .98, .99]:\n        if \u03bb&gt;=.0: \u03b1s = np.arange(.1,  1.8,  .1)\n        if \u03bb&gt;=.6: \u03b1s = np.arange(.1,  1.8,  .1)\n        if \u03bb&gt;=.8: \u03b1s = np.arange(.1,  1.8,  .1)\n        if \u03bb&gt;=.9: \u03b1s = np.arange(.1,  1.8,  .15)\n        if \u03bb&gt;=.98: \u03b1s = np.arange(.1,  .7,  .15)\n        if \u03bb&gt;=.98: \u03b1s = np.arange(.1,  .7,  .07)\n\n        Compare(algorithm=algo(env=env(ntilings=8, ntiles=8), \u03bb=\u03bb, episodes=50, \u03b5=0), runs=runs, \n                                  hyper={'\u03b1':\u03b1s/8}, \n                                  plotT=True).compare(label='\u03bb=%.2f'%\u03bb)\n    plt.xlabel(r'$\\alpha \\times 8$ since we used 8 tiles for each tilings')\n    plt.show()\n\n\nfigure_12_10 = MountainCarTiledCompare_\u03bb\n</code></pre>"},{"location":"unit5/lesson15/lesson15.html#comparing-sarsa-with-n-step-sarsa","title":"Comparing Sarsa(\u03bb) with n-step Sarsa","text":"<pre><code>%time MountainCarTiledCompare_\u03bb()\n</code></pre> <pre><code>100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|17/17\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|17/17\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|17/17\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|12/12\n</code></pre> <pre><code>CPU times: user 2min 23s, sys: 658 ms, total: 2min 24s\nWall time: 2min 25s\n</code></pre> <pre><code>from RLvec import *\n</code></pre> <pre><code>%time MountainCarTiledCompare_n()\n</code></pre> <pre><code>100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|14/14\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|16/16\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|17/17\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|16/16\n100%|\u001b[92m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m|13/13\n</code></pre> <pre><code>CPU times: user 5min 6s, sys: 707 ms, total: 5min 7s\nWall time: 5min 7s\n</code></pre> <p>Note that we have not used hashing and hence our range is different from the one stated in the book. Nevertheless, the pattern is maintained.</p>"},{"location":"unit5/lesson15/lesson15.html#conclusion","title":"Conclusion","text":"<p>In this lesson, we covered an important mechanism in RL that allows us to efficiently implement n-step methods online without waiting for n-steps, as we did earlier in the tabular case. Eligibility traces are very useful when we use the approximate function to estimate the value function in RL, but they can also be used in the tabular case. We have seen the classic Sarsa() for control, which can be used with any function approximation, including a neural network. Finally, we have seen how True Online Sarsa(\u03bb) behaves and that it is more efficient and holds a guarantee of the online and offline equivalency of TD(), which took several years to achieve in the RL development. True online TD(\u03bb) only applies to the linear function approximation, and there is no counterpart for it when we move to neural networks. In this lesson, you also saw how to deal with a continuous state space using function approximation and how to apply previous concepts to a more difficult control problem.</p>"},{"location":"unit5/lesson15/lesson15.html#your-turn","title":"Your turn","text":"<ol> <li>apply TD(\u03bb) and true online TD(\u03bb) on the 1000 tiled random walk environment (below we show you how to do that on the 1000 random walk), make sure to set \u03b1 Appropriately around .002.</li> <li>apply Sarsa(\u03bb) and true online Sarsa(\u03bb) on the tiled mountain car environment and confirm that its behaviour is similar to the n-step Sarsa.</li> </ol>"},{"location":"unit5/lesson15/lesson15.html#challenge","title":"Challenge","text":"<ol> <li>Implement a true online Q-learning algorithm. Note that you would need to restart the traces when the max action is not the same as the selected action since you cannot blame the pure greedy policy (the one Q-learning is learning about) for what the e-greedy policy took to explore, refer to the paper</li> <li>Can you think of a reason why we did not implement an offline version of the trueTD\u03bb?</li> <li>Implement and offline of trueTD\u03bb and TD\u03bb and compare between them and their online counterparts do you see a difference between them.</li> </ol>"},{"location":"unit5/lesson16/lesson16.html","title":"16. Nonlinear Approximation for Control","text":"<p>The notebook uses a library of functionality in RL that aims for simplicity and general insight into how algorithms work, these libraries are written from scratch using standard Python libraries (numpy, matplotlib etc.). Please note that you will need permission from the author to use the code for research, commercially or otherwise.</p>"},{"location":"unit5/lesson16/lesson16.html#lesson-15-non-linear-action-value-approximation-and-policy-gradient-methods-for-control","title":"Lesson 15: Non-linear Action-Value Approximation and Policy Gradient Methods for Control","text":"<p>Learning outcomes 1. build on previous concepts to come up with suitable and sometimes novel algorithms to solve a problem at hand 1. understand how to combine reinforcement learning with non-linear function approximators such as a neurla network to create a powerful framework that allows automatic agent learning by observation or self-play. 1. understand how a replay buffer helps us to come closer to supervised learning and appreciate the important role it plays in reaching convergence for difficult problems that involve image processing and reinforcement learning</p> <p>Reading: The accompanying reading of this lesson is chapter 16 of our text book available online here. Please note that we explain the ideas of this topic from a practical perspective and not from a theoretical perspective which is already covered in the textbook.</p> <p>This lesson deals with non-linear function approximation and RL, particularly with neural networks combined with RL. We have seen earlier how to deal with linear function approximations and the benefits that they bring in terms of a richer state representation. Now we move to an even richer but integrated and automatic feature extraction via a neural network. We can of course do feature extraction separate from training an RL, but integrating these two stages brings the benefit of extracting features that are particularly useful for the RL algorithm. One of the earliest examples of successfully using a neural network is Tesauro Backgammon TDGammon. Although the theoretical convergence guarantees do not extend from linear network cases to the non-linear function approximation, however, that did stop researchers from integrating both albeit in a few examples prior to the deep learning era, after which a vast number of models that uses both deep learning and reinforcement learning emerges with impressive results.</p> <p>We will utilise the idea of an experience replay buffer. We have already seen how to benefit from past experience on a large scale in the planning lesson, where we build a model of the environment. Here, we will not build a model, so we are still in the vicinity of model-free RL, but we will see how to execute a batch of updates instead of one update at a time. Training a neural network is an important addition to our arsenal of techniques because it brings RL closer to how we train supervised learning models, which is useful in two folds. The first is to train based on a mix of old and new experiences is useful for incorporating new experiences without forgetting old experiences. The second is to benefit from the built-in parallelisation of neural network training, which is greatly useful for more complex domains such as games and robotics. </p> <p>Note that the replay buffer dictates the choice of an off-policy algorithm, i.e. Q-learning, since the replayed experience is old and the agent will be learning from a policy different to the one it pursues.</p>"},{"location":"unit5/lesson16/lesson16.html#dependencies","title":"Dependencies","text":"<p>Please refer to libraries installation in the Introduction to find a list of libraries that you will need to install. If you are using the Azure VM then these packages will be already there, so you can get started and jum to the next section. If you cannot find a list of pip3 install in the IntroductionTOC notebook then re-download the notebook from minerva.</p> <p>Let us test if it is working</p> <pre><code># !conda list -f tensorflow\n# print('-------------check that the two commands give you the same version--------------------')\n# print('-------------otherwise it means you are using a kernel without a GPU------------------')\n# !pip3 show tensorflow \n</code></pre> <p>We can also check if our GPU is in use as follows.</p> <pre><code>from tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())\n# or\n# !python3 -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\n</code></pre> <pre><code>2025-02-23 16:56:04.777721: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n[name: \"/device:CPU:0\"\ndevice_type: \"CPU\"\nmemory_limit: 268435456\nlocality {\n}\nincarnation: 12904346926301707512\nxla_global_id: -1\n]\n</code></pre> <pre><code>from tensorflow.python.platform import build_info as tf_build_info\n# print(\"cudnn_version\",tf_build_info.build_info['cudnn_version'])\n# print(\"cuda_version\",tf_build_info.build_info['cuda_version'])\n</code></pre> <p>Ok, we are ready, let us get started...!</p> <p>To run this notebook on a remote Azure lab server without using remote desktop check this link</p>"},{"location":"unit5/lesson16/lesson16.html#non-linear-funciton-approximation-reinforcement-learning","title":"Non-linear Funciton Approximation Reinforcement Learning","text":"<p>First let us import the necessary libraries</p> <pre><code>%matplotlib inline\n</code></pre> <pre><code>from rl.rl import *\n</code></pre> <pre><code>import time\nimport os\nimport cv2\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nfrom numpy.random import rand\nfrom collections import deque\nfrom itertools import islice\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport matplotlib.animation as animation\n</code></pre> <pre><code>import tensorflow as tf\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import layers, losses\nfrom tensorflow.keras.datasets import fashion_mnist\nfrom tensorflow.keras.models import Model\n# from tensorflow.keras.callbacks import ProgbarLogger\n\nfrom IPython.display import clear_output\n\nfrom tensorflow.keras.layers import Conv2D, Dense, Flatten, Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n</code></pre> <p>Note that we are importing the basic MRP and MDP classes because this is suitable to create a non-linear MRP and MDP as we do not want to mix up non-linear solutions with linear solutions produced in previous lesson.</p>"},{"location":"unit5/lesson16/lesson16.html#rl-with-neural-networks","title":"RL with Neural Networks","text":"<p>It is time to extend our basic MRP class to handle function approximation using neural networks. Note that in their paper DeepMinds trained for 200M frames, we set the max_t_exp (used in stop_exp() function) to 2M due to hardware limitation. We can also make the stop_exp() tied to R-star that is specific to the application under consideration.</p>"},{"location":"unit5/lesson16/lesson16.html#buffer-implementation","title":"Buffer Implementation","text":"<p>If you are familiar with the concepts of deque then please skip to the next section. </p> <p>It is better to implement the buffer as queue because it guarantees an O(1) complexity for append() and pop() and it is preferred over the list which gives us a O(n) for adding and retrieving an item. In Python we can utilise the double queue structure which gives us the flexibility to add and retrieve from both ends of the queue. Below, we show a quick example. Note that the buffer will be overwritten when. the number of items exceeds its length. This is useful for us because we just want the buffer to overwritten with new experience after it s full and to be kept updated accordingly.</p> <pre><code>from collections import deque\nbuffer = deque(maxlen=5)\nbuffer.append(1)\nbuffer.append(2)\nbuffer.append(3)\nbuffer.append(4)\nprint(buffer)\n</code></pre> <pre><code>deque([1, 2, 3, 4], maxlen=5)\n</code></pre> <pre><code>buffer.append(5)\nbuffer.append(6)\nbuffer.append(7)\nprint(buffer)\n</code></pre> <pre><code>deque([3, 4, 5, 6, 7], maxlen=5)\n</code></pre> <p>To access the last item we use the usual indexing.</p> <pre><code>print(buffer[-1])\nbuffer.append(8)\nprint(buffer[-1])\n</code></pre> <pre><code>7\n8\n</code></pre>"},{"location":"unit5/lesson16/lesson16.html#random-sampling-form-the-buffer","title":"Random Sampling form the Buffer","text":"<p>Let us now take few random samples of size 2 from the buffer.</p> <pre><code>from random import sample\nfor i in range(3):\n    print(sample(buffer,2))\n</code></pre> <pre><code>[4, 8]\n[7, 8]\n[4, 6]\n</code></pre>"},{"location":"unit5/lesson16/lesson16.html#buffer-with-complex-element-tuples","title":"Buffer with Complex Element (Tuples)","text":"<p>Let us assume that we have a set of tuples each consists of (s,a,sn). In this case we can add these tuples as is. Below we show an example, we have represented actions as integers but states/observations as string to help identifying them visually, but bear in mind that they are going to be a more complex entities such as images.</p> <pre><code>buffer = deque(maxlen=4)\nbuffer.append(('2',1,'3'))\nbuffer.append(('3',2,'4'))\nbuffer.append(('4',2,'5'))\nbuffer.append(('5',1,'4'))\n\nprint(buffer)\n</code></pre> <pre><code>deque([('2', 1, '3'), ('3', 2, '4'), ('4', 2, '5'), ('5', 1, '4')], maxlen=4)\n</code></pre> <p>Now in order to sample we can directly sample from the buffer </p> <pre><code>batch = sample(buffer,3)\nprint(batch)\n</code></pre> <pre><code>[('2', 1, '3'), ('5', 1, '4'), ('3', 2, '4')]\n</code></pre> <p>However, the above is not useful, usually we want to place all the actions and states and next states in their own list to feed them as a batch into a neural network. To put all states and actions together each in its own list we can use zip</p> <pre><code>for i in zip(*batch): print(i)\n</code></pre> <pre><code>('2', '5', '3')\n(1, 1, 2)\n('3', '4', '4')\n</code></pre> <p>We can also convert them into a numpy array directly.</p> <pre><code>[np.array(i) for i in zip(*batch)]\n</code></pre> <pre><code>[array(['2', '5', '3'], dtype='&lt;U1'),\n array([1, 1, 2]),\n array(['3', '4', '4'], dtype='&lt;U1')]\n</code></pre> <pre><code>buffer = deque(maxlen=4)\nbuffer.append(('2',1,'3'))\nbuffer.append(('3',2,'4'))\nbuffer.append(('4',2,'5'))\nbuffer.append(('5',1,'4'))\nbuffer.append(('7',8,'6'))\n\nprint(buffer)\nsamples = [np.array(item) for item in zip(*sample(buffer,3))]\nprint(samples)\n</code></pre> <pre><code>deque([('3', 2, '4'), ('4', 2, '5'), ('5', 1, '4'), ('7', 8, '6')], maxlen=4)\n[array(['5', '7', '4'], dtype='&lt;U1'), array([1, 8, 2]), array(['4', '6', '5'], dtype='&lt;U1')]\n</code></pre> <p>sampling an empty batch</p> <pre><code>nbatch = 1\nsample(buffer, nbatch-1)\n</code></pre> <pre><code>[]\n</code></pre> <p>sampling last nbatch elements from the buffer</p> <pre><code>nbatch = 3\ndef slice_(buffer):\n    return list(islice(buffer,len(buffer)-nbatch,len(buffer)))\n</code></pre> <pre><code>slice_(buffer)\n</code></pre> <pre><code>[('4', 2, '5'), ('5', 1, '4'), ('7', 8, '6')]\n</code></pre>"},{"location":"unit5/lesson16/lesson16.html#neural-network-based-mrp","title":"Neural Network Based MRP","text":"<p>In this class we implement the basic functionality for dealing with creating, saving and loading deep learning models. In addition, we make these models the default functions used to obtain the value function via self.V_. We also adjust the stope_exp criterion so that the algorithm stops when a specific averaged reward is achieved or when a specific total number of steps (self.t_ not self.t) have been elapsed. This means also that we free ourselves from the notion of an episode, so our model can run as many episodes as it takes to achieve this total number of steps. We still can assign episodes=x to store metrics for last y episodes where y&lt;x. Note that nF is usually used in the Env(ironment) class but feature extraction is embedded the model itself in deep learning model so it is defined in the Deep_MRP class.</p> <pre><code>class nnMRP(MRP):\n    def __init__(self, \u03b3=0.99, nF=512, nbuffer=10000, nbatch=32, rndbatch=True,\n                 save_weights=1000,     # save weights every now and then\n                 load_weights=False,\n                 print_=True,\n                 **kw):\n\n        super().__init__(\u03b3=\u03b3, **kw)\n        self.nF = nF   # feature extraction is integrated within the neural network model not the env\n\n        self.nbuffer  = nbuffer\n        self.nbatch   = nbatch\n        self.rndbatch = rndbatch\n\n        self.load_weights_= load_weights\n        self.save_weights_= save_weights # used to save the target net every now and then\n\n        self.update_msg = 'update %s network weights...........! %d'\n        self.saving_msg = 'saving %s network weights to disk...! %d'\n        self.loading_msg = 'loading %s network weights from disk...!'\n\n    def init(self):\n        self.vN = self.create_model('V')                      # create V deep network\n        if self.load_weights_: self.load_weights(self.vN,'V.weights.h5') # from earlier training proces   \n        self.V = self.V_\n\n    #-------------------------------------------Deep model related---------------------------\n    def create_model(self, net_str):\n        x0 = Input(self.env.reset().shape)#(84,84,1))#self.env.frame_size_)\n        x = Conv2D(32, 8, 4, activation='relu')(x0)\n        x = Conv2D(64, 4, 2, activation='relu')(x)\n        x = Conv2D(64, 3, 1, activation='relu')(x)\n        x = Flatten()(x)\n        x = Dense(self.nF, 'relu')(x)\n        x = Dense(1 if net_str=='V' else self.env.nA)(x) \n        model = Model(x0, x)\n        model.compile(Adam(self.\u03b1), loss='mse')\n        model.summary() if net_str != 'Qn' else None\n        model.net_str = net_str\n        return model\n\n    def load_weights(self, net, net_str ):\n        print(self.loading_msg%net_str)\n        loaded_weights = net.load_weights(net_str)\n        loaded_weights.assert_consumed()\n\n    def save_weights(self):\n        print(self.saving_msg%('V ',self.t_))\n        self.vN.save_weights('V.weights.h5')\n\n    #------------------------------------- value related \ud83e\udde0-----------------------------------\n    def V_(self, s, Vs=None):\n\n        # update the V network if Vs is passed\n        if Vs is not None: self.vN.fit(s, Vs, verbose=False); return None\n\n        # predict for one state for \u03b5greedy, or predict for a batch of states, copy to avoid auto-grad issues\n        return self.vN.predict(np.expand_dims(s, 0), verbose=0)[0] if len(s.shape)!=4 else np.copy(self.vN.predict(s, verbose=0)) \n\n    #-------------------------------------------buffer related----------------------------------\n    def allocate(self):\n        self.buffer = deque(maxlen=self.nbuffer)\n\n    def store_(self, s=None,a=None,rn=None,sn=None,an=None, done=None, t=0):\n        self.save_weights() if self.save_weights_ and self.t_%self.save_weights_==0 else None\n        self.buffer.append((s, a, rn, sn, done))\n\n    # deque slicing, cannot use buffer[-nbatch:]\n    def slice_(self, buffer, nbatch):\n        return list(islice(buffer, len(buffer)-nbatch, len(buffer)))\n\n    def batch(self):\n        # if nbatch==nbuffer==1 then (this should give the usual qlearning without replay buffer)\n        # sample nbatch tuples (each tuple has 5 items) without replacement or obtain latest nbatch from the buffer\n        # zip the tuples into one tuple of 5 items and convert each item into a np array of size nbatch \n\n        samples = sample(self.buffer, self.nbatch) if self.rndbatch else self.slice_(self.buffer, self.nbatch)\n        samples = [np.array(experience) for experience in zip(*samples)]\n\n        # generate a set of indices handy for filtering, to be used in online()\n        inds = np.arange(self.nbatch)\n\n        return samples, inds\n</code></pre>"},{"location":"unit5/lesson16/lesson16.html#neural-network-based-mdp","title":"Neural Network Based MDP","text":"<p>Now we create the MDP class which implements policy related functionality</p> <pre><code>class nnMDP(MDP(nnMRP)):\n\n    # update the target network every t_qNn steps\n    def __init__(self, create_vN=False, **kw):\n        super().__init__(**kw)\n        self.create_vN = create_vN\n\n    def init(self):\n        super().init() if self.create_vN else None                     # to create also vN, suitable for actor-critic\n        self.qN  = self.create_model('Q')                              # create main policy network\n        self.qNn = self.create_model('Qn')                             # create target network to estimate Q(sn)\n\n        self.load_weights(self.qN,'Q.weights.h5') if self.load_weights_ else None # from earlier training proces\n        self.load_weights(self.qNn,'Q.weights.h5') if self.load_weights_ else None # from earlier training proces\n\n        self.Q = self.Q_\n\n    def save_weights(self):\n        super().save_weights() if self.create_vN else None             # save vN weights, for actor-critic\n        print(self.saving_msg%('Q', self.t_))\n        self.qN.save_weights('Q.weights.h5')\n\n    def set_weights(self, net):\n        print(self.update_msg%(net.net_str, self.t_))\n        net.set_weights(self.qN.get_weights())\n\n    #------------------------------------- policies related \ud83e\udde0-----------------------------------\n    def Q_(self, s, Qs=None):\n        # update the Q networks if Qs is passed\n        if Qs is not None: self.qN.fit(s, Qs, verbose=0); return None\n\n        # predict for one state for \u03b5greedy, or predict for a batch of states, \n        # copy to avoid auto-grad issues\n        return self.qN.predict(np.expand_dims(s, 0), verbose=0)[0] if len(s.shape)!=4 \\\n    else np.copy(self.qN.predict(s, verbose=0))\n\n    def Qn(self, sn, update=False):\n        # update the Qn networks if Qn is passed\n        if update: self.set_weights(self.qNn); return None\n        return self.qNn.predict(sn, verbose=0)\n</code></pre> <p>Bellow we double-check that the policies assigned via class inheritance is suitable. MRP should have a stationary policy while MDP has an \u03b5greedy policy.</p> <pre><code>print(nnMRP().policy)\nprint(nnMDP().policy)\n</code></pre> <pre><code>&lt;bound method MRP.stationary of &lt;__main__.nnMRP object at 0x14c455b80&gt;&gt;\n&lt;bound method MDP.&lt;locals&gt;.MDP.\u03b5greedy of &lt;__main__.nnMDP object at 0x14c455b50&gt;&gt;\n</code></pre>"},{"location":"unit5/lesson16/lesson16.html#deep-q-learning-architecture","title":"Deep Q-Learning Architecture","text":"<p>Note that we need to set \u03b5 here otherwise it will be set by default to .1 in the parent class.</p> <pre><code>class DQN(nnMDP):\n    def __init__(self, \u03b1=1e-4, t_Qn=1000, **kw): \n        print('--------------------- \ud83e\udde0  DQN is being set up \ud83e\udde0 -----------------------')\n        super().__init__(**kw)\n        self.\u03b1 = \u03b1\n        self.store = True\n        self.t_Qn = t_Qn\n\n    #------------------------------- \ud83c\udf16 online learning ---------------------------------\n    # update the online network in every step using a batch\n    def online(self, *args):\n        # no updates unless the buffer has enough samples\n        if len(self.buffer) &lt; self.nbuffer: return\n\n        # sample a tuple batch: each component is a batch of items \n        # (s and a are sets of states and actions)\n        (s, a, rn, sn, dones), inds = self.batch() \n\n        # obtain the action-values estimation from the two networks and \n        # ensure target is 0 for terminal states\n        Qs = self.Q(s)\n        Qn = self.Qn(sn); Qn[dones] = 0\n\n        # now dictate what the target should have been as per the Q-learning update rule\n        Qs[inds, a] = self.\u03b3*Qn.max(1) + rn\n\n        # then update both\n        self.Q(s, Qs)\n        self.Qn(sn, self.t_%self.t_Qn==0)\n</code></pre> <p>Let us now deal with Grid as a game and state as images. We will try first just to run as usual without sampling but with randomisation form the buffer. To do so, we simply assign the same number for the nbuffer and nbatch which will force the algorithm to pass all of what it has in the buffer albeit randomised in terms of order.</p> <p>To properly force the algorithm not to randomise the samples, we can pass this flag explicitly.</p> <pre><code># deal with the Grid states as images and learn from them to navigate it\n# please be patient as it takes much longer to learn from pixles\n\n\n%time nqlearn = DQN(env=iGrid(style='maze', reward='reward_1'), seed=10, episodes=100, \\\n                    rndbatch=False, t_Qn=500, nbuffer=32, nbatch=32, **demoGame()).interact() \n</code></pre> <pre><code>CPU times: user 3h 32min 14s, sys: 44min 36s, total: 4h 16min 50s\nWall time: 3h 11min 34s\n</code></pre> <p></p> <pre><code>nqlearn.interact(resume=True, episodes=103, **demoGame())\n</code></pre> <pre><code>&lt;__main__.DQN at 0x14a0f16d0&gt;\n</code></pre> <p></p> <pre><code>nqlearn.interact(resume=True, episodes=110, view=10, **demoGame())\n</code></pre> <pre><code>&lt;__main__.DQN at 0x14a0f16d0&gt;\n</code></pre> <p></p> <pre><code># deal with the Grid states as images and learn from them to navigate it\n%time nqlearn = DQN(env=iGrid(style='maze', reward='reward_1'), seed=10, \\\n                    episodes=100, nbuffer=1, nbatch=1, **demoGame()).interact() \n</code></pre> <pre><code>CPU times: user 6h 47min 50s, sys: 58min 54s, total: 7h 46min 45s\nWall time: 6h 44min 8s\n</code></pre> <p></p> <pre><code># deal with the Grid states as images and learn from them to navigate it\n%time nqlearn = DQN(env=iGrid(style='maze', reward='reward_1'), seed=10, \\\n                    episodes=100, nbuffer=8, nbatch=8, **demoGame()).interact()\n</code></pre> <pre><code>CPU times: user 4h 57min 59s, sys: 49min 30s, total: 5h 47min 30s\nWall time: 4h 39min 39s\n</code></pre> <p></p> <pre><code># deal with the Grid states as images and learn from them to navigate it\n%time nqlearn = DQN(env=iGrid(style='maze', reward='reward_1'), \\\n                    seed=10, episodes=100, nbuffer=1000, nbatch=32, **demoGame()).interact() \n</code></pre> <pre><code>CPU times: user 1h 49min 58s, sys: 15min 27s, total: 2h 5min 26s\nWall time: 1h 33min 50s\n</code></pre> <p></p> <pre><code>nqlearn.policy\n</code></pre> <pre><code>&lt;bound method MDP.&lt;locals&gt;.MDP.\u03b5greedy of &lt;__main__.DQN object at 0x19819fc20&gt;&gt;\n</code></pre> <pre><code>print(nqlearn.env.img.shape)\n</code></pre> <pre><code>(50, 84, 1)\n</code></pre> <pre><code>%time nqlearn.interact(resume=True, episodes=151, **demoGame())\n</code></pre> <pre><code>CPU times: user 23min 48s, sys: 3min 35s, total: 27min 24s\nWall time: 19min 47s\n\n\n\n\n\n&lt;__main__.DQN at 0x19819fc20&gt;\n</code></pre> <p></p> <pre><code>nqlearn.env.render__()\n</code></pre> <p></p> <pre><code># nqlearn.ep -=1\n# nqlearn.plotT = False\n# nqlearn.visual = True\n# nqlearn.underhood='maxQ' # uncomment to see also the policy\nnqlearn.interact(resume=True, episodes=159, plotT=True)\n</code></pre> <pre><code>&lt;__main__.DQN at 0x19819fc20&gt;\n</code></pre> <p></p> <pre><code>plt.imread('img/img0.png').shape\n</code></pre> <pre><code>(231, 387, 4)\n</code></pre> <pre><code>plt.imshow(nqlearn.env.img)\n</code></pre> <pre><code>&lt;matplotlib.image.AxesImage at 0x197c04ec0&gt;\n</code></pre> <p></p> <pre><code>163*278*4\n</code></pre> <pre><code>181256\n</code></pre> <pre><code>nqlearn.env.img=None\nnqlearn.env.i=0\nnqlearn.ep -=1\nnqlearn.plotT = False\nnqlearn.visual = True\n# nqlearn.underhood='maxQ' # uncomment to see also the policy\nnqlearn.interact(train=False, **demoGame())\n</code></pre> <pre><code>&lt;__main__.DQN at 0x19819fc20&gt;\n</code></pre> <p></p>"},{"location":"unit5/lesson16/lesson16.html#double-dqn-learning","title":"Double DQN Learning","text":"<pre><code>class DDQN(DQN):\n    def __init__(self, \u03b1=1e-4, **kw):\n        print('----------- \ud83e\udde0 Double DQN is being set up \ud83e\udde0 ---------------------')\n        super().__init__(**kw)\n        self.\u03b1 = \u03b1\n        self.store = True\n    #--------------------------- \ud83c\udf16 online learning -----------------------------\n    def online(self, *args):\n        # sample a tuple batch: each component is a batch of items \n        #(ex. s is a set of states, a is a set of actions) \n        (s, a, rn, sn, dones), inds = self.batch()\n\n        # obtain the action-values estimation from the two networks \n        # and make sure the target is 0 for terminal states\n        Qs = self.Q(s)\n        Qn = self.Qn(sn); Qn[dones] = 0\n\n        # now dictate what the target should have been as per the *Double* Q-learning \n        # update rule, this is where the max estimations are decoupled from the max \n        # action selections\n        an_max = self.Q(sn).argmax(1)\n        Qs[inds, a] = self.\u03b3*Qn[inds, an_max] + rn\n\n        # update\n        self.Q(s, Qs) \n        self.Qn(sn, self.t_%self.t_Qn==0)\n</code></pre> <pre><code># %time ddqlearn = DDQN(env=Gridi(style='maze'), seed=10, episodes=2, max_t=200, \\\n#                       max_t_exp=2000, nbuffer=100, nbatch=32, **demoGame()).interact() \n</code></pre>"},{"location":"unit5/lesson16/lesson16.html#conclusion","title":"Conclusion","text":"<p>In this lesson you saw how to deal with a continuous state space using function approximation and how to apply previous concepts on a more difficult control problems. We have built a wrapper class that allowed us to take advantage of the environments provided by OpenAI Gym library. We have duplicated what we have done in the previous lesson in order to 1. examine that our previous environment worked well, 2. see an example of how to deal with OpenAI Gym environment. </p> <p>You have also seen how to combine deep learning with reinforcement learning to create a powerful model that is capable of learning from watching a game. This is really interesting since it opens up the possibility for enormous applications where an agent can watch and learn to arrive to a complex behaviour that allows it to accomplish a task or win a competition. </p>"},{"location":"unit5/lesson16/lesson16.html#discussion-and-activity","title":"Discussion and Activity","text":"<p>Read the following classic Nips paper and Nature paper and discuss it in the discussion forum.</p>"},{"location":"unit5/lesson16/lesson16.html#your-turn","title":"Your turn","text":"<ol> <li> <p>try to apply the same concept on other simple environments provided by Gym such as the acrobot.</p> </li> <li> <p>apply DQN on another Atari game such as SpaceInvaders or Breakout and report the score that you got in the discussion forum.</p> </li> </ol>"},{"location":"unit5/lesson16/lesson16.html#challenge","title":"Challenge++","text":"<ol> <li>check the implementation of the deep network in the tutorial and try to integrate it into the provided infrastructure.</li> </ol>"},{"location":"unit5/lesson17/lesson17.html","title":"17. Application on Robot Navigation","text":"<p>The notebook uses a library of functionality in RL that aims for simplicity and general insight into how algorithms work, these libraries are written from scratch using standard Python libraries (numpy, matplotlib etc.). Please note that you will need permission from the author to use the code for research, commercially or otherwise.</p>"},{"location":"unit5/lesson17/lesson17.html#lesson-16-rl-on-robotics","title":"Lesson 16: RL on Robotics","text":"<p>Learning outcomes 1. understand how to create a simple Robot environment that links to Gazebo 1. understand how to deal with the simulated environment in a grid world fashion 1. appreciate the intricacy of applying RL to the robotics domain 1. build on previous concepts to come up with a suitable solution to a problem at hand 1. understand how a replay buffer helps us to come closer to supervised learning and appreciate the important role it plays in reaching convergence 1. understand how to combine deep reinforcement learning with deep learning to create a powerful framework that allows automatic agent learning by observation or self-play. 1. understand how a replay buffer helps us to come closer to supervised learning and appreciate the important role it plays in reaching convergence for difficult problems that involve image processing and reinforcement learning</p> <p>Reading: We cover applications of RL on robotics based on previouse units which you can refere to.</p> <p>In this notebook, we deal with how to set up a robot environment class that can handle the publish-subscribe on topics and deal with services in ROS. We must have ROS and Gazebo installed and set up on our machine. The code is a starting point and is not fully developed. You will need to write the necessary functionality to address a specific requirement. The main idea of tackling robotics applications in a Jupyter notebook is to utilise the provided infrastructure and libraries of code we covered in earlier units.</p>"},{"location":"unit5/lesson17/lesson17.html#instructions-for-running-experiments-on-azure-vm","title":"Instructions for running Experiments on Azure VM","text":"<p>The VM usage limit is set to 40 hours. Please turn off the machine when not using it to preserve your time. The VM is not set to disconnect you automatically so that you can leave it training the robot continuously for assessment 2.</p> <p>Please turn off the screen save and screen lock in Xfce(Applications-&gt;Settings-&gt;Light Locker) as it may cause the machine to become not responsive, which in turn, causes Azure to stop it automatically.</p> <p>If the VM becomes corrupted for some reason, then you can reimage it by going to Azure Lab page and selecting the three dots, then reimage. Reimage will reset the VM to its initial settings but it causes all data you have on the machine to be lost. You are advised to backup your data, you may want to use OneDrive or other backup methods.</p> <p>Note If Gazebo stops for any reason, the provided code has a try-except statement (in lesson 4 Monte Carlo) that you can activate (comment in). It allows you to continue training even if the robot becomes not responsive without having to restart the experiment.</p> <p>If you are running out of time, please let your tutor know in advance and they will try to increase your VM time allowance.</p>"},{"location":"unit5/lesson17/lesson17.html#turtlebot3","title":"Turtlebot3","text":"<p>Install turtlebot3 packages. If you are in our VM, they would have been already installed.</p>"},{"location":"unit5/lesson17/lesson17.html#more-realistic-simulation-running-gazebo","title":"More realistic Simulation: Running Gazebo","text":"<p>You will need to launch a gazebo environment with Turtlebot3 in it. So long as the /scan(LaserScan), /odom (Odometry) and /cmd_vel(Twist) topics are available, the environment should work fine. Our target is to build an environment that will allow us to use the algorithms we developed in earlier units directly.</p> <p>To launch an environment, you should open a terminal and run the following command</p> <p>ros2 launch turtlebot3_gazebo turtlebot3_house.launch</p> <p>Note that you cannot do that here because that will block the notebook from executing other code.  You must select restart your notebook kernel, ex. Kernel-&gt; Restart and Run ALL, whenever you want to re-establish a connection with the environment.</p> <pre><code>%matplotlib inline\n</code></pre> <pre><code>import rclpy as ros\nfrom rclpy.node import Node\n\nfrom geometry_msgs.msg import Twist\nfrom nav_msgs.msg  import Odometry\nfrom sensor_msgs.msg import LaserScan\nfrom std_srvs.srv import Empty\nfrom gazebo_msgs.srv import SpawnModel\n\nimport numpy as np\nfrom numpy import Inf\nfrom random import randint\nfrom math import atan2, atan, pi\nimport matplotlib.pyplot as plt\nros.init()\n</code></pre> <pre><code># !export PATH=\"home/rl/.local/bin:/opt/ros/humble/share:$PATH\"\n# !python3 -c \"import rclpy as ros; ros.init()\"\n</code></pre>"},{"location":"unit5/lesson17/lesson17.html#moving-the-robotactions","title":"Moving the robot(Actions)","text":"<p>We start by controlling the robot only</p> <p>Go to /opt/ros/humble/share/turtlebot3_gazebo/models/turtlebot3_burger/model.sdf and adjust the 30 to 5 for the odomotry turtlebot3_diff_drive. This will change its frequency from 30 to 5 so that it aligns with the scanner frequency.</p> <pre><code>def name(): return 'node'+str(randint(1,1000))\n</code></pre> <pre><code>class Env(Node):\n\n# initialisation--------------------------------------------------------------\n    # frequency: how many often (in seconds) the spin_once is invoked, or the publisher is publishing to the /cmd_vel\n    def __init__(self, name=name(), \n                 freq=1/20, n=28, \n                 speed=.5, \u03b8speed=pi/5, \n                 rewards=[30, -10, 0, -1],\n                 verbose=False):\n        super().__init__(name)\n\n        self.freq = freq\n        self.n = n\n\n        self.speed = speed\n        self.\u03b8speed = round(\u03b8speed,2)\n\n        self.robot = Twist()\n        self.rewards = rewards\n        self.verbose = verbose\n\n        # do not change----------------------------------------------------\n        self.x = 0 # initial x position\n        self.y = 0 # initial y position\n        self.\u03b8 = 0 # initial \u03b8 angle\n        self.scans = np.zeros(60) # change to how many beams you are using\n        self.t = 0\n\n        self.tol = .6  # meter from goal as per the requirement (tolerance)\n        self.goals =  [[2.0, 2.0], [-2.0, -2.0]]\n        # -----------------------------------------------------------------\n\n        self.controller = self.create_publisher(Twist, '/cmd_vel', 0)\n        self.timer = self.create_timer(self.freq, self.control)\n\n        self.scanner = self.create_subscription( LaserScan, '/scan', self.scan, 0)\n        self.odometr = self.create_subscription( Odometry, '/odom', self.odom, 0)\n\n        self.range_max = 3.5\n        self.range_min = .28               # change as you see fit\n\n\n        # establish a reset client \n        self.reset_world = self.create_client(Empty, '/reset_world')\n        while not self.reset_world.wait_for_service(timeout_sec=2.0):\n            print('world client service...')\n\n\n        # compatibility----------------------------------------------\n        nturns = 15 # number of turns robot takes to complete a full circle\n        resol = speed/2\n\n        \u03b8resol = 2*pi/nturns\n        dims = [4,4]\n        self.xdim = dims[0]  # realted to the size of the environment\n        self.ydim = dims[1]  # realted to the size of the environment\n\n        self.resol = round(resol,2)\n        self.\u03b8resol = round(\u03b8resol,2)\n\n        self.cols  = int(self.xdim//self.resol) +1   # number of grid columns, related to linear speed\n        self.rows  = int(self.ydim//self.resol) +1   # number of grid rows,    related to linear speed\n        self.orts  = int(2*pi//self.\u03b8resol)     +1   # number of angles,       related to angular speed\n\n        self.nC = self.rows*self.cols              # Grid size\n        self.nS = self.rows*self.cols*self.orts # State space size\n        self.nA = 3\n\n\n        self.Vstar = None # for compatibility\n        # --------------------------------------------------------------- \n        # self.rate = self.create_rate(30)\n        self.reset()\n\n        print('speed  = ', self.speed)\n        print('\u03b8speed = ', self.\u03b8speed)\n        print('freq   = ', self.freq)\n\n# sensing--------------------------------------------------------------\n    # odometry (position and orientation) readings\n    def odom(self, odoms):\n        self.x = round(odoms.pose.pose.position.x, 1)\n        self.y = round(odoms.pose.pose.position.y, 1)\n        self.\u03b8 = round(self.yaw(odoms.pose.pose.orientation),2) \n        self.odom = np.array([self.x, self.y, self.\u03b8])\n        if self.verbose: print('odom = ',  self.odom )\n\n    # laser scanners readings\n    def scan(self, scans):\n        self.scans = np.array(scans.ranges)\n        self.scans[scans==Inf] = self.range_max\n        # if self.verbose: print('scan = ', self.scans[:10].round(2))\n        if self.verbose: print('scan = ', np.r_[self.scans[-5:], self.scans[:5]].round(2))\n\n    # converting to the quaternion self.z to Euler\n    # see https://www.allaboutcircuits.com/technical-articles/dont-get-lost-in-deep-space-understanding-quaternions/#\n    # see https://eater.net/quaternions/video/intro\n\n    def yaw(self, orient):\n        x, y, z, w = orient.x, orient.y, orient.z, orient.w\n        yaw = atan2(2.0*(x*y + w*z), w*w + x*x - y*y - z*z)\n        return yaw if yaw&gt;0 else yaw + 2*pi # in radians, [0, 2pi]\n\n    # angular distance of robot to a goal.............................................\n    def \u03b8goal(self, goal):\n        xgoal, ygoal = self.goals[goal] \n        x, y  = self.x, self.y\n        \u03b8goal = atan2(abs(xgoal-x), abs(ygoal-y)) # anglegoal\n        # if \u03b8goal&lt;=0  \u03b8goal += 2*pi\n        return round(\u03b8goal, 2) # in radians, [0, 2pi]\n\n    # Eucleadian distance of robot to nearest goal......................................   \n    def distgoal(self):\n        dists = [Inf, Inf]        # distances of robot to the two goals\n        for goal, (xgoal, ygoal) in enumerate(self.goals):\n            dists[goal] = (self.x - xgoal)**2 + (self.y - ygoal)**2\n\n        dist = min(dists)         # nearest goal distance\n        goal = dists.index(dist)  # nearest goal index\n\n        if self.verbose: print('seeking goal ____________________', goal)\n        return round(dist**.5, 2), goal\n\n    # robot reached goal ...............................................................\n    def atgoal(self):\n        tol, x, y = self.tol,  self.x, self.y\n        atgoal = False\n        for xgoal, ygoal in self.goals:\n            atgoal = xgoal + tol &gt; x &gt; xgoal - tol  and  \\\n                     ygoal + tol &gt; y &gt; ygoal - tol\n\n            if atgoal: print('Goal has been reached woohoooooooooooooooooooooooooooooo!!'); break\n        return atgoal\n\n    # robot hits a wall...................................................................\n    def atwall(self, rng=5):\n        # check only 2*rng front scans for collision, given the robot does not move backward\n        return np.r_[self.scans[-rng:], self.scans[:rng]].min() &lt; self.range_min \n        #return self.scans.min()&lt;self.range_min\n\n    # reward function to produce a suitable policy..........................................\n    def reward(self, a, imp=2):\n        stype = [self.atgoal(), self.atwall(), a==1, a!=1].index(True)\n\n        dist, goal = self.distgoal()\n        \u03b8goal = self.\u03b8goal(goal)\n\n        # get angular distance to reward/penalise robot relative to its orientation towards a goal\n        \u03b8dist = abs(self.\u03b8 - \u03b8goal)\n        if goal==1: \u03b8dist -= pi\n        \u03b8dist = round(abs(\u03b8dist),2)\n\n        reward = self.rewards[stype] \n        if stype: reward -= imp*(dist+\u03b8dist) \n\n        if self.verbose: \n            print('reward components=', \n                  'Total reward=', reward, \n                  'state reward=', self.rewards[stype],\n                  'goal dist=', dist, \n                  '|\u03b8-\u03b8goal|=', \u03b8dist)\n                #   '\u03b8robot=', self.\u03b8, \n                #   '\u03b8goal =', \u03b8goal, \n\n        # reset without restarting an episode if the robot hits a wall\n        if stype==1: self.reset() \n\n        return reward, stype==0, stype==1\n\n# State representation-------------------------------------------------\n   # change this to generate a suitable state representation\n    def s_(self):\n\n        self.xi = int((self.x+self.xdim/2)//self.resol)     # x index = col, assuming the grid middle is (0,0)\n        self.yi = int((self.y+self.ydim/2)//self.resol)     # y index = row, assuming the grid middle is (0,0)\n\n        # pi/2 to be superficially resilient to slight angle variation to keep \u03b8i unchanged\n        self.\u03b8i = int((self.\u03b8+pi/2)%(2*pi)//self.\u03b8resol)\n\n        self.si = self.xi + self.yi*self.cols     # position state in the grid\n        self.s = self.nC*(self.\u03b8i) + self.si      # position state with orientation\n        if self.verbose: print('grid cell= ', self.si, 'state = ', self.s)\n        return self.s \n\n\n# Control--------------------------------------------------------------    \n    def spin_n(self, n):\n        for _ in range(n): ros.spin_once(self)\n\n    def control(self): \n        self.controller.publish(self.robot) \n\n    # move then stop to get a defined action\n    def step(self, a=1, speed=None, \u03b8speed=None):\n        if speed is None: speed = self.speed\n        if \u03b8speed is None: \u03b8speed = self.\u03b8speed\n\n        self.t +=1\n        if self.verbose: print('step = ', self.t)\n\n        if  a==-1: self.robot.linear.x  = -speed  # backwards\n        elif a==1: self.robot.linear.x  =  speed  # forwards\n        elif a==0: self.robot.angular.z =  \u03b8speed # turn left\n        elif a==2: self.robot.angular.z = -\u03b8speed # turn right\n\n        # Now move and stop so that we can have a well defined actions  \n        self.spin_n(self.n) if a==1 else self.spin_n(self.n//2)\n        self.stop()\n\n        reward, done, wall = self.reward(a)\n        return self.s_(), reward, done, {}\n\n    def stop(self):\n        self.robot.linear.x = .0\n        self.robot.angular.z = .0\n        #  spin less so that we have smoother actions\n        self.spin_n(self.n//8)\n\n# reseting--------------------------------------------------------------\n    def reset(self):\n        print('resetting world..........................................')\n        # to ensure earlier queued actions are flushed, there are better ways to do this\n        for _ in range(1): self.reset_world.call_async(Empty.Request())\n        for _ in range(2): self.step(a=1, speed=0.001)  # move slightly forward to update the odometry to prevent repeating an episode unnecessary\n        for _ in range(1): self.reset_world.call_async(Empty.Request())\n\n        return self.s_()\n\n    # for compatibility, do not delete\n    def render(self, **kw):\n        pass\n</code></pre> <p>Ok, let us now test our little environment, to do so, open a terminal and launch the simple environment be executing the following command:</p> <p><code> ros2 launch turtlebot3_gazebo turtlebot3_simple.launch.py </code></p> <p>To make the testing smoother, you can right-click Gazebo and keep the window on top. You can also press ctrl+R to reset the environment.</p>"},{"location":"unit5/lesson17/lesson17.html#rotational-and-translational-calibration","title":"Rotational and Translational Calibration","text":"<p>Let us calibrate the rotational and translational movements of our robot settings. The idea here is to be able to get a consistent behaviour where a robot can consistently complete a full circle in a specified number of times most of the times. This is a trial and error process, we usually need to experiment with different settings, bearing in minde the accuracy and efficiency of the robot training that will take place later.</p> <p>The frequency plays an important role as it specifies how many times the velocity changes commands are going to be executed per seconds. This is via our subscription to the /cmd_vel topic and the create_timer() function of the Node class. The second important factor is the number of times the spin_once() is going to be executed. Spining a few times after publishing a command helps stablise the behaviour and gives us more consistency because it helps flush any delayed execution as well as any delayed subscription due to the robot hardware limitation which is simulated to an extent in Gazebo.</p>"},{"location":"unit5/lesson17/lesson17.html#rotation-in-place-to-form-a-full-2pi-circle","title":"Rotation in place to form a full \\(2\\pi\\) circle","text":"<p>You could try to increase the \u03b8speed but that will result in more slippage. It is also possible to increase the speed of execution (rather that the speed of the robot) by playing with n which is the number of times a spin_once() is executed. You could also speed up the clock by increasing the hz (frequency) of execution.</p> <pre><code>hz = 20       # increase to speed up, default is 20, max 30 to speed up\nn  = 28       # decrease to shorten the movements, default is 30, min 5 to speed up\n\nenv = Env(speed=.5, \u03b8speed=pi/5, freq=1/hz, n=n, verbose=True)\n</code></pre> <pre><code>resetting world..........................................\nstep =  1\nscan =  [1.19 2.51 2.43 2.41 2.39 2.38 2.39 1.54 1.51 2.62]\nodom =  [ 0. -0.  0.]\nodom =  [0. 0. 0.]\nodom =  [0. 0. 0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nscan =  [1.22 2.53 2.45 2.39 2.37 2.37 2.4  1.54 1.51 2.62]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nscan =  [1.22 2.53 2.45 2.39 2.38 2.38 2.4  1.52 1.49 2.62]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nseeking goal ____________________ 0\nreward components= Total reward= -7.24 state reward= 0 goal dist= 2.83 |\u03b8-\u03b8goal|= 0.79\ngrid cell=  144 state =  1011\nstep =  2\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nscan =  [1.19 2.52 2.43 2.4  2.39 2.38 2.39 1.53 1.52 2.6 ]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nscan =  [1.2  2.51 2.45 2.4  2.36 2.38 2.39 1.54 1.5  2.62]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nscan =  [1.21 2.51 2.44 2.4  2.39 2.39 2.38 1.54 1.53 2.61]\nodom =  [ 0. -0.  0.]\nseeking goal ____________________ 0\nreward components= Total reward= -7.24 state reward= 0 goal dist= 2.83 |\u03b8-\u03b8goal|= 0.79\ngrid cell=  144 state =  1011\ngrid cell=  144 state =  1011\nspeed  =  0.5\n\u03b8speed =  0.63\nfreq   =  0.05\n</code></pre> <pre><code>def rotate_test(env):\n    env.reset( )\n    for _ in range(1+39):\n        env.step(0)\n\n# %time rotate_test(env)\n</code></pre>"},{"location":"unit5/lesson17/lesson17.html#translation-calibration-moving-in-a-straight-line","title":"Translation calibration, moving in a straight line","text":"<p>You could try to increase the speed but that will result in bending.</p> <pre><code>def forward_test(env):\n    env.reset( )\n    # for _ in range(2): env.step(0)\n    for _ in range(10):\n        env.step()\n\n# forward_test(env)\n</code></pre> <pre><code># env.reset()\n</code></pre>"},{"location":"unit5/lesson17/lesson17.html#manual-solution-policy-to-the-given-problem","title":"Manual solution (policy) to the given problem","text":"<pre><code>def optimal_policy1(env):\n    env.reset( )\n\n    for _ in range(3): env.step(0)\n    for _ in range(5): env.step()\n    for _ in range(3): env.step(0)\n    for _ in range(2): env.step()\n\n# %time optimal_policy1(env)\n</code></pre> <pre><code>def optimal_policy2(env):\n    env.reset( )\n\n    for _ in range(13): env.step(2)\n    for _ in range(6): env.step()\n    for _ in range(5): env.step(2)\n    for _ in range(2): env.step()\n\n# %time optimal_policy2(env)\n</code></pre>"},{"location":"unit5/lesson17/lesson17.html#applying-an-rl-algorithms-to-train-a-turtlebot3-to-autonomously-reach-the-goals","title":"Applying an RL Algorithms to Train a Turtlebot3 to Autonomously Reach the Goals","text":"<p>Now let us apply Sarsa on this problem</p> <pre><code>from rl.rl import *\n</code></pre> <pre><code># env_slow = Env(speed=.5 , \u03b8speed=pi/2, verbose=False) # slower more thorough\n# env_fast = Env(speed= 1., \u03b8speed=.75*pi, verbose=False) # useful for testing\n</code></pre> <pre><code># env = env_fast\n# short max_t so that an episode does not take long\n# sarsa = Sarsa(env=env, \u03b1=.1, max_t=200, episodes=300, seed=0, **demoGame()).interact()\n</code></pre>"},{"location":"unit5/lesson17/lesson17.html#resume-training-and-extend-training","title":"Resume Training and Extend training","text":"<p>If training intrrupted for any reason (including finishing the assigned number of episodes), you can resume it by passing resume=True to the interact() function.</p> <pre><code># env.reset()\n</code></pre> <pre><code># sarsa.interact(resume=True)\n</code></pre> <pre><code># sarsa.episodes = 1000\n# # sarsa.rewards=[100, -10, 0, -1]\n# %time sarsa.interact(resume=True)\n</code></pre>"},{"location":"unit5/lesson17/lesson17.html#vectorised-environment","title":"Vectorised Environment","text":"<p>Let us now try to changed teh states from a number/index into vector. We will simply utilise the laser scans. We can use them as is or try to turn them into some form of a hot encoding or tile coding. Below we show a simple implementaiton which you can build on. Note that we will import algorithms from RLv instead of RL so that we can use the vectorised linear model RL algorithms such Sarsa and Q_learning.</p> <pre><code>class vEnv(Env):\n    def __init__(self, nscans=60, **kw):\n        self.nF = nscans\n        super().__init__(**kw)\n\n\n    def s_(self):\n        max, min = self.range_max, self.range_min\n        \u03c6 = self.scans\n        \u03c6[\u03c6==Inf] = max\n        \u03c6[\u03c6==np.nan] = 0\n        \u03c6[\u03c6&lt;min] = 0\n        \u03c6 = 1 - \u03c6/max\n        return  \u03c6/\u03c6.sum() \n\n    # def s_(self):\n    #     nF, scans, range_max = self.nF, self.scans, self.range_max\n    #     \u03c6 = np.r_[scans[-nF//2:], scans[:nF//2]]\n    #     \u03c6[\u03c6==Inf] = range_max\n    #     # \u03c6[\u03c6 &gt; range_max/2] = 0\n    #     # \u03c6[\u03c6 != 0] = 1\n    #     # print(\u03c6)\n    #     return \u03c6/\u03c6.sum()\n</code></pre> <pre><code>venv = vEnv(speed= .5, \u03b8speed=pi/5, rewards=[30,-10,0,-1], verbose=True) # useful for testing\n</code></pre> <pre><code>[WARN] [1715334431.685891644] [rcl.logging_rosout]: Publisher already registered for provided node name. If this is due to multiple nodes with the same name then all logs for that logger name will go out over the existing publisher. As soon as any node with that name is destructed it will unregister the publisher, preventing any further logs for that name from being published on the rosout topic.\n\n\nresetting world..........................................\nstep =  1\nodom =  [0. 0. 0.]\nscan =  [1.23 2.52 2.44 2.4  2.38 2.38 2.4  1.53 1.52 2.63]\nodom =  [0.   0.   6.28]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nscan =  [1.19 2.49 2.44 2.4  2.36 2.39 2.41 1.56 1.52 2.6 ]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nscan =  [1.21 2.51 2.44 2.39 2.38 2.39 2.38 1.55 1.5  2.61]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nseeking goal ____________________ 0\nreward components= Total reward= -7.24 state reward= 0 goal dist= 2.83 |\u03b8-\u03b8goal|= 0.79\nstep =  2\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nscan =  [1.19 2.52 2.44 2.4  2.38 2.39 2.38 1.53 1.5  2.63]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nscan =  [1.2  2.51 2.43 2.39 2.37 2.38 2.4  1.51 1.52 2.62]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nscan =  [1.21 2.51 2.44 2.38 2.39 2.38 2.4  1.53 1.52 2.6 ]\nseeking goal ____________________ 0\nreward components= Total reward= -7.24 state reward= 0 goal dist= 2.83 |\u03b8-\u03b8goal|= 0.79\nspeed  =  0.5\n\u03b8speed =  0.63\nfreq   =  0.05\n</code></pre> <p>Now it is time to apply Sarsa on robotics! Note that this might not generate a useful policy yet. You must adjust the above code and tune your RL method hyperparameters.</p> <pre><code>from rl.rlln import *\n</code></pre> <pre><code>%time vsarsa = Sarsa\u03bb(env=venv, \u03b1=.05, \u03b5=.1, max_t=1000, episodes=500, seed=1, **demoGame()).interact()\n</code></pre> <pre><code>CPU times: user 35min 38s, sys: 1min 23s, total: 37min 2s\nWall time: 7h 53min 35s\n</code></pre> <p></p> <pre><code>%time vtruesarsa = trueSarsa\u03bb(env=venv, \u03b1=.05, \u03b5=.1, max_t=1000, episodes=500, seed=1, **demoGame()).interact()\n</code></pre> <pre><code>CPU times: user 30min 2s, sys: 1min 9s, total: 31min 12s\nWall time: 7h 4min 40s\n</code></pre> <p></p> <pre><code>\n</code></pre> <pre><code># venv.\u03b8speed = pi/3 \n# that means we are changing the env dynamics which is more challenging for the agent\n# vsarsa.\u03b5 = .1\n# vsarsa.d\u03b5= 1\n# vsarsa.\u03b5min\n</code></pre> <pre><code>vsarsa.episodes = 540\n# sarsa.rewards=[100, -10, 0, -1]\n%time vsarsa.interact(env=venv, resume=True)\n</code></pre> <pre><code>CPU times: user 30.8 s, sys: 1.42 s, total: 32.3 s\nWall time: 6min 31s\n\n\n\n\n\n&lt;RLv.Sarsa\u03bb at 0x7f5ffd716f40&gt;\n</code></pre> <p></p> <pre><code>vtruesarsa.episodes = 530\n# sarsa.rewards=[100, -10, 0, -1]\n%time vtruesarsa.interact(env=venv, resume=True)\n</code></pre> <pre><code>CPU times: user 1min 25s, sys: 3.76 s, total: 1min 29s\nWall time: 18min 5s\n\n\n\n\n\n&lt;RLv.trueSarsa\u03bb at 0x7f60340968b0&gt;\n</code></pre> <p></p> <pre><code>venv.reset()\n</code></pre> <pre><code>resetting world..........................................\nstep =  116815\nscan =  [0.23 0.24 0.28 0.28 0.12 0.12 0.12 0.15 0.17 0.18]\nodom =  [2.3  1.6  0.57]\nscan =  [1.2  2.52 2.43 2.39 2.38 2.37 2.41 1.54 1.51 2.62]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nscan =  [1.19 2.52 2.45 2.4  2.39 2.39 2.4  1.54 1.51 2.6 ]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nscan =  [1.2  2.51 2.44 2.39 2.38 2.4  2.39 1.54 1.52 2.61]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nseeking goal ____________________ 0\nreward components= Total reward= -7.24 state reward= 0 goal dist= 2.83 |\u03b8-\u03b8goal|= 0.79\nstep =  116816\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nscan =  [1.22 2.52 2.43 2.39 2.39 2.39 2.41 1.54 1.52 2.6 ]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nscan =  [1.21 2.52 2.45 2.41 2.37 2.4  2.4  1.54 1.5  2.62]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nodom =  [ 0. -0.  0.]\nscan =  [1.21 2.52 2.44 2.39 2.37 2.38 2.39 1.53 1.49 2.61]\nodom =  [ 0. -0.  0.]\nseeking goal ____________________ 0\nreward components= Total reward= -7.24 state reward= 0 goal dist= 2.83 |\u03b8-\u03b8goal|= 0.79\n\n\n\n\n\narray([0.01292713, 0.01281209, 0.0227836 , 0.02324465, 0.01027762,\n       0.00855598, 0.00619121, 0.00852212, 0.01901032, 0.01732261,\n       0.01271966, 0.01092166, 0.0123869 , 0.01278423, 0.01308601,\n       0.02420145, 0.02379162, 0.02363867, 0.02289693, 0.02216109,\n       0.02112418, 0.0060394 , 0.0024645 , 0.00575699, 0.00831019,\n       0.02626327, 0.02679824, 0.02755685, 0.02754739, 0.02787054,\n       0.02779497, 0.02766693, 0.02734911, 0.01158986, 0.01050995,\n       0.01039502, 0.01457711, 0.01629029, 0.00589362, 0.00818161,\n       0.01017182, 0.02215024, 0.02146195, 0.01321665, 0.01336568,\n       0.01321665, 0.01286833, 0.01212773, 0.01087723, 0.00950818,\n       0.02127673, 0.02375081, 0.02529406, 0.02671511, 0.0276625 ,\n       0.02650686, 0.01138107, 0.01230194, 0.01287275, 0.01305804],\n      dtype=float32)\n</code></pre>"},{"location":"unit5/lesson17/lesson17.html#training-headless","title":"Training Headless","text":"<p>To train more efficiently, turn off the gui in gazebo. To do so, go to the .launch file that you have launched gazebo with and comment out the follwoing lines:</p> <pre><code>   IncludeLaunchDescription(\n       PythonLaunchDescriptionSource(\n           os.path.join(pkg_gazebo_ros, 'launch', 'gzclient.launch.py')\n       ),\n   ),\n</code></pre>"},{"location":"unit6/lesson18/lesson18.html","title":"18. Application on Games(optional)","text":""},{"location":"unit6/lesson18/lesson18.html#lesson-18-rl-application-on-games","title":"Lesson 18: RL Application on Games","text":"<p>Unit 6: Learning Outcomes </p> <ol> <li>understand how to create a simple wrapper for a Gym environment to take advantage of its provided functionality</li> <li>understand how to integrate our previous classes with Gym to combine them in a powerful way</li> <li>appreciate the intricacy of applying RL to different domains such as games and robotics</li> <li>build on previous concepts to come up with suitable and sometimes novel algorithms to solve a problem at hand</li> <li>understand how to combine deep reinforcement learning with deep learning to create a powerful framework that allows automatic agent learning by observation or self-play.</li> <li>understand how a replay buffer helps us to come closer to supervised learning and appreciate the important role it plays in reaching convergence for difficult problems that involve image processing and reinforcement learning</li> </ol> <p>Reading: The accompanying reading of this lesson is chapter 16 of our text book available online here. Please note that we explain the ideas of this topic from a practical perspective and not from a theoretical perspective which is already covered in the textbook.</p> <p>This lesson deals with gym openai. OPenAI Gym provides a rich set of libraries and environments to try our algorithms on. These include the Atari games the DQN used in their Nature paper to show that Deep Reinforcement Learning can supersede human performance on Atari games with the same generic model that is used in all of the presented games. </p> <p>This was a big and important breakthrough since, previously, RL applications stayed largely within simpler control environments and required, as it was customary at that time, manual feature extraction. So, when this paper showed that it is feasible to build an end-to-end RL model that can automatically extract useful features that can be used directly in an RL training algorithm without any human engineering or intervention, it was an important landmark in our field. Prior to that, there was the Tesauro Backgammon TDGammon as another success story which used a neural network to train a model to learn to play backgammon (see the chapter for more details). </p> <p>From that moment on, DeepMind went into a progressive trajectory of successful research and applications in the domain of deep reinforcement learning that reached an unprecedented level of beating the world champion in the game of Go see AlphaGo paper and then moved on to create a generic architecture that is capable of self-playing to reach superhuman levels in chess, Go and shogi that they called AlphaZero. Contrary to AlphaGo, AlphaZero did not use any human knowledge to train the model, and it completely started from scratch and let the AI train itself by itself! see here. </p> <p>Note that even the replay idea is an old idea in RL it just got rehashed and done a bit differently inside a buffer that allows us to conduct learning in mini-batches similar to what we do in supervised learning (since this was tried and tested by the ML community and it is proven to work very well with backpropagation). Yet, this idea was not new since other researchers have tried batches with RL. The paper's main flair is the impressive performance level that could be achieved via an adequately long period of training, a foot that has not been achieved before. Note that the replay buffer dictates the choice of an off-policy algorithm i.e. Q-learning, since the replayed experience is old and the agent will be learning from a policy different to the one it pursues.</p> <p>We expect this trajectory to continue and that RL with robotics will create the next wave of innovation that will hopefully change how we conduct our daily lives. We hope this will lead to positive changes and prosperity in the long run, but that does not prevent mistakes. You will tackle this ethical side in another module. For now, enjoy dealing with the revolutionary side of AI that will change the world!</p> <p>Ok, let us get started...!</p>"},{"location":"unit6/lesson18/lesson18.html#openai-gym-classical-environment","title":"OpenAI Gym Classical Environment","text":"<p>We first tackle classical environments in OpenAI Gym as useful training for dealing with the library. In particular, we will start with their mountain car environment. We have already created our own class for this problem in the previous lesson, however, here, we will inherit their class, and we will override their rendering mechanism to be able to embed its visualisation in our notebook. </p> <p>This is a useful exercise to familiarise ourselves with Gym and verify our findings in the previous lesson. Therefore, we will repeat some of the experiments we conducted in the previous lesson. No commentary is provided as it replicates previous experiments on our newly created class that depends on Gym. Note that we have already set up our classes in previous lessons to be ready to integrate easily with Gym. We will show this in the following example.</p>"},{"location":"unit6/lesson18/lesson18.html#useful-resources","title":"Useful Resources","text":"<p>There are plenty of videos and resources which you can search online for. - You may want to follow the following tutorial which shows how to deal with tensorflow on cart pole but the input is not the frames. - You may then follow this tutorial which uses deep learning on the frames of a cart pole problem.</p> <ul> <li>You may find it useful to watch this video for a Keras tutorial with RL, or this video for a pytorch tutorial with RL. </li> <li>You may want to have a look at some tutorials.</li> </ul> <p>We start by showing you how to inherit from gym environment. We will do exactly what we did in the previous lesson regarding training a mountain car. This time we will utilise the openai gym environment rendering and </p> <pre><code>class MountainCar(MountainCarEnv):\n    def __init__(self, ntiles=8, **kw):   #  \u03c9: position window, rd: velocity window\n        super().__init__(**kw)\n\n        # constants                          \n        self.X0,  self.Xn  = -1.2, .5       # position range\n        self.Xv0, self.Xvn = -.07, .07      # velocity range\n        self.\u03b7 = 3                          # we rescale by 3 to get the wavy valley/hill\n\n        # for render()\n        self.X  = np.linspace(self.X0,  self.Xn, 100)     # car's position\n        self.Xv = np.linspace(self.Xv0, self.Xvn, 100)    # car's speed\n        self.Y  = np.sin(self.X*self.\u03b7)\n\n        # for state encoding (indexes)\n        self.ntiles  = ntiles\n        # number of states is nS*nSd but number of features is nS+nSd with an econdoing power of 2^(nS+nSd)&gt;&gt;nS*nSd!\n        self.nF = self.nS = 2*(self.ntiles+1)\n        self.nA = 3\n        # for compatability\n        self.Vstar = None\n\n        # reset\n        self.x, _ = super().reset()\n        self.xv = 0\n\n        # figure setup\n        self.figsize = (17, 3)\n        #plt.gcf().set_size_inches(self.figsize[0], self.figsize[1])\n\n        self.render_mode=\"rgb_array\"\n\n    def s(self, tilings=1):\n        return (tilings*self.ntiles*(self.x  - self.X0 )/(self.Xn  - self.X0 )).astype(int)\n\n    def sv(self, tilings=1):\n        return int(tilings*self.ntiles*(self.xv - self.Xv0)/(self.Xvn - self.Xv0))\n\n    def reset(self):\n        self.x, _ = super().reset(seed=0)\n        self.xv = 0\n        return self.s_()\n\n    def s_(self):\n        \u03c6 = np.zeros(self.nF)\n        \u03c6[self.s()] = 1\n        \u03c6[self.sv() + self.ntiles] = 1\n        return \u03c6\n\n    # for compatibility\n    def S_(self):\n        return np.eye(self.nF)\n\n    def isatgoal(self):\n        return self.x&gt;=self.Xn\n\n\n    def step(self,a):\n        obs, r, done, _,_ = super().step(a)\n        self.x, self.xv = obs[0], obs[1]\n        return self.s_(), r, done, {}\n\n    def render(self, visible=True, pause=0, subplot=131, animate=True, **kw):\n\n        if not visible: return\n        self.ax0 = plt.subplot(subplot)\n        self.figsize = (17, 3)\n        plt.gcf().set_size_inches(self.figsize[0], self.figsize[1])\n        plt.imshow(super().render())\n        plt.axis('off')\n\n        if animate: \n            clear_output(wait=True)\n            plt.show(); time.sleep(pause)\n</code></pre> <pre><code>%time sarsa = Sarsa(env=MountainCar(ntiles=8), \u03b1=.1/8, episodes=45, seed=1, animate=True, **demoTR()).interact()\n</code></pre> <pre><code>CPU times: user 42.4 s, sys: 622 ms, total: 43 s\nWall time: 44.1 s\n</code></pre> <p></p> <p>Note that we were only showing the end state in each episode and not the whole training process. We can make the process faster by not showing(animating) the training progress as we do below.</p> <pre><code>%time sarsa = Sarsa(env=MountainCar(ntiles=8), \u03b1=.1/8, episodes=45, seed=1, **demoTR()).interact()\n</code></pre> <pre><code>CPU times: user 27.5 s, sys: 347 ms, total: 27.9 s\nWall time: 28.3 s\n</code></pre> <p></p> <p>No exploration</p> <pre><code>sarsa = Sarsa(env=MountainCar(ntiles=8), \u03b1=.3/8, \u03b5=0, episodes=500, seed=1, plotT=True, plotR=True).interact()\n</code></pre> <p></p> <pre><code>sarsa = Sarsa(env=MountainCar(ntiles=16), \u03b1=.3/8, \u03b5=0, episodes=500, seed=1, plotT=True, plotR=True).interact()\n</code></pre> <p></p> <pre><code>sarsa = Sarsa(env=MountainCar(ntiles=32), \u03b1=.3/8, \u03b5=0, episodes=500, seed=1, plotT=True).interact()\nplt.yscale('log')\n</code></pre> <p></p> <pre><code>MountainCarRuns()\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [01:40&lt;00:00,  5.01s/it]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [01:25&lt;00:00,  4.29s/it]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [01:08&lt;00:00,  3.44s/it]\n</code></pre> <p></p> <p>To run similar experiments that we did in the previous lesson we need to use the tiledMountainCar class. Below we restate the tiledMountainCar class that we developed in the previous lesson. No changes here except that it inherits now from the new MountainCar cvlass that uses gym MountainCarEnv class. We could have avoided this by putting tiledMountainCar in a class factory function. This is left for you as an exercise.</p> <pre><code>class tiledMountainCar(MountainCar):\n    def __init__(self, ntilings=1, **kw): #ntilings: is number of tiles\n        super().__init__(**kw)\n        self.ntilings = ntilings\n        self.dim = (self.ntilings, self.ntiles+2, self.ntiles+3) # the redundancy to mach the displacements of position(x) and velocity(xv)\n        self.nF = self.dim[0]*self.dim[1]*self.dim[2]\n\n\n    def inds(self):\n        s_tiling = self.s(self.ntilings)\n        sv_tiling = self.sv(self.ntilings)\n\n        inds = []\n        for tiling in range(self.ntilings):\n            s  = (s_tiling  + 1*tiling )//self.ntilings\n            sv = (sv_tiling + 3*tiling )//self.ntilings\n\n            inds.append((tiling,s,sv))\n\n        return inds\n\n    def s_(self):\n        \u03c6 = np.zeros(self.dim)\n        for ind in self.inds(): \n            \u03c6[ind]=1\n\n        return \u03c6.flatten()\n</code></pre> <pre><code>for n in trange(5):\n    SarsaOnMountainCar(ntilings=2**n)\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:22&lt;00:00,  4.42s/it]\n</code></pre> <p></p> <pre><code>MountainCarRuns(runs=10, env=IHTtiledMountainCar(ntilings=8,ntiles=8), label='with index hashed table for 8*8*8 tiles')\nMountainCarRuns(runs=10, env=tiledMountainCar(ntilings=8,ntiles=8), label='with 8*8*8 tiles')\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:12&lt;00:00,  7.21s/it]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:00&lt;00:00,  6.00s/it]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:50&lt;00:00,  5.10s/it]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:42&lt;00:00, 10.27s/it]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:25&lt;00:00,  8.56s/it]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [01:17&lt;00:00,  7.72s/it]\n</code></pre> <p></p> <pre><code>class IHTtiledMountainCar(tiledMountainCar):\n    def __init__(self, iht_size=1024, **kw): # by default we have 8*8*8 (position tiles * velocity tiles * tilings)\n        super().__init__(**kw)\n        self.nF = iht_size\n\n\n    def s_(self):\n        \u03c6 = np.zeros(self.nF)\n        inds = np.where(super().s_()!=0)[0]\n        \u03c6[inds%self.nF]=1\n        return \u03c6\n</code></pre> <pre><code>MountainCarTilings()\n</code></pre> <pre><code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [01:31&lt;00:00,  4.58s/it]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [01:24&lt;00:00,  4.25s/it]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [01:25&lt;00:00,  4.30s/it]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [01:36&lt;00:00,  4.82s/it]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [01:56&lt;00:00,  5.85s/it]\n</code></pre> <p></p> <p>As we can see we got identical results to the same experiments that we ran in the previous lesson.</p>"},{"location":"unit6/lesson18/lesson18.html#openai-gym-atari-games-environments","title":"OpenAI Gym Atari Games Environments","text":"<p>Now we are ready to move to our Atari environment. To be able to follow the logic it is better if you read the Nature paper since most tutorial follow the deep learning architecture that the paper presented. Note that the deep neural network architecture is just three CNNs, nevertheless the paper pioneered and showed the effectiveness of combing deep learning with reinforcement learning. The paper has few inventions such as the experience replay buffer and the batch training something which had not been done successfully before in RL in such a context. The main contribution is to prove that RL is generic enough to be applied to learn from images just as human learn to play a game. Note that the algorithm needed to be an off-policy because of the usage of the experience replay buffer. The use of experience replay means that the agent is learning its current policy from an old experience that stemmed from following an old policy (previous version of its current policy). </p> <p>Note that in terms of theory we are still lagging to prove convergence of such approaches. This is where we can be uncertain which shadow of doubt on the ethical controllability of our own creation.</p>"},{"location":"unit6/lesson18/lesson18.html#atari-gym-wrapper","title":"Atari Gym Wrapper","text":"<p>Let us now build our own wrapper for Atari game so that we are able to apply Deep Q-Learning on the screen pixels coming from the environment. We have equipped our environment with necessary video storing as well as with pre-processing and to be able to handle the frames of the Gym environment. Note that we are dealing with environments that do not perform skipping to avoid the stochasticity associated with randomly applying actions. Therefore, we had to do the frame skipping in the wrapper class. It is also sufficient to deal with grey scale images not RGB since this will reduce the processing demands on our machines. Feel free to experiments with the GymEnv class to change its underlying preprocessing and or its assumptions.</p> <pre><code>def GymEnvi(Wrapper=gym.Wrapper):\n\n    class GymEnvi(Wrapper):\n        def __init__(self, env_name='ALE/Pong-v5', seed=0,   # 'PongNoimgskip-v4'\n                     nimgs_skip=4, nimgs_stack=1, img_size=(84, 84), \n                     video=True, i=0, animate=False, saveimg=False):\n\n            # if Wrapper==gym.Wrapper:\n            super().__init__(gym.make(env_name, render_mode='rgb_array'))\n\n            self.nA = 3 if 'Pong' in env_name else self.action_space.n \n            self.env_name = env_name\n            # seeding the game for consistency when testing\n            # self.env.seed(seed)\n\n            self.animate = animate\n            self.saveimg = saveimg\n            self.nimgs_skip = nimgs_skip      # how many imgs will be skipped to form one step\n            self.imgs_skip  = deque(maxlen=2)   # a buffer to store last few imgs that will take their max\n\n            self.nimgs_stack = nimgs_stack    # how many imgs will be put together to form a one state (we use first and last only)\n            self.img_size    = img_size       # (w,h)how much reduction will be applied on the original imgs\n            self.img_size_   = (*img_size, max(1,nimgs_stack)) # extended dimension of state space\n            self.imgs_stack  = deque(maxlen=nimgs_stack)\n\n\n            self.nS = 10 # for compatibility\n            self.i = i   # video number\n\n            # ineffective for compatibility \n            self.Vstar = None\n\n            # for rendering and video\n            self.ax0 = None\n            self.figsize = (20, 4)\n            self.video = video\n            self.video_imgs  = []\n            self.video_imgs_ = []\n\n        # self.s holds the latest img *after* preprocessing (state/observation seen by the agent)\n        def preprocess(self, img):\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n            img = cv2.resize(img, self.img_size) / 255\n            self.s = np.expand_dims(img, -1)\n            return self.s \n\n        # self.img holds the latest img *before* preprocessing (will be used for videos)\n        def step(self, a):\n            if a and 'Pong' in self.env_name: a+=1 # suitable for pong only\n            self.a = a\n            self.r = 0\n            # skip 4 imgs (apply same action) and stack last 2 imgs then take their max\n            for i in range(self.nimgs_skip):\n                img, r, self.done, _, _ = self.env.step(a)\n                self.r += r\n                self.imgs_skip.append(img)\n                if self.done: break\n\n            img = self.preprocess(np.stack(self.imgs_skip).max(axis=0))\n            self.imgs_stack.append(img)\n\n            # stack only the first and last imgs to save computation and to convey movement direction to the model\n            img_inds = [0,-1] if self.nimgs_stack&gt;1 else [0]\n            self.img = np.dstack([self.imgs_stack[i] for i in img_inds ])\n\n            return self.img, self.r, self.done, {}\n\n        def reset(self):\n            self.imgs_skip.clear()\n            self.imgs_stack.clear()\n\n            # reset the environment and retain its initial state (image) \n            img = self.env.reset()[0]\n            self.imgs_skip.append(img)\n            img = self.preprocess(img)\n\n            # now stack the same img n times for consistency with step()\n            for _ in range(self.nimgs_stack):\n                self.imgs_stack.append(img)\n\n            img_inds = [0,-1] if self.nimgs_stack&gt;1 else [0]\n            self.img = np.dstack([self.imgs_stack[i] for i in img_inds ])\n\n            return self.img \n\n        #------------------------------------------render \u270d\ufe0f-------------------------------------------------\n        def render(self, visible=True, pause=0, subplot=131,  animate=True, **kw):\n\n            if not visible: return\n\n            self.ax0 = plt.subplot(subplot)\n            plt.gcf().set_size_inches(self.figsize[0], self.figsize[1])\n\n            # saving it as a video if needed, note that we render only at the last few episode\n            if self.video and animate:\n                self.video_imgs.append(self.img)\n                plt.axis('off')\n                # create the video at the end of the set of episodes\n                if self.done:\n                    for obsv in self.video_imgs: self.video_imgs_.append([plt.imshow(obsv, cmap=cm.Greys_r, animated=True)])\n                    anim = animation.ArtistAnimation(self.ax0.figure, self.video_imgs_, interval=50, blit=True, repeat_delay=1000)\n                    anim.save('atari%d.mp4'%self.i)\n            img_inds = [0, int(self.nimgs_stack/2), -1] if self.nimgs_stack &gt; 2 else [0]\n            img = np.dstack([self.imgs_stack[i] for i in img_inds])\n\n            plt.imshow(img)\n            plt.axis('off')\n            if animate:\n                clear_output(wait=True)\n                plt.show()\n                time.sleep(pause)\n\n    return GymEnvi\n\nGymEnv = GymEnvi()\n</code></pre> <p>Let us create some handy function to play an Atari games and observe the states.</p> <pre><code>def play(ep=1, env=GymEnv(nimgs_stack=3), render=True): # try nframes_stack=5 it is fun!\n    for ep in range(ep):\n        env.reset()\n        done=False\n        for _ in range(50):\n            s,r, done, _ = env.step(randint(3))\n            if render: env.render(pause=0)\n    print(s.shape)\n    return s\n</code></pre> <pre><code>s = play()\n</code></pre> <p></p> <pre><code>(84, 84, 2)\n</code></pre>"},{"location":"unit6/lesson18/lesson18.html#rl-with-deep-learning","title":"RL with Deep Learning","text":"<p>It is time to extend our basic MRP class to handle function approximation using deep neural networks. Note that in their paper DeepMinds trained for 200M frames, we set the max_t_exp (used in stop_exp() function) to 2M due to hardware limitation. We can also make the stop_exp() tied to R-star that is specific to the game under consideration. For example, in Pong, we can set this to 18. This means that on average (for the last 100 or 200 games/episodes), the opponent could only score only a max of 3 goals, and our agent scores the wining 21 goals.</p>"},{"location":"unit6/lesson18/lesson18.html#buffer-implementation","title":"Buffer Implementation","text":"<p>It is better to implement the buffer as queue because it guarantees an O(1) complexity for append() and pop() and it is preferred over the list which gives us a O(n) for adding and retrieving an item. In Python we can utilise the double queue structure which gives us the flexibility to add and retrieve from both ends of the queue. Below, we show a quick example. Note that the buffer will be overwritten when. the number of items exceeds its length. This is useful for us because we just want the buffer to overwritten with new experience after it s full and to be kept updated accordingly.</p> <pre><code>from collections import deque\nbuffer = deque(maxlen=5)\nbuffer.append(1)\nbuffer.append(2)\nbuffer.append(3)\nbuffer.append(4)\nprint(buffer)\n</code></pre> <pre><code>deque([1, 2, 3, 4], maxlen=5)\n</code></pre> <pre><code>buffer.append(5)\nbuffer.append(6)\nbuffer.append(7)\nprint(buffer)\n</code></pre> <pre><code>deque([3, 4, 5, 6, 7], maxlen=5)\n</code></pre>"},{"location":"unit6/lesson18/lesson18.html#sampling-form-the-buffer","title":"Sampling form the buffer","text":"<pre><code>import random\n\nrandom.sample(buffer,2)\n</code></pre> <pre><code>[4, 6]\n</code></pre>"},{"location":"unit6/lesson18/lesson18.html#buffer-with-complex-element-tuples","title":"Buffer with Complex Element (Tuples)","text":"<p>Let us assume that we have a set of tuples each consists of (s,a,sn). In this case we can add these tuples as is. Below we show an example, we have represented actions as integers but states/observations as string to help identifying them visually, but bear in mind that they are going to be a more complex entities such as images.</p> <pre><code>buffer = deque(maxlen=4)\nbuffer.append(('2',1,'3'))\nbuffer.append(('3',2,'4'))\nbuffer.append(('4',2,'5'))\nbuffer.append(('5',1,'4'))\n\nprint(buffer)\n</code></pre> <pre><code>deque([('2', 1, '3'), ('3', 2, '4'), ('4', 2, '5'), ('5', 1, '4')], maxlen=4)\n</code></pre>"},{"location":"unit6/lesson18/lesson18.html#deep-mrp","title":"Deep MRP","text":"<p>In this class we implement the basic functionality for dealing with creating, saving and loading deep learning models. In addition, we make these models the default functions used to obtain the value function via self.V_. We also adjust the stope_exp criterion so that the algorithm stops when a specific averaged reward is achieved or when a specific total number of steps (self.t_ not self.t) have been elapsed. This means also that we free ourselves from the notion of an episode, so our model can run as many episodes as it takes to achieve this total number of steps. We still can assign episodes=x to store metrics for last y episodes where y&lt;x. Note that nF is usually used in the Env(ironment) class but feature extraction is embedded the model itself in deep learning model so it is defined in the Deep_MRP class.</p> <pre><code>class Deep_MRP(MRP):\n    def __init__(self, \n                 env=GymEnv(), \n                 \u03b3=0.99,\n                 nF=512, \n                 R_star=None, \n                 last=40,\n                 buffer_size=10000, \n                 batch_size=32, \n                 max_t_exp=int(1e6),\n                 load_weights=False,\n                 print_=False,\n                 **kw):\n\n        super().__init__(env=env, \u03b3=\u03b3, last=last, print_=print_, **kw)\n        self.nF           = nF # feature extraction is integrated within the deep learning model not the env\n        self.R_star       = R_star\n        self.buffer_size  = buffer_size\n        self.batch_size   = batch_size\n        self.load_weights_= load_weights\n        self.max_t_exp    = max_t_exp # used to stop learning\n\n    def init(self):\n        self.vN = self.create_model('V')                      # create V deep network\n        if self.load_weights_: self.load_weights(self.vN,'V') # from earlier training proces\n        self.vN.summary()\n\n        self.V = self.V_\n\n    #-------------------------------------------Deep model related---------------------------\n    def create_model(self, net_str):\n        print(f'model for {net_str} network is being loaded from disk........!')\n        x0 = Input((84,84,1))#self.env.frame_size_)\n        x = Conv2D(32, 8, 4, activation='relu')(x0)\n        x = Conv2D(64, 4, 2, activation='relu')(x)\n        x = Conv2D(64, 3, 1, activation='relu')(x)\n        x = Flatten()(x)\n        x = Dense(self.nF, 'relu')(x)\n        x = Dense(1 if net_str=='V' else self.env.nA)(x) \n        model = Model(x0, x)\n        model.compile(Adam(self.\u03b1), loss='mse')\n        return model\n\n    def load_weights(self, net, net_str ):\n        print(f'weights for {net_str} network are being loaded from disk........!')\n        loaded_weights = net.load_weights(net_str)\n        loaded_weights.assert_consumed()\n\n    def save_weights(self):\n        print(f'weights for V network are being saved to disk........!')\n        self.vN.save_weights('V')\n\n    #------------------------------------- value related \ud83e\udde0-----------------------------------\n    def V_(self, s):\n        if len(s.shape)!=4: \n            # this needs to be tested to make sure it give us the required dim\n            return self.vN.predict(np.expand_dims(s, 0))[0]  # prediction for one state for \u03b5greedy to work well\n        return np.copy(self.vN.predict(s))  # prediction for a batch of states, we copy to avoid auto-grad issues\n\n    # ------------------------------------ experiments related --------------------------------\n    # overriding: we do not specify a preset number of episodes to stop the experiments\n    def stop_exp(self):  \n\n        if self.ep and self.R_star is not None and self.Rs[:self.ep].mean()&gt;= self.R_star: \n            print(f'target reward is achieved in {self.t_} steps!')\n            return True\n\n        if self.t_ &gt; self.max_t_exp:\n            print(f'max steps of {self.t_} is reached and training is stopped, weights are being saved!')\n            self.save_weights()\n            return True\n\n        # save model's weights at least 10 times during the life of the agent\n        if (self.t_+1)%int(.1*self.max_t_exp)==0: self.save_weights()\n        return False\n\n    # stop_ep must be overriden because otherwise it will stope when max_t is reached\n    def stop_ep(self, done): return done\n\n    #-------------------------------------------buffer related----------------------------------\n    def allocate(self):\n        self.buffer = deque(maxlen=self.buffer_size)\n\n    def store_(self, s=None,a=None,rn=None,sn=None,an=None, done=None, t=0):\n        self.buffer.append((s, a, rn, sn, done))\n\n    def sample(self):\n        # sample a set of batch_size tuples (each tuple has 5 items) without replacement \n        # zip the tuples into one tuple of 5 items and convert each item into a np array \n        # of size batch_size   \n        samples = [np.array(experience) for experience in zip(*sample(self.buffer, self.batch_size))]\n\n        # generate a set of indices handy for filtering, to be used in online()\n        inds = np.arange(self.batch_size)\n\n        return samples, inds\n</code></pre>"},{"location":"unit6/lesson18/lesson18.html#deep-mdp","title":"Deep MDP","text":"<p>Now we create the Deep_MDP class which implements policy related functionality</p> <pre><code>class Deep_MDP(Deep_MRP):\n    def __init__(self, \n                 \u03b5=1.0,  \n                 \u03b5min=0.01, \n                 \u03b5T=200000,    # linear decay by default\n                 t_qNn=1000,   # update the target network every t_qNn steps\n                 create_vN=False,\n                 **kw):\n\n        super().__init__(**kw) \n        self.t_qNn = t_qNn \n        self.create_vN = create_vN\n        self.\u03b5 = \u03b5\n        self.\u03b5min = \u03b5min\n        self.\u03b5T = \u03b5T\n\n    def init(self):\n        self.create_vN: super().init()                        # to create also vN, suitable for actor-critic\n\n        self.qN  = self.create_model('Q')                     # create main policy network\n        self.qNn = self.create_model('Q')                     # create target network to estimate Q(sn)\n        if self.load_weights_: self.load_weights(self.qN,'Q') # from earlier training proces\n        self.qNn.set_weights(self.qN.get_weights())\n\n        self.qN.summary()\n\n        self.Q = self.Q_\n\n    def save_weights(self):\n        if self.create_vN: super().save_weights()\n        print(f'weights for Q network are being saved to disk........!')\n        self.qN.save_weights('Q')\n    #------------------------------------- policies related \ud83e\udde0-----------------------------------\n    def Q_(self, s):\n        if len(s.shape)!=4: \n            return self.qN.predict(np.expand_dims(s, 0))[0]  # prediction for one state for \u03b5greedy to work well\n        return np.copy(self.qN.predict(s))  # prediction for a batch of states, we copy to avoid auto-grad issues\n\n    def Qn(self, sn):\n        return self.qNn.predict(sn)    \n    #-------------------------------------- \ud83d\udd0d conditions -----------------------------------------\n\n    def online_cond(self):  return len(self.buffer) &gt;= self.buffer_size\n    def target_cond(self):  return self.t_ % self.t_qNn==0\n\n    #-------------------------------------- \ud83e\udde0 deep nets updates ----------------------------------- \n    def update_before(self, *args): pass\n    def update_after(self, *args): pass\n    def update_online_net(self): pass    \n\n    # update the target network every now and then\n    def update_target_net(self):\n        if self.target_cond(): \n            print('assigning weights for target network.....')\n            self.qNn.set_weights(self.qN.get_weights())\n\n\n    #------------------------------------- \ud83c\udf16 online learning --------------------------------------       \n    def online(self, *args):\n        # make sure the buffer is full before doing any update or sampling\n        if self.online_cond(): \n            self.update_before(*args) \n            self.update_target_net()\n            self.update_online_net()\n            self.update_after(*args)\n</code></pre>"},{"location":"unit6/lesson18/lesson18.html#deep-q-learning-architecture","title":"Deep Q-Learning Architecture","text":"<p>Note that we need to set \u03b5 here otherwise it will be set by default to .1 in the parent class.</p> <pre><code>class DQN(Deep_MDP):\n    def __init__(self, \u03b1=1e-4, **kw): \n        print('--------------------- \ud83e\udde0  DQN is being set up \ud83e\udde0 -----------------------')\n        super().__init__(**kw)\n        self.\u03b1 = \u03b1\n\n    #------------------------------- \ud83c\udf16 online learning ---------------------------------\n    # update the online network in every step using a batch\n    def update_online_net(self):\n        # sample a tuple batch: each componenet is a batch of items \n        #(ex. s is a set of states, a is a set of actions)\n        (s, a, rn, sn, dones), inds = self.sample() \n\n        # obtain the action-values estimation from the two networks \n        # and make sure target is 0 for terminal states\n        Qs = self.Q(s)\n        Qn = self.Qn(sn); Qn[dones] = 0\n\n        # now dictate what the target should have been as per the Q-learning update rule\n        Qs[inds, a] = self.\u03b3*Qn.max(1) + rn\n        self.qN.fit(s, Qs, verbose=False)\n</code></pre> <pre><code># deal with the Grid states as images and learn from them to navigate it\n%time deepqlearn = DQN(env=GymEnv(), episodes=5, max_t_exp=20000, \u03b5T=500, buffer_size=10000, batch_size=32).interact() \n</code></pre> <pre><code>--------------------- \ud83e\udde0  DQN is being set up \ud83e\udde0 -----------------------\nmodel for Q network is being loaded from disk........!\nmodel for Q network is being loaded from disk........!\nModel: \"model_4\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_5 (InputLayer)        [(None, 84, 84, 1)]       0\n\n conv2d_12 (Conv2D)          (None, 20, 20, 32)        2080\n\n conv2d_13 (Conv2D)          (None, 9, 9, 64)          32832\n\n conv2d_14 (Conv2D)          (None, 7, 7, 64)          36928\n\n flatten_4 (Flatten)         (None, 3136)              0\n\n dense_8 (Dense)             (None, 512)               1606144\n\n dense_9 (Dense)             (None, 3)                 1539\n\n=================================================================\nTotal params: 1679523 (6.41 MB)\nTrainable params: 1679523 (6.41 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nassigning weights for target network.....\n1/1 [==============================] - 0s 80ms/step\n1/1 [==============================] - 0s 81ms/step\n...\n</code></pre> <pre><code># play pong and take the mean of the last n episodes but will keep going until R.mean() reaches R_star\n%time deepqlearn = DQN(env=GymEnv(),R_star=10, episodes=50, max_t_exp=200000, \u03b5T=50000, buffer_size=10000, batch_size=32).interact() \n</code></pre>"},{"location":"unit6/lesson18/lesson18.html#double-dqn-learning","title":"Double DQN Learning","text":"<pre><code>class DDQN(DQN):\n    def __init__(self, \u03b1=1e-4, **kw):\n        print('----------- \ud83e\udde0 Double DQN is being set up \ud83e\udde0 ---------------------')\n        super().__init__(**kw)\n        self.\u03b1 = \u03b1\n    #--------------------------- \ud83c\udf16 online learning -----------------------------\n    def update_online_net(self):\n        # sample a tuple batch: each componenet is a batch of items \n        #(ex. s is a set of states, a is a set of actions) \n        (s, a, rn, sn, dones), inds = self.sample()\n        # obtain the action-values estimation from the two networks \n        # and make sure target is 0 for terminal states\n        Qs = self.Q(s)\n        Qn = self.Qn(sn); Qn[dones] = 0\n\n        # now dictate what the target should have been as per the *Double* Q-learning update rule\n        # this is where the max estimations are decoupled from the max action selections\n        an_max = self.Q(sn).argmax(1)\n        Qs[inds, a] = self.\u03b3*Qn[inds, an_max] + rn\n        self.qN.fit(s, Qs, verbose=0)\n</code></pre> <pre><code># play pong and take the mean of the last n episodes but will keep going until R.mean() reaches R_star\n%time doubledeepqlearn = DDQN(env=GymEnv(), episodes=20, buffer_size=10000, batch_size=32).interact() \n</code></pre>"},{"location":"unit6/lesson18/lesson18.html#extracting-features-via-auto-encoders","title":"Extracting Features via Auto-Encoders","text":"<p>In this section we show how to use the latent variables of an auto-encoder in order to extract useful features from the frames grabbed from the games. This will allow us to apply previously covered algorithms that can be applied on linear models. For example we can take the latent variables and use them as the input for true online TD(\\(\\lambda\\)).  Refer to keras auto-encoder tutorial.</p>"},{"location":"unit6/lesson18/lesson18.html#build-a-dataset-from-a-game","title":"Build a dataset from a game","text":"<p>Let us collect a dataset from our game by assigning the max_t_exp to be as large as the buffer_size so that the games stops when the buffer is almost full.</p> <pre><code>dqn = DQN(env=GymEnv(), buffer_size=41000, max_t_exp=40000).interact() \n</code></pre> <p>Now extract the data frames from the buffer.</p> <pre><code>experience = [np.array(experience) for experience in zip(*dqn.buffer)]\nframes = experience[0]\nx_train, x_test = train_test_split(frames, test_size=0.3, random_state=42)\nprint('training data size', x_train.shape)\nprint('testing data size', x_test.shape)\n</code></pre>"},{"location":"unit6/lesson18/lesson18.html#conclusion","title":"Conclusion","text":"<p>In this lesson you saw how to deal with a continuous state space using function approximation and how to apply previous concepts on a more difficult control problems. We have built a wrapper class that allowed us to take advantage of the environments provided by OpenAI Gym library. We have duplicated what we have done in the previous lesson in order to 1. examine that our previous environment worked well, 2. see an example of how to deal with OpenAI Gym environment. </p> <p>You have also seen how to combine deep learning with reinforcement learning to create a powerful model that is capable of learning from watching a game. This is really interesting since it opens up the possibility for enormous applications where an agent can watch and learn to arrive to a complex behaviour that allows it to accomplish a task or win a competition. </p>"},{"location":"unit6/lesson18/lesson18.html#units-conclusion","title":"Unit's conclusion","text":"<p>This lesson concludes our unit where we have studied important formulation of RL that we use a function approximation to represent the stare space which can be continuous and infinite.</p> <p>We only expect that the interest is going to continue to grow and that RL with robotics will create the next wave of innovation that will hopefully change the way we conduct our daily lives. We hope that this will lead to positive changes and to prosperity in the long run but that does not prevent mistakes. You will tackle this ethical side in another module, for now enjoy dealing with revolutionary side of AI that will change the world!</p> <p>Congratulations on completing this last unit on RL!</p>"},{"location":"unit6/lesson18/lesson18.html#discussion-and-activity","title":"Discussion and Activity","text":"<p>Read the following classic Nips paper and Nature paper and discuss it in the discussion forum.</p>"},{"location":"unit6/lesson18/lesson18.html#extra-resources","title":"Extra Resources","text":"<ul> <li>You may find see this series of talks about the future of AI by Stuart Russell interesting.</li> <li> <p>If you are intersted in self-driving cars, then:</p> <ul> <li>See the ALVIN paper for an early stage vehicle road control neural network, it is an early precursor of the current road systems that control autonomous vehicle. A lot of the new systems retain some similarities with this systems. </li> <li>See this video of the history of this system. </li> <li>See this video lecture of the topic autonomous vehicle driving.</li> </ul> </li> <li> <p>Carla Autonomous Vehicle Simulation</p> <ul> <li>See this video for Carla simulation tutorial 1</li> <li>See this video for Carla simulation tutorial 1</li> </ul> </li> </ul>"},{"location":"unit6/lesson18/lesson18.html#your-turn","title":"Your turn","text":"<ol> <li> <p>try to apply the same concept on other simple environments provided by Gym such as the acrobot.</p> </li> <li> <p>apply DQN on another Atari game such as SpaceInvaders or Breakout and report the score that you got in the discussion forum.</p> </li> <li> <p>try to think of way to adopt a more complex neural network architecture that uses a more advanced CNN block in order to advance the state of the art in computer vision such as EfficentNetV2. As a starting point, we should note that this kind of neural networks is designed for classification. I.e. is there a way to deal with RL algorithms as a classification problem not a regression of estimating the value function v?.</p> </li> </ol>"},{"location":"unit6/lesson18/lesson18.html#challenge","title":"Challenge++","text":"<ol> <li>check the implementation of the deep network in the tutorial and try to integrate it into the provided infrastructure.</li> </ol>"}]}