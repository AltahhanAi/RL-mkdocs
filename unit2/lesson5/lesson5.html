
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
        <meta name="author" content="Dr Abdulrahman Altahhan">
      
      
      
        <link rel="prev" href="../../unit1/lesson4/lesson4.html">
      
      
        <link rel="next" href="../lesson6/lesson6.html">
      
      
      <link rel="icon" href="../../img/favicon.jpg">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.4">
    
    
      
        <title>5. Dynamic Programming - Reinforcement Learning and Robotics</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.8608ea7d.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../css/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#lesson-5-dynamic-programming-model-based-approach" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
        <aside class="md-banner">
          <div class="md-banner__inner md-grid md-typeset">
            
            
    <div id="versionIndicator"><b>Version:</b> 04.06.21.a</div>
    <img id="customlogo" src="../../img/logo.svg" alt="University of Leeds logo.">

          </div>
          
        </aside>
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../index.html" title="Reinforcement Learning and Robotics" class="md-header__button md-logo" aria-label="Reinforcement Learning and Robotics" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Reinforcement Learning and Robotics
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              5. Dynamic Programming
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../index.html" class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit1/lesson1/lesson1.html" class="md-tabs__link">
          
  
  Unit 1

        </a>
      </li>
    
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="lesson5.html" class="md-tabs__link">
          
  
  Unit 2

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit3/lesson8/lesson8.html" class="md-tabs__link">
          
  
  Unit 3

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit4/lesson12/lesson12.html" class="md-tabs__link">
          
  
  Unit 4

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit5/lesson15/lesson15.html" class="md-tabs__link">
          
  
  Unit 5

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit6/lesson18/lesson18.html" class="md-tabs__link">
          
  
  Unit 6

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../index.html" title="Reinforcement Learning and Robotics" class="md-nav__button md-logo" aria-label="Reinforcement Learning and Robotics" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Reinforcement Learning and Robotics
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../index.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 1
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Unit 1
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit1/lesson1/lesson1.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1. Tabular Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit1/lesson2/lesson2.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2. K-Arm Bandit
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit1/lesson3/lesson3.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    3. MDP
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit1/lesson4/lesson4.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    4. ROS
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Unit 2
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Unit 2
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    5. Dynamic Programming
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="lesson5.html" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    5. Dynamic Programming
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#basics-dynamic-programming" class="md-nav__link">
    <span class="md-ellipsis">
      Basics Dynamic Programming
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#policy-evaluation-and-value-iteration" class="md-nav__link">
    <span class="md-ellipsis">
      Policy Evaluation and Value Iteration
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#inducing-the-dynamics-by-interacting-with-the-environment" class="md-nav__link">
    <span class="md-ellipsis">
      Inducing the dynamics by interacting with the environment
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sources-stochasticity-dynamics-and-policy" class="md-nav__link">
    <span class="md-ellipsis">
      Sources Stochasticity- Dynamics and Policy
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lesson6/lesson6.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    6. Monte Carlo
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lesson7/lesson7.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    7. Mobile Robots
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 3
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Unit 3
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson8/lesson8.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    8. Temporal Difference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson9/lesson9.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    9. n-Step Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson10/lesson10.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    10. Planning in RL(optional)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson11/lesson11.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    11. Localisation and SLAM
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 4
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Unit 4
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit4/lesson12/lesson12.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    12. Function Approximation Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit4/lesson13/lesson13.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    13. Linear Approximation for Prediction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit4/lesson14/lesson14.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    14. Linear Approximation for Control
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 5
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Unit 5
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit5/lesson15/lesson15.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    15. Linear Approximation with Eligibility Traces(prediction and control)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit5/lesson16/lesson16.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    16. Nonlinear Approximation for Control
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit5/lesson17/lesson17.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    17. Application on Robot Navigation
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 6
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Unit 6
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit6/lesson18/lesson18.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    18. Application on Games(optional)
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#basics-dynamic-programming" class="md-nav__link">
    <span class="md-ellipsis">
      Basics Dynamic Programming
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#policy-evaluation-and-value-iteration" class="md-nav__link">
    <span class="md-ellipsis">
      Policy Evaluation and Value Iteration
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#inducing-the-dynamics-by-interacting-with-the-environment" class="md-nav__link">
    <span class="md-ellipsis">
      Inducing the dynamics by interacting with the environment
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sources-stochasticity-dynamics-and-policy" class="md-nav__link">
    <span class="md-ellipsis">
      Sources Stochasticity- Dynamics and Policy
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="lesson-5-dynamic-programming-model-based-approach">Lesson 5- Dynamic Programming: Model-Based Approach</h1>
<p><strong>Unit 2: Learning Outcomes</strong><br />
By the end of this unit, you will be able to:  </p>
<ol>
<li><strong>Compute</strong> the value function for a given policy in tabular settings.  </li>
<li><strong>Implement</strong> control methods that infer an agent’s policy from an action-value function.  </li>
<li><strong>Explain</strong> the concept of Generalized Policy Iteration (GPI) and how it underpins many RL methods.  </li>
<li><strong>Compare</strong> full-backup action-value-based control methods with direct policy estimation control methods.  </li>
<li><strong>Evaluate</strong> how Monte Carlo (MC) methods provide unbiased but high-variance estimates through interaction with the environment.  </li>
<li><strong>Analyze</strong> how REINFORCE achieves unbiased but high-variance policy gradient estimation through interaction with the environment.  </li>
</ol>
<hr />
<p>In the first lesson, you looked at a basic RL problem, the k-arm bandit, which involves only actions and no states (non-associative problem). In general, in RL, we are faced with different situations, and we need to take different actions in each situation in order to achieve a certain goal. This general type of environment with states and actions imposes a different flavour to the solution we can design. From now on, we will tackle associative problems. For associative problems, there are two approaches:</p>
<ol>
<li>Model-based approach</li>
<li>Model-free approach</li>
</ol>
<p>In this lesson, we will take the first approach. We will learn how to use a model of the environment to solve an RL problem. The model is given in the form of the dynamics of the environment. These usually come in the form of 4 dimensions of conditional probability involving an answer to the following question: what is the probability of obtaining a certain reward r in a certain state s' given that the agent was previously in a state s and applied action a.</p>
<p>We will assume that there is already a model for the environment and try to take advantage of this model to come up with the best policy. Nevertheless, we will see simple ways to build such models and come back to this question later when we tackle planning algorithms in RL.</p>
<p><strong>Plan</strong>
As usual, in general there are two types of RL problems that we will attempt to design methods to deal with 
1. Prediction problem
For These problems we will design Policy Evaluation Methods that attempt to find the best estimate for the value-function given a policy.</p>
<ol>
<li>Control problems 
For These problems we will design Value Iteration methods which utilise the idea of Generalised Policy Iteration. They attempt to find the best policy, via estimating an action-value function for a current policy then moving to a better and improved policy by choosing a greedy action often.</li>
</ol>
<h2 id="basics-dynamic-programming">Basics Dynamic Programming</h2>
<iframe src="https://leeds365-my.sharepoint.com/personal/scsaalt_leeds_ac_uk/_layouts/15/embed.aspx?UniqueId=55e740c0-8e7f-4735-b5cd-3a8911e9fc16&embed=%7B%22ust%22%3Atrue%2C%22hv%22%3A%22CopyEmbedCode%22%7D&referrer=StreamWebApp&referrerScenario=EmbedDialog.Create"  width="470" height="200" frameborder="0" scrolling="no" allowfullscreen title="8. Dynamic Programming 1.mkv"></iframe>

<iframe src="https://leeds365-my.sharepoint.com/personal/scsaalt_leeds_ac_uk/_layouts/15/embed.aspx?UniqueId=da8f61f0-2c2d-4ec2-9ac1-ef73d31de388&embed=%7B%22ust%22%3Atrue%2C%22hv%22%3A%22CopyEmbedCode%22%7D&referrer=StreamWebApp&referrerScenario=EmbedDialog.Create"  width="470" height="200"frameborder="0" scrolling="no" allowfullscreen title="8. Dynamic Programming 2.mkv"></iframe>

<h2 id="policy-evaluation-and-value-iteration">Policy Evaluation and Value Iteration</h2>
<iframe src="https://leeds365-my.sharepoint.com/personal/scsaalt_leeds_ac_uk/_layouts/15/embed.aspx?UniqueId=7523c957-a9cb-4e87-b55c-1c449702ba9c&embed=%7B%22ust%22%3Atrue%2C%22hv%22%3A%22CopyEmbedCode%22%7D&referrer=StreamWebApp&referrerScenario=EmbedDialog.Create"  width="470" height="200" frameborder="0" scrolling="no" allowfullscreen title="9. policy evaluation.mkv"></iframe>

<iframe src="https://leeds365-my.sharepoint.com/personal/scsaalt_leeds_ac_uk/_layouts/15/embed.aspx?UniqueId=7523c957-a9cb-4e87-b55c-1c449702ba9c&embed=%7B%22ust%22%3Atrue%2C%22hv%22%3A%22CopyEmbedCode%22%7D&referrer=StreamWebApp&referrerScenario=EmbedDialog.Create"  width="470" height="200" frameborder="0" scrolling="no" allowfullscreen title="9. policy evaluation.mkv"></iframe>

<h2 id="inducing-the-dynamics-by-interacting-with-the-environment">Inducing the dynamics by interacting with the environment</h2>
<p>We cover obtaining the dynamics from an actual environment. We will use mainly the random walk environment and the grid world environment and generate their dynamics. These are deterministic simple environments. Nevertheless, they are very useful to demonstrate the ideas of RL. </p>
<p>Note that when we move to the real world the dynamics become much more complex and building or obtaining the dynamic becomes impractical in most cases. Therefore, towards that end instead of dealing directly with the environment's dynamics, we will see later how we can substitute this requirement by having to <em>interact</em> with the environment to gain <em>experience</em> which will help us <em>infer</em> a good <em>estimate</em> of the <em>expected</em> value function (discounted sum of rewards) which in turn will help us to <em>infer</em> a close to <em>optimal policy</em> for the task in hand. </p>
<p>The exercise of dealing with probabilities and then using them in designing a Dynamic Programming solution is valuable since most of the other solutions utilise the basic ideas (policy iteration, value iteration algorithms and policy improvements theorem) that we cover here and will mainly show us that we can devise a form of Bellman equation that is suitable for interaction, where we use sampling, model-free algorithms instead of using probabilities(dynamics), model-based algorithms. </p>
<p>Dynamic programming suffers from what Bellman described as the curse of dimensionality which indicates that the computational resources required to solve a problem grow exponentially with the dimensionality of the problem. So in our case, the dimensionality is the number of states (as well as actions and rewards). So for example if the dynamic programming solution computational complexity is <span class="arithmatex">\(2^{|S|}\)</span> and the number of states <span class="arithmatex">\(|S|=10\)</span> then it costs <span class="arithmatex">\(2^{10}=1024\)</span> but when the number of states <span class="arithmatex">\(|S|\)</span> grows to 100 the cost becomes <span class="arithmatex">\(2^{100}=1267650600228229401496703205376\)</span>.</p>
<h2 id="sources-stochasticity-dynamics-and-policy">Sources Stochasticity- Dynamics and Policy</h2>
<p>One important point to make is that stochasticity comes from different elements of the MDP and from the policy itself.</p>
<ol>
<li>There might be stochasticity in the dynamics at the state transition level,
   where applying action <span class="arithmatex">\(a\)</span> in a state <span class="arithmatex">\(s\)</span> may cause the agent to transition to different states, each with a different probability</li>
<li>
<p>There might be stochasticity in the dynamics at the reward level,
   where applying action <span class="arithmatex">\(a\)</span> in a state <span class="arithmatex">\(s\)</span> may result in different rewards, each with a certain probability</p>
</li>
<li>
<p>There might be stochasticity in the policy itself,
   where the policy applies different actions in a state <span class="arithmatex">\(s\)</span>  with different probabilities</p>
</li>
<li>
<p>There might be stochasticity or randomness in observing the current state due to the complexity of the state space. 
   For example, when a robot moves around in the environment, after a while, we cannot reliably designate its position from its motor encoders even when we know the start position due to dead-reckoning. This is called partial observability, and there is a framework called BOMDP or partially observable MDP to tackle this problem. However, we will not study this branch. The field is divided about the necessity of BOMDP with a line of thought that considers that we can overcome this difficulty by encoding our states differently but staying in the MDP framework.</p>
</li>
</ol>
<p>These sources of stochasticity dictate using suitable techniques to obtain the dynamics and to evaluate or improve stochastic and deterministic policy.</p>
<p>Some environments might allow the agent to jump over some obstacles or simply skip cells. For these, we define slightly altered dynamics to take the jumps into account. Below we show the definition.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">dynamics</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">randwalk</span><span class="p">(),</span> <span class="n">stoch</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">repeat</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span> <span class="c1"># , maxjump=1</span>

    <span class="n">rewards</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">rewards_set</span><span class="p">()</span>
    <span class="n">nS</span><span class="p">,</span> <span class="n">nA</span><span class="p">,</span> <span class="n">nR</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">nS</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">nA</span><span class="p">,</span> <span class="n">rewards</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">p</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nS</span><span class="p">,</span><span class="n">nR</span><span class="p">,</span>  <span class="n">nS</span><span class="p">,</span><span class="n">nA</span><span class="p">))</span>
    <span class="n">randjump</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">randjump</span>
    <span class="n">env</span><span class="o">.</span><span class="n">randjump</span> <span class="o">=</span> <span class="kc">False</span> <span class="c1"># so that probability of all intermed. jumps is correctly calculated</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">trange</span><span class="p">(</span><span class="n">repeat</span> <span class="k">if</span> <span class="n">stoch</span> <span class="k">else</span> <span class="mi">1</span><span class="p">):</span> <span class="c1"># in case the env is stochastic (non-deterministic)</span>
        <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nS</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">goals</span><span class="p">:</span> <span class="k">continue</span> <span class="c1"># uncomment to explicitly make pr of terminal states=0</span>
            <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nA</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">jump</span> <span class="ow">in</span> <span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">env</span><span class="o">.</span><span class="n">jump</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="n">randjump</span> <span class="k">else</span> <span class="p">[</span><span class="n">env</span><span class="o">.</span><span class="n">jump</span><span class="p">]):</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">i</span> <span class="ow">and</span> <span class="n">show</span><span class="p">:</span> <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span> <span class="c1"># render the first repetition only</span>
                    <span class="n">env</span><span class="o">.</span><span class="n">s</span> <span class="o">=</span> <span class="n">s</span>
                    <span class="n">env</span><span class="o">.</span><span class="n">jump</span> <span class="o">=</span> <span class="n">jump</span>
                    <span class="n">rn</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">a</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
                    <span class="n">sn</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">s</span>
                    <span class="n">rn_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">rewards</span><span class="o">==</span><span class="n">rn</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># get reward index we need to update</span>
                    <span class="n">p</span><span class="p">[</span><span class="n">sn</span><span class="p">,</span><span class="n">rn_</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span><span class="mi">1</span>

    <span class="n">env</span><span class="o">.</span><span class="n">randjump</span> <span class="o">=</span> <span class="n">randjump</span>
    <span class="c1"># making sure that it is a conditional probability that satisfies Bayes rule</span>
    <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nS</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nA</span><span class="p">):</span>
            <span class="n">sm</span><span class="o">=</span><span class="n">p</span><span class="p">[:,:,</span> <span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">sm</span><span class="p">:</span> <span class="n">p</span><span class="p">[:,:,</span> <span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">]</span> <span class="o">/=</span> <span class="n">sm</span>

    <span class="k">return</span> <span class="n">p</span>
</code></pre></div>
<h1 id="dynamic-programming-methods">Dynamic Programming Methods</h1>
<p>Ok so we are ready now to move to Dynamic programming algorithms to solve the RL problem of finding a best estimate of a value function and or finding an optimal policy.</p>
<h2 id="policy-evaluation">Policy evaluation</h2>
<p>The first step to improving any policy is to evaluate how good or bad the policy is for the given task. This fundamental question can be addressed by tying up the task with a reward function that basically rewards the agent for achieving the task or a subtask that leads to the final goal. The agent's aim then becomes to collect as many rewards as possible (or to incur as few losses as possible), which should help the agent achieve the given task. One example is when a robot is moving in an environment, and we want it to reach a specific location, then we can reward/punish the robot for each step that is taking it close to the goal or away from it. But this awareness of the goal location is usually difficult to attain in real environments. Hence it is replaced by rewarding the agent when it reaches the goal or punishing the agent for each step taken without reaching the goal location.</p>
<p>We can devise an evaluation strategy based on the discounted sum of rewards the agent is <em>expected</em> to collect while executing the task. The strategy depends on the dynamics of the environment. You may want to read section 4.1 and come back here to continue reading the code for the policy evaluation algorithm to get an insight into how it works.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">Policy_evaluation</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">randwalk</span><span class="p">(),</span> <span class="n">p</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">V0</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">π</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">γ</span><span class="o">=</span><span class="mf">.99</span><span class="p">,</span> <span class="n">θ</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span> 

    <span class="c1"># env parameters</span>
    <span class="n">nS</span><span class="p">,</span> <span class="n">nA</span><span class="p">,</span> <span class="n">nR</span><span class="p">,</span> <span class="n">rewards</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">nS</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">nA</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">nR</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">rewards_set</span><span class="p">()</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">dynamics</span><span class="p">(</span><span class="n">env</span><span class="p">)</span> <span class="k">if</span> <span class="n">p</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

    <span class="c1"># policy parameters</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nS</span><span class="p">)</span>     <span class="k">if</span> <span class="n">V0</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">V0</span><span class="p">);</span> <span class="n">V</span><span class="p">[</span><span class="n">env</span><span class="o">.</span><span class="n">goals</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># initial state values</span>
    <span class="n">π</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nS</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span> <span class="k">if</span> <span class="n">π</span>  <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">π</span><span class="p">)</span> <span class="c1"># policy to be evaluated **stochastic/deterministic**</span>

    <span class="n">i</span><span class="o">=</span><span class="mi">0</span>
    <span class="c1"># policy evaluation --------------------------------------------------------------</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">Δ</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">i</span><span class="o">+=</span> <span class="mi">1</span>
        <span class="c1"># show indication to keep us informed</span>
        <span class="k">if</span> <span class="n">show</span><span class="p">:</span> <span class="n">clear_output</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="kc">True</span><span class="p">);</span> <span class="n">rng</span> <span class="o">=</span> <span class="n">trange</span><span class="p">(</span><span class="n">nS</span><span class="p">)</span> <span class="c1"># </span>
        <span class="k">else</span><span class="p">:</span> <span class="n">rng</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="n">nS</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">rng</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">goals</span><span class="p">:</span> <span class="k">continue</span> <span class="c1"># only S not S+</span>
            <span class="n">v</span><span class="p">,</span> <span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">],</span> <span class="mi">0</span>            
            <span class="k">for</span> <span class="n">sn</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nS</span><span class="p">):</span> <span class="c1"># S+</span>
                <span class="k">for</span> <span class="n">rn_</span><span class="p">,</span> <span class="n">rn</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">rewards</span><span class="p">):</span> <span class="c1"># get the next reward rn and its index rn_</span>
                    <span class="k">if</span> <span class="n">π</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span> <span class="c1"># deterministic policy</span>
                        <span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">+=</span> <span class="n">p</span><span class="p">[</span><span class="n">sn</span><span class="p">,</span><span class="n">rn_</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span><span class="n">π</span><span class="p">[</span><span class="n">s</span><span class="p">]]</span><span class="o">*</span><span class="p">(</span><span class="n">rn</span> <span class="o">+</span> <span class="n">γ</span><span class="o">*</span><span class="n">V</span><span class="p">[</span><span class="n">sn</span><span class="p">])</span>
                    <span class="k">else</span><span class="p">:</span>           <span class="c1"># stochastic policy </span>
                        <span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">+=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">π</span><span class="p">[</span><span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">]</span><span class="o">*</span><span class="n">p</span><span class="p">[</span><span class="n">sn</span><span class="p">,</span><span class="n">rn_</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">rn</span> <span class="o">+</span> <span class="n">γ</span><span class="o">*</span><span class="n">V</span><span class="p">[</span><span class="n">sn</span><span class="p">])</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nA</span><span class="p">))</span>
            <span class="n">Δ</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">Δ</span><span class="p">,</span> <span class="nb">abs</span><span class="p">(</span><span class="n">v</span><span class="o">-</span><span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">]))</span>
        <span class="k">if</span> <span class="n">Δ</span><span class="o">&lt;</span><span class="n">θ</span><span class="p">:</span> <span class="k">break</span>
    <span class="k">if</span> <span class="n">show</span><span class="p">:</span> 
        <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">underhood</span><span class="o">=</span><span class="s1">&#39;V&#39;</span><span class="p">,</span> <span class="n">V</span><span class="o">=</span><span class="n">V</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;policy evaluation stopped @ iteration </span><span class="si">%d</span><span class="s1">:&#39;</span><span class="o">%</span><span class="n">i</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">V</span>
</code></pre></div>
<p>Note that we assume that when the policy is deterministic, it takes the shape (nS,) and its entries are actions ex. π[s]=1 means take right(1) when in state s.</p>
<p>On the other hand if the policy is probabilistic, it has a shape of (nS, nA) and its entries are probabilities of each action given a state s, i.e π[a|s] written as π[s,a] in the code.</p>
<p>Note that <span class="arithmatex">\(\gamma\)</span> must be <span class="arithmatex">\(&lt; 1\)</span> to guarantee convergence of the Bellman equation because, in general, we do not know whether the policy guarantees reaching a terminal(goal) state; if we do, then <span class="arithmatex">\(\gamma=1\)</span> is ok. </p>
<p>Refer to section 4.1 in the book: 'The existence and uniqueness of <span class="arithmatex">\(v_\pi\)</span> guaranteed as long as either  <span class="arithmatex">\(\gamma&lt; 1\)</span> or eventual termination is guaranteed from all states under the policy <span class="arithmatex">\(\pi\)</span>'.</p>
<p>This condition can be relaxed when we move to sampling instead of dynamic programming in the next consequent lessons.</p>
<h3 id="policy-evaluation-for-2d-mdp-grid-world">Policy Evaluation for 2d MDP Grid World</h3>
<p>Let us test our policy evaluation algorithm on the following simple 3x3 grid world</p>
<p><div class="highlight"><pre><span></span><code><span class="n">env3x3</span> <span class="o">=</span> <span class="n">Grid</span><span class="p">(</span><span class="n">gridsize</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">s0</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">goals</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">],</span> <span class="n">figsize</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">π</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># guarantee to reach the terminal state hence γ=1 is ok</span>
<span class="n">π_</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># no way to terminal state hence γ=1 leads to an infinite loop</span>
<span class="n">env3x3</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">underhood</span><span class="o">=</span><span class="s1">&#39;π&#39;</span><span class="p">,</span> <span class="n">π</span><span class="o">=</span><span class="n">π</span><span class="p">)</span>
</code></pre></div>
<img alt="png" src="output_71_0.png" />
<div class="highlight"><pre><span></span><code><span class="n">V0</span> <span class="o">=</span> <span class="p">[</span><span class="mf">.6</span><span class="p">,</span> <span class="mf">.5</span><span class="p">,</span> <span class="mf">.5</span><span class="p">,</span> <span class="mf">.2</span><span class="p">,</span> <span class="mf">.1</span><span class="p">,</span> <span class="mf">.2</span><span class="p">,</span> <span class="mf">.1</span><span class="p">,</span> <span class="mf">.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">Policy_evaluation</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">env3x3</span><span class="p">,</span> <span class="n">π</span><span class="o">=</span><span class="n">π</span><span class="p">,</span> <span class="n">V0</span><span class="o">=</span><span class="n">V0</span><span class="p">,</span> <span class="n">γ</span><span class="o">=</span><span class="mf">.9</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">env3x3</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">underhood</span><span class="o">=</span><span class="s1">&#39;V&#39;</span><span class="p">,</span> <span class="n">V</span><span class="o">=</span><span class="n">V</span><span class="p">)</span>
</code></pre></div>
<img alt="png" src="output_72_0.png" /></p>
<div class="highlight"><pre><span></span><code><span class="n">env</span> <span class="o">=</span> <span class="n">maze</span><span class="p">()</span>
<span class="n">π</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">nS</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint32</span><span class="p">)</span><span class="o">*</span><span class="mi">3</span> <span class="c1"># always go up</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">Policy_evaluation</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">,</span> <span class="n">π</span><span class="o">=</span><span class="n">π</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<p><img alt="png" src="output_75_0.png" /></p>
<div class="highlight"><pre><span></span><code>policy evaluation stopped @ iteration 6:
</code></pre></div>
<p>As we can see moving up yeild some benefits mainly in the cells that lead to the goal.</p>
<p>Let us now generate a random policy and evaluate it for a maze environment.</p>
<div class="highlight"><pre><span></span><code><span class="n">env</span> <span class="o">=</span> <span class="n">maze</span><span class="p">()</span>
<span class="n">π</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">nA</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">nS</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint32</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">V</span> <span class="o">=</span> <span class="n">Policy_evaluation</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">,</span> <span class="n">π</span><span class="o">=</span><span class="n">π</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<p><img alt="png" src="output_80_0.png" />
    policy evaluation stopped @ iteration 1:</p>
<p>As we can see, the randomly generated policy is chaotic and carry little value for the agent. However, evaluating different policies is highly important for an agent since it can guide the improvement of its adopted policy (based on this ability). One example is to keep evaluating random policies until some computational resources are consumed and pick the best. Below we show such a strategy of searching for an optimal policy. You can apply all other search algorithms that you have come across before in conventional AI (breadth-first etc.).</p>
<div class="highlight"><pre><span></span><code><span class="n">env</span> <span class="o">=</span> <span class="n">maze</span><span class="p">()</span>
<span class="n">Vmax</span><span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">env</span><span class="o">.</span><span class="n">nS</span><span class="p">))</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">dynamics</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Vmax before search&#39;</span><span class="p">,</span> <span class="n">Vmax</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>Vmax before search 0.0
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">random</span>
<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">trange</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="c1"># π = np.random.randint(env.nA, size=env.nS)</span>
    <span class="n">π</span> <span class="o">=</span> <span class="n">choices</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">nA</span><span class="p">),</span> <span class="n">k</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">nS</span><span class="p">)</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">Policy_evaluation</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">,</span> <span class="n">π</span><span class="o">=</span><span class="n">π</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">V</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">Vmax</span><span class="o">.</span><span class="n">sum</span><span class="p">():</span> 
        <span class="n">Vmax</span> <span class="o">=</span> <span class="n">V</span>
        <span class="n">πmax</span> <span class="o">=</span> <span class="n">π</span>

<span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">underhood</span><span class="o">=</span><span class="s1">&#39;π&#39;</span><span class="p">,</span> <span class="n">π</span><span class="o">=</span><span class="n">πmax</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Vmax after search&#39;</span><span class="p">,</span> <span class="n">Vmax</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
</code></pre></div>
<p><img alt="png" src="output_83_0.png" /></p>
<div class="highlight"><pre><span></span><code>Vmax after search 10.704460219301
</code></pre></div>
<p>As we can see, finding the optimal policy by random search is difficult since the space of policies is huge, making exhaustive or random search infeasible (dimensionality problem). We need a way to take and maintain a step in the right direction of improving the policy. As you have already seen in another module, a greedy search can often lead to a good result. The next section shows a simple but effective strategy to gradually improve a policy by taking a greedy step towards the solution.</p>
<h2 id="policy-iteration">Policy Iteration</h2>
<p>Now that we know how to evaluate a policy, it is time to improve it. Policy iteration is a basic and simple algorithm. It explicitly and iteratively tries first to reach a highly accurate estimate of the value function of the current policy, then it tries to improve the policy by maximising the probability of greedy actions as per the current value function. Evaluating the current policy fully and then improving it via policy iteration is inefficient, but it shows the fundamental ideas behind reinforcement learning. Please spend some time comprehending the code and reading the corresponding section 4.3 in the book.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">Policy_iteration</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">randwalk</span><span class="p">(),</span> <span class="n">p</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">V0</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">π0</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">γ</span><span class="o">=</span><span class="mf">.99</span><span class="p">,</span> <span class="n">θ</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span> 

    <span class="c1"># env parameters</span>
    <span class="n">nS</span><span class="p">,</span> <span class="n">nA</span><span class="p">,</span> <span class="n">nR</span><span class="p">,</span> <span class="n">rewards</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">nS</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">nA</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">nR</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">rewards_set</span><span class="p">()</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">dynamics</span><span class="p">(</span><span class="n">env</span><span class="p">)</span> <span class="k">if</span> <span class="n">p</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

    <span class="c1"># policy parameters </span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nS</span><span class="p">)</span>     <span class="k">if</span> <span class="n">V0</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">V0</span><span class="p">);</span> <span class="n">V</span><span class="p">[</span><span class="n">env</span><span class="o">.</span><span class="n">goals</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># initial state values</span>
    <span class="n">π</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nS</span><span class="p">,</span><span class="nb">int</span><span class="p">)</span> <span class="k">if</span> <span class="n">π0</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">π0</span><span class="p">);</span> <span class="c1"># initial **deterministic** policy </span>
    <span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nS</span><span class="p">,</span><span class="n">nA</span><span class="p">))</span>  <span class="c1"># state action values storage</span>
    <span class="c1"># π = randint(0,nA,nS)</span>

    <span class="n">j</span><span class="o">=</span><span class="mi">0</span>
    <span class="k">while</span> <span class="kc">True</span> <span class="k">if</span> <span class="n">epochs</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">j</span><span class="o">&lt;</span><span class="n">epochs</span><span class="p">:</span>
        <span class="n">j</span><span class="o">+=</span><span class="mi">1</span>
        <span class="c1"># 1. Policy evaluation---------------------------------------------------</span>
        <span class="n">i</span><span class="o">=</span><span class="mi">0</span>
        <span class="k">while</span> <span class="kc">True</span> <span class="k">if</span> <span class="n">epochs</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">i</span><span class="o">&lt;</span><span class="n">epochs</span><span class="p">:</span>
            <span class="n">Δ</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">i</span><span class="o">+=</span> <span class="mi">1</span>
            <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nS</span><span class="p">):</span> 
                <span class="k">if</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">goals</span><span class="p">:</span> <span class="k">continue</span> <span class="c1"># S not S+</span>
                <span class="n">v</span><span class="p">,</span> <span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">],</span> <span class="mi">0</span>
                <span class="k">for</span> <span class="n">sn</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nS</span><span class="p">):</span> <span class="c1"># S+</span>
                    <span class="k">for</span> <span class="n">rn_</span><span class="p">,</span> <span class="n">rn</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">rewards</span><span class="p">):</span> <span class="c1"># get the reward rn and its index rn_</span>
                        <span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">+=</span> <span class="n">p</span><span class="p">[</span><span class="n">sn</span><span class="p">,</span><span class="n">rn_</span><span class="p">,</span>  <span class="n">s</span><span class="p">,</span> <span class="n">π</span><span class="p">[</span><span class="n">s</span><span class="p">]]</span><span class="o">*</span><span class="p">(</span><span class="n">rn</span> <span class="o">+</span> <span class="n">γ</span><span class="o">*</span><span class="n">V</span><span class="p">[</span><span class="n">sn</span><span class="p">])</span>

                <span class="n">Δ</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">Δ</span><span class="p">,</span> <span class="nb">abs</span><span class="p">(</span><span class="n">v</span><span class="o">-</span><span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">]))</span>
            <span class="k">if</span> <span class="n">Δ</span><span class="o">&lt;</span><span class="n">θ</span><span class="p">:</span> <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;policy evaluation stopped @ iteration </span><span class="si">%d</span><span class="s1">:&#39;</span><span class="o">%</span><span class="n">i</span><span class="p">);</span> <span class="k">break</span>

        <span class="c1"># 2. Policy improvement----------------------------------------------------</span>
        <span class="n">policy_stable</span><span class="o">=</span><span class="kc">True</span>
        <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nS</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">goals</span><span class="p">:</span> <span class="k">continue</span> <span class="c1"># S not S+</span>
            <span class="n">πs</span> <span class="o">=</span> <span class="n">π</span><span class="p">[</span><span class="n">s</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nA</span><span class="p">):</span>
                <span class="n">Q</span><span class="p">[</span><span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span>
                <span class="k">for</span> <span class="n">sn</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nS</span><span class="p">):</span> <span class="c1"># S+</span>
                    <span class="k">for</span> <span class="n">rn_</span><span class="p">,</span> <span class="n">rn</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">rewards</span><span class="p">):</span> <span class="c1"># get the reward rn and its index rn_</span>
                        <span class="n">Q</span><span class="p">[</span><span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="n">p</span><span class="p">[</span><span class="n">sn</span><span class="p">,</span><span class="n">rn_</span><span class="p">,</span>  <span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">rn</span> <span class="o">+</span> <span class="n">γ</span><span class="o">*</span><span class="n">V</span><span class="p">[</span><span class="n">sn</span><span class="p">])</span> 

            <span class="n">π</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">Q</span><span class="p">[</span><span class="n">s</span><span class="p">]</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span> <span class="c1"># simple greedy step</span>
            <span class="k">if</span> <span class="n">π</span><span class="p">[</span><span class="n">s</span><span class="p">]</span><span class="o">!=</span><span class="n">πs</span><span class="p">:</span> <span class="n">policy_stable</span><span class="o">=</span><span class="kc">False</span>

        <span class="k">if</span> <span class="n">policy_stable</span><span class="p">:</span> <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;policy improvement stopped @ iteration </span><span class="si">%d</span><span class="s1">:&#39;</span><span class="o">%</span><span class="n">j</span><span class="p">);</span> <span class="k">break</span>
        <span class="k">if</span> <span class="n">show</span><span class="p">:</span> <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">underhood</span><span class="o">=</span><span class="s1">&#39;π&#39;</span><span class="p">,</span> <span class="n">π</span><span class="o">=</span><span class="n">π</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">π</span><span class="p">,</span> <span class="n">Q</span>
</code></pre></div>
<p>We can now apply policy iteration to a proper Environments. We start by a random walk and then we move into grid world.</p>
<h3 id="2-d-grid-world-mdp-examples">2-d Grid World MDP Examples</h3>
<p>Let us try it on a slightly complex environment such as the maze.</p>
<div class="highlight"><pre><span></span><code><span class="n">env</span><span class="o">=</span><span class="n">maze</span><span class="p">()</span>
<span class="n">π</span> <span class="o">=</span> <span class="n">Policy_iteration</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div>
<p><img alt="png" src="output_105_0.png" /></p>
<div class="highlight"><pre><span></span><code>policy evaluation stopped @ iteration 2:
policy improvement stopped @ iteration 16:
</code></pre></div>
<h2 id="value-iteration-algorithm">Value Iteration Algorithm</h2>
<p>Our final step to fully develop the ideas of dynamic programming is to shorten the time it takes for a policy to be evaluated and improved. One simple idea we will follow here is to slightly improve the evaluation and immediately improve the policy. We do these two steps iteratively until our policy has stopped to improve. This is a very effective strategy because we do not wait until the policy is fully evaluated to improve it; we weave and interleave the two loops together in one loop. Below we show this algorithm. Read section 4.4 to further your understanding of this algorithm.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">value_iteration</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">randwalk</span><span class="p">(),</span> <span class="n">p</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">V0</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">γ</span><span class="o">=</span><span class="mf">.99</span><span class="p">,</span> <span class="n">θ</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span> 

    <span class="c1"># env parameters</span>
    <span class="n">nS</span><span class="p">,</span> <span class="n">nA</span><span class="p">,</span> <span class="n">nR</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">i</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">nS</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">nA</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">nR</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">rewards_set</span><span class="p">(),</span> <span class="mi">0</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">dynamics</span><span class="p">(</span><span class="n">env</span><span class="p">)</span> <span class="k">if</span> <span class="n">p</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

    <span class="c1"># policy parameters</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nS</span><span class="p">)</span> <span class="k">if</span> <span class="n">V0</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">V0</span><span class="p">);</span> <span class="n">V</span><span class="p">[</span><span class="n">env</span><span class="o">.</span><span class="n">goals</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># initial state values</span>
    <span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nS</span><span class="p">,</span><span class="n">nA</span><span class="p">))</span> <span class="c1"># state action values storage</span>

    <span class="k">while</span> <span class="kc">True</span> <span class="k">if</span> <span class="n">epochs</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">i</span><span class="o">&lt;</span><span class="n">epochs</span><span class="p">:</span>
        <span class="n">Δ</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">i</span><span class="o">+=</span> <span class="mi">1</span>
        <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nS</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">goals</span><span class="p">:</span> <span class="k">continue</span>
            <span class="n">v</span><span class="p">,</span> <span class="n">Q</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">],</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nA</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">sn</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nS</span><span class="p">):</span>
                    <span class="k">for</span> <span class="n">rn_</span><span class="p">,</span> <span class="n">rn</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">rewards</span><span class="p">):</span>            <span class="c1"># get the reward rn and its index rn_</span>
                        <span class="n">Q</span><span class="p">[</span><span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="n">p</span><span class="p">[</span><span class="n">sn</span><span class="p">,</span><span class="n">rn_</span><span class="p">,</span>  <span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">rn</span> <span class="o">+</span> <span class="n">γ</span><span class="o">*</span><span class="n">V</span><span class="p">[</span><span class="n">sn</span><span class="p">])</span>  <span class="c1"># max operation is embedded now in the evaluation</span>

            <span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">Q</span><span class="p">[</span><span class="n">s</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>                                     <span class="c1"># step which made the algorithm more concise </span>
            <span class="n">Δ</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">Δ</span><span class="p">,</span> <span class="nb">abs</span><span class="p">(</span><span class="n">v</span><span class="o">-</span><span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">]))</span>

        <span class="k">if</span> <span class="n">Δ</span><span class="o">&lt;</span><span class="n">θ</span><span class="p">:</span> <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;loop stopped @ iteration: </span><span class="si">%d</span><span class="s1"> , Δ = %2.f&#39;</span><span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">Δ</span><span class="p">));</span> <span class="k">break</span>
        <span class="k">if</span> <span class="n">show</span><span class="p">:</span> <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">underhood</span><span class="o">=</span><span class="s1">&#39;π&#39;</span><span class="p">,</span> <span class="n">π</span><span class="o">=</span><span class="n">Q</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">Q</span>
</code></pre></div>
<h3 id="value-iteration-on-a-grid-world">Value Iteration on a Grid World</h3>
<p>Now that you understand the value-iteration algorithm, you can apply it on a different and more complex envornment such as the grid worlds.</p>
<p>Let us try on a grid environment.</p>
<div class="highlight"><pre><span></span><code><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">()</span>
<span class="n">Q</span> <span class="o">=</span> <span class="n">value_iteration</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;optimal action for state&#39;</span><span class="p">,</span> <span class="n">Q</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div>
<p><img alt="png" src="output_126_0.png" /></p>
<div class="highlight"><pre><span></span><code>loop stopped @ iteration: 10 , Δ =  0
optimal action for state [1 1 1 1 1 1 3 0 0 0 1 1 1 1 1 1 3 0 0 0 1 1 1 1 1 1 3 0 0 0 1 1 1 1 1 1 0
 0 0 0 1 1 1 1 1 1 2 0 0 0 1 1 1 1 1 1 2 0 0 0 1 1 1 1 1 1 2 0 0 0 1 1 1 1
 1 1 2 0 0 0]
</code></pre></div>
<p>To interpret the policy we provided you with a useful function to render the environemt with its policy as shown above.</p>
<p>As we can see the policy-iteration algorithm successfuly gave us the best policy for this simple environment.</p>
<h3 id="value-iteration-on-a-windy-grid-world">Value Iteration on a Windy Grid World</h3>
<p>Below we show the results of applying the value iteration method on a windy grid world. This is almost identical to the previous simple grid world without any obstacles, the only difference is that there is a wind blowing upwards, which shifts the agent 2 or 1 cell depending on its location. See page 130 of the book.</p>
<div class="highlight"><pre><span></span><code><span class="n">env</span><span class="o">=</span><span class="n">windy</span><span class="p">()</span>
<span class="n">Q</span> <span class="o">=</span> <span class="n">value_iteration</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<p><img alt="png" src="output_130_0.png" /></p>
<p>Let us now apply the policy-iteration on the maze env.</p>
<div class="highlight"><pre><span></span><code><span class="n">env</span><span class="o">=</span><span class="n">maze</span><span class="p">()</span>
<span class="n">policy</span> <span class="o">=</span> <span class="n">value_iteration</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># print(&#39;optimal action for state&#39;, policy)</span>
</code></pre></div>
<p><img alt="png" src="output_132_0.png" /></p>
<div class="highlight"><pre><span></span><code>loop stopped @ iteration: 14 , Δ =  0
</code></pre></div>
<h2 id="conclusion">Conclusion</h2>
<p>In this lesson, we covered the main dynamic programming algorithms. We saw how evaluating a policy was extremely useful in being the key component to allowing us to improve the policy. We then developed a policy iteration algorithm which improves the policy in two main steps:
1. a step that evaluates the policy fully to reach an accurate estimation of the action values of the current policy
2. a step that improves the policy by adopting a greedy action. The usage of an action-value function Q(s,a) was key in allowing us to choose between actions since the state-value function V(s) does not differentiate between the values of actions</p>
<p>We finally saw how the value-iteration algorithm has a similar structure to the policy-iteration algorithm with one important difference; it can arrive at an optimal policy by just taking a step <em>towards</em> the optimal policy by slightly refines its estimation of the action-value function without fully evaluating it.  Hence, it improves its policy more concisely and with much less overhead than the full policy iteration method.</p>
<p>In the next lesson, we will take a different approach and move to cover sampling methods that do not use the dynamics of the environment explicitly and instead try to improve its policy by interacting with the environment.</p>
<p><strong>Reading</strong>:
For further reading you can consult chapter 4 from the Sutton and Barto <a href="http://incompleteideas.net/book/RLbook2020.pdf">book</a>.</p>
<h2 id="your-turn">Your turn</h2>
<p>Now it is time to experiemnt further and interact with code in <a href="../../workseets/worksheet5.ipynb">worksheet5</a>.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright Abdulrahman Altahhan
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.sections"], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.f1b6f286.min.js"></script>
      
        <script src="../../javascript/tablecontentsoverride.js"></script>
      
        <script src="../../javascript/config.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>