
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
        <meta name="author" content="Abdulrahman Altahhan, 2024.">
      
      
      
        <link rel="prev" href="../lesson5/lesson5.html">
      
      
        <link rel="next" href="../lesson7/lesson7.html">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.4">
    
    
      
        <title>6. Monte Carlo - Reinforcement Learning and Robotics</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.8608ea7d.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../css/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#lesson-5-tabular-methods-monte-carlo" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
        <aside class="md-banner">
          <div class="md-banner__inner md-grid md-typeset">
            
            
    <div id="versionIndicator"><b>Version:</b> 04.06.21.a</div>
    <img id="customlogo" src="../../img/logo.svg" alt="University of Leeds logo.">

          </div>
          
        </aside>
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../index.html" title="Reinforcement Learning and Robotics" class="md-header__button md-logo" aria-label="Reinforcement Learning and Robotics" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Reinforcement Learning and Robotics
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              6. Monte Carlo
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../index.html" class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit1/lesson1/lesson1.html" class="md-tabs__link">
          
  
  Unit 1

        </a>
      </li>
    
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../lesson5/lesson5.html" class="md-tabs__link">
          
  
  Unit 2

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit3/lesson8/lesson8.html" class="md-tabs__link">
          
  
  Unit 3

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit4/lesson12/lesson12.html" class="md-tabs__link">
          
  
  Unit 4

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit5/lesson15/lesson15.html" class="md-tabs__link">
          
  
  Unit 5

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit6/lesson18/lesson18.html" class="md-tabs__link">
          
  
  Unit 6

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../index.html" title="Reinforcement Learning and Robotics" class="md-nav__button md-logo" aria-label="Reinforcement Learning and Robotics" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Reinforcement Learning and Robotics
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../index.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 1
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Unit 1
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit1/lesson1/lesson1.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1. Tabular Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit1/lesson2/lesson2.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2. K-Arm Bandit
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit1/lesson3/lesson3.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    3. MDP
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit1/lesson4/lesson4.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    4. ROS
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Unit 2
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Unit 2
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lesson5/lesson5.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    5. Dynamic Programming
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    6. Monte Carlo
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="lesson6.html" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    6. Monte Carlo
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#plan" class="md-nav__link">
    <span class="md-ellipsis">
      Plan
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#first-visit-mc-policy-evaluation-prediction" class="md-nav__link">
    <span class="md-ellipsis">
      First visit MC Policy-evaluation (prediction)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mrp-environment-for-prediction" class="md-nav__link">
    <span class="md-ellipsis">
      MRP environment for prediction
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MRP environment for prediction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#comparing-speed-for-random-number-generation" class="md-nav__link">
    <span class="md-ellipsis">
      Comparing speed for random number generation.
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lesson7/lesson7.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    7. Mobile Robots
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 3
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Unit 3
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson8/lesson8.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    8. Temporal Difference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson9/lesson9.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    9. n-Step Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson10/lesson10.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    10. Planning in RL(optional)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson11/lesson11.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    11. Localisation and SLAM
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 4
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Unit 4
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit4/lesson12/lesson12.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    12. Function Approximation Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit4/lesson13/lesson13.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    13. Linear Approximation for Prediction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit4/lesson14/lesson14.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    14. Linear Approximation for Control
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 5
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Unit 5
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit5/lesson15/lesson15.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    15. Linear Approximation with Eligibility Traces(prediction and control)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit5/lesson16/lesson16.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    16. Nonlinear Approximation for Control
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit5/lesson17/lesson17.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    17. Application on Robot Navigation
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 6
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Unit 6
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit6/lesson18/lesson18.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    18. Application on Games(optional)
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#plan" class="md-nav__link">
    <span class="md-ellipsis">
      Plan
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#first-visit-mc-policy-evaluation-prediction" class="md-nav__link">
    <span class="md-ellipsis">
      First visit MC Policy-evaluation (prediction)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mrp-environment-for-prediction" class="md-nav__link">
    <span class="md-ellipsis">
      MRP environment for prediction
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MRP environment for prediction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#comparing-speed-for-random-number-generation" class="md-nav__link">
    <span class="md-ellipsis">
      Comparing speed for random number generation.
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<p>The notebook uses a library of functionality in RL that aims for simplicity and general insight into how algorithms work, these libraries are written from scratch using standard Python libraries (numpy, matplotlib etc.).
Please note that you will need permission from the author to use the code for research, commercially or otherwise.</p>
<h1 id="lesson-5-tabular-methods-monte-carlo">Lesson 5-Tabular Methods: Monte Carlo</h1>
<p><strong>Learning outcomes</strong>
1. understand the difference between learning the expected return and computing it via dynamic programming
1. understand the strengths and weaknesses of MC methods
1. appreciating that MC methods need to wait till the end of the task to obtain its estimate of the expected return
1. compare MC methods with dynamic programming methods
1. understand the implication of satisfying and not satisfying the explore-start requirement for the MC control and how to mitigate it via the reward function
1. understand how to move from prediction to control by extending the V function to a Q function and make use of the idea of generalised policy iteration-GPI
1. understand how policy gradient methods work and appreciate how they differ from value function methods</p>
<p><strong>Reading</strong>:
The accompanying reading of this lesson is <strong>chapter 5</strong> from our textbook by Sutton and Barto available online <a href="http://incompleteideas.net/book/RLbook2020.pdf">here</a>. Please note that we explain the ideas of this topic from a practical perspective and not from a theoretical perspective, which is already covered in the textbook. Please note that off-policy methods are not covered and hence can be skipped safely when reading from the textbook.</p>
<p>In this lesson, we develop the ideas of Monte Carlo methods. Monte Carlo methods are powerful and widely used in settings other than RL. You may have encountered them in a previous module where they were mainly used for sampling. We will also use them here to sample observations and average their expected returns. Because they average the returns, Monte Carlo methods have to wait until <em>all</em> trajectories are available to estimate the return. Later, we will find out that Temporal Difference methods do not wait until the end of the episode to update their estimate and outperform MC methods.</p>
<p>Note that we have now moved to <em>learning</em> instead of <em>computing</em> the value function and its associated policy. This is because we expect our agent to learn from <em>interacting with the environment</em> instead of using the dynamics of the environment, which is usually hard to compute except for a simple lab-confined environment. </p>
<p>Remember that we are dealing with <em>expected return</em>, and we are either finding an <em>exact solution for this expected return</em> as when we solve the set of Bellman equations or finding an <em>approximate solution for the expected return</em> as in DP or MC.
Remember also that the expected return for a state is the future cumulative discounted rewards given that the agent follows a specific policy.</p>
<p>One pivotal observation that summarises the justification for using MC methods over DP methods is that it is often the case that we are able to interact with the environment instead of obtaining its dynamics due to tractability issues.</p>
<h2 id="plan">Plan</h2>
<p>As usual, in general, there are two types of RL problems that we will attempt to design methods to deal with 
1. Prediction problem
For These problems, we will design Policy Evaluation Methods that attempt to find the best estimate for the value function given a policy.</p>
<ol>
<li>Control problems 
For These problems, we will design Value Iteration methods that utilise Generalised Policy Iteration. They attempt to find the best policy by estimating an action-value function for a current policy and then moving to a better and improved one by often choosing a greedy action. They minimise an error function to improve their value function estimate, used to deduce a policy.
We will then move to Policy Gradient methods that directly estimate a useful policy for the agent by maximising its value function.</li>
</ol>
<p>We start by assuming that the policy is fixed. This will help us develop an algorithm that predicts the state space's value function (expected return). Then we will move to the policy improvement methods, i.e. these methods that help us to compare and improve our policy with respect to other policies and move to a better policy when necessary. Then we move to the control case (policy iteration methods).</p>
<h2 id="first-visit-mc-policy-evaluation-prediction">First visit MC Policy-evaluation (prediction)</h2>
<p>Value-function approximation Method</p>
<p>Because MC methods depend entirely on experience, a natural way to approximate the cumulative future discounted reward is by taking their average once they become available through experience. So we must collect the cumulative <em>future</em> discounted reward once this experience has elapsed. In other words, we need to take the sum <em>after</em> the agent has finished an episode for all the rewards obtained from the current state to the end of the episode. Then we average those returns over all of the available episodes. </p>
<p>Note that MC methods only apply for episodic tasks, which is one of its limitations in addition to having to wait until the episode is finished.</p>
<p>Note also that the agent can visit the same state more than once inside the same episode. We can take the sum starting from the first visit, or every visit, each yields a different algorithm. The first-visit algorithm is more suitable for tabular methods, while the every-visit algorithm is more suitable when using function approximation methods (such as neural networks).</p>
<h2 id="mrp-environment-for-prediction">MRP environment for prediction</h2>
<p>To be able to develop the methods of MC we would need to develop an MRP and MDP classes that is able to interact and collect experience from an environment for prediction and control, respectively. Below we show the skeleton of this class. But first we show some efficiency comparisons, you may skip directly to the MRP class <a href="#MRP-Class-for-prediction">section</a>.</p>
<div class="highlight"><pre><span></span><code><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">random</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">numpy.random</span><span class="w"> </span><span class="kn">import</span> <span class="n">rand</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="n">randint</span><span class="p">,</span> <span class="n">choice</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">random</span><span class="w"> </span><span class="kn">import</span> <span class="n">choices</span><span class="p">,</span> <span class="n">sample</span>
</code></pre></div>
<h3 id="comparing-speed-for-random-number-generation">Comparing speed for random number generation.</h3>
<p>Before we get started, it is useful to study our options for random number generation. We will be using random number generation intensively when we sample. Below we show how each function will take to perform 10^6 random number generation for 2 actions.
Qs = np.ones(4)
Qs[1] = 2
Qs[3] = 2</p>
<p>choice(np.where(Qs==Qs.max())[0]) # choices(Qs==Qs.max(), k=1)np.array(sample(range(1000), k=32))np.random.choice(1000, 32, replace=False)n=int(1e5)
print('with replacement')
%time for _ in range(n): choices(np.where(Qs==Qs.max())[0])[0]
%time for _ in range(n): choice (np.where(Qs==Qs.max())[0])
print()</p>
<h1 id="sampling-without-replacement">sampling without replacement</h1>
<p>print('without replacement')
%time for _ in range(n): sample(range(10000), k=32)
%time for _ in range(n): np.random.choice(10000, 32, replace=False)
print()</p>
<h1 id="when-we-have-a-binary-choice-such-as-when-use-greedy">when we have a binary choice (such as when use ε-greedy)</h1>
<p>print('binary choices')
%time for _ in range(n): np.random.randint(0,1)
%time for _ in range(n): np.random.binomial(1, p=0.5)
As we can see, the <em>choice**s**()</em> function is far more efficient. However, this is part of the story because the quality of the random number generation of these functions varies significantly. Usually, the more it takes, the better distribution it maintains. To that end, the binomial distribution seems to give a sweet spot for generating two actions, which is far more efficient than <em>choice()</em>, both of which are from numpy. However, the binomial is good for two actions only. If we want to deal with more actions, we can use np.random.multinomial, but it is less efficient than binomial. The choice**s**() function is the most efficient, but it has less quality which we compensate for by running more experiments (to eliminate the bias), while choice() is the least efficient of all of these functions. With that in mind, we develop the infrastructure for our RL algorithms. Of course, when we use random number generation, we need to use the seed() function to repeat a set of experiments consistently. The call will depend on the library that we use:
random.seed(0)
np.random.seed(0)
Below we also show a useful function to obtain the last n elements of a circular array.</p>
<h1 id="retruns-indexes-of-last-n-elements-that-spans-two-edges-of-an-array-i-is-current-index">retruns indexes of last n elements that spans two edges of an array, i is current index</h1>
<h1 id="also-it-retruns-the-element-of-current-index">also it retruns the element of current index</h1>
<p>def circular_n(A, i, n):
    N = len(A)
    i, n, inds = i%N, min(i+1, n), np.ones(N, dtype=bool)            <br />
    inds[i+1: N+1 - (n-i)] = False  # turn off indexes that we do not want, to deal with circular indexes
    return A[inds][-n:], A[i]</p>
<p>A = circular_n(A=np.arange(100), i=105, n=10)
Adef n_a_side(A=np.arange(24), n=4):
    return circular_n(A, i=len(A)+n-1, n=2*n)[0]
n_a_side()</p>
<h3 id="mrp-class-for-prediction">MRP Class for prediction</h3>
<p>In the following class, we will try to build a useful and generic MDP/MRP class that will serve our different needs in various RL coverage steps. In particular, we want the interact() and the steps() functions to be as flexible and generic as possible. Towards that end, we have constructed our class to have the following sections:</p>
<ol>
<li>Initialisation part: initialises the different variables necessary for our treatment</li>
<li>Buffer storage section: store experience</li>
<li>Steps section: takes a step in the environment and stores its correspondent dynamic (r,s,a). We have two types of steps: step_a(), suitable for most algorithms, and step_an(), which requires knowing the next action in advance. These two are useful in unifying the treatment of different RL algorithms, including prediction and control. For example, TD (prediction) and Q-learning(control) have a similar algorithm structure that entails using step_a(), while the Sarsa algorithm (control) uses step_an(). You will see these algorithms in the next lesson. Just be aware that you might want to change the default step function, step_a(), if your algorithm needs to know the next action, designated as <em>an</em>, to update its value function estimation.</li>
<li>Interact section: this part is the heart and soul of our class. It runs several episodes, each with several steps, until a goal is reached, a buffer is full, or some other condition is met.</li>
<li>Policy section: this is a set of policies according to which the agent will act. They can be either stationary (i.e., their probabilities do not change) or non-stationary (i.e., their probability will vary with Q, our action-value-function estimation).</li>
<li>Metric section: to measure the performance of our algorithms. Basically, we use three metrics: </li>
<li>the number of steps an agent took to reach a goal</li>
<li>the sum of rewards an agent collected during an episode</li>
<li>the root mean squared error of the value function estimation and the true values of an MDP or MTRP problem. This metric implies that we know a solution for a prediction in advance.</li>
<li>Visualisation functions can be overridden in children's classes as per our needs.</li>
</ol>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">env.grid</span><span class="w"> </span><span class="kn">import</span> <span class="o">*</span>
</code></pre></div>
<style>.container {width:90% !important}</style>

<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">MRP</span><span class="p">:</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env</span><span class="o">=</span><span class="n">randwalk</span><span class="p">(),</span> <span class="n">γ</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">α</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">v0</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">view</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                 <span class="n">store</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="c1"># Majority of methods are pure one-step online and no need to store episodes trajectories </span>
                 <span class="n">max_t</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">visual</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">underhood</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> 
                 <span class="n">last</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">print_</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>


        <span class="c1"># hyper parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env</span> <span class="o">=</span> <span class="n">env</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">γ</span> <span class="o">=</span> <span class="n">γ</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">α</span> <span class="o">=</span> <span class="n">α</span> <span class="c1"># average methods(like MC1st) do not need this but many other methods (like MCα) do</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v0</span> <span class="o">=</span> <span class="n">v0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">episodes</span> <span class="o">=</span> <span class="n">episodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">store</span> <span class="o">=</span> <span class="n">store</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_t</span> <span class="o">=</span> <span class="n">max_t</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">visual</span> <span class="o">=</span> <span class="n">visual</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">view</span> <span class="o">=</span> <span class="n">view</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">underhood</span> <span class="o">=</span> <span class="n">underhood</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last</span> <span class="o">=</span> <span class="n">last</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">print</span> <span class="o">=</span> <span class="n">print_</span>

        <span class="c1"># reference to two important functions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">policy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stationary</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_a</span>
        <span class="c1"># we might want to skip a step</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">skipstep</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="n">nA</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">nA</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">As</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">nA</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pAs</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="o">/</span><span class="n">nA</span><span class="p">]</span><span class="o">*</span><span class="n">nA</span>

        <span class="c1"># useful to repeate the same experiement</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
        <span class="c1"># to protect interact() in case of no training </span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ep</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> 

    <span class="c1"># set up important metrics</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">init_metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Ts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">episodes</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint32</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Rs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">episodes</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Es</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">episodes</span><span class="p">)</span>  

    <span class="k">def</span><span class="w"> </span><span class="nf">extend_metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Ts</span><span class="p">)</span><span class="o">&gt;=</span><span class="bp">self</span><span class="o">.</span><span class="n">episodes</span><span class="p">:</span> <span class="k">return</span> <span class="c1"># no need to resize if size is still sufficient</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Ts</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">episodes</span><span class="p">,</span> <span class="n">refcheck</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Rs</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">episodes</span><span class="p">,</span> <span class="n">refcheck</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Es</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">episodes</span><span class="p">,</span> <span class="n">refcheck</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># set up the V table</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">init_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">nS</span><span class="p">)</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">v0</span>

    <span class="c1"># useful for inheritance, gives an expected return (value) for state s</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">V_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span> 
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span>  <span class="k">if</span> <span class="n">s</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">seed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">);</span> <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="c1">#-------------------------------------------buffer related-------------------------------------------------</span>
    <span class="c1"># The buffer get reinitialised by reinitialising t only but we have to be careful not to exceed t+1 at any time</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">allocate</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> 
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">store</span><span class="p">:</span> <span class="k">return</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_t</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_t</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint32</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">nS</span><span class="o">+</span><span class="mi">10</span><span class="p">)</span> <span class="c1"># states are indices:*(nS+10)for debugging </span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_t</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint32</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">nA</span><span class="o">+</span><span class="mi">10</span><span class="p">)</span> <span class="c1"># actions are indices:*(nA+10)for debugging       </span>
        <span class="bp">self</span><span class="o">.</span><span class="n">done</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_t</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">store_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">a</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">rn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">sn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">an</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">done</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">store</span><span class="p">:</span> <span class="k">return</span>

        <span class="k">if</span> <span class="n">s</span>  <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">s</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">s</span>
        <span class="k">if</span> <span class="n">a</span>  <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span>
        <span class="k">if</span> <span class="n">rn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">rn</span>
        <span class="k">if</span> <span class="n">sn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">s</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">sn</span>
        <span class="k">if</span> <span class="n">an</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">an</span>
        <span class="k">if</span> <span class="n">done</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">done</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">done</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">stop_ep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">done</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">done</span> <span class="ow">or</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">store</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_t</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># goal reached or storage is full</span>

    <span class="c1"># ------------------------------------ experiments related --------------------------------------------</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">stop_exp</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stop_early</span><span class="p">():</span> <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;experience stopped at episode </span><span class="si">%d</span><span class="s1">&#39;</span><span class="o">%</span><span class="bp">self</span><span class="o">.</span><span class="n">ep</span><span class="p">);</span> <span class="k">return</span> <span class="kc">True</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">ep</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">episodes</span> <span class="o">-</span> <span class="mi">1</span>

    <span class="c1">#----------------------------------- 🐾steps as per the algorithm style --------------------------------</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">step_0</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>                                 <span class="c1"># set env/agent to the start position</span>
        <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">policy</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">s</span><span class="p">,</span><span class="n">a</span>

    <span class="c1"># accomodates Q-learning and V style algorithms</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">step_a</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span><span class="n">_</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>                          
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">skipstep</span><span class="p">:</span> <span class="k">return</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">True</span>
        <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">policy</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="n">sn</span><span class="p">,</span> <span class="n">rn</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

        <span class="c1"># we added s=s for compatibility with deep learning</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">store_</span><span class="p">(</span><span class="n">s</span><span class="o">=</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">a</span><span class="p">,</span> <span class="n">rn</span><span class="o">=</span><span class="n">rn</span><span class="p">,</span> <span class="n">sn</span><span class="o">=</span><span class="n">sn</span><span class="p">,</span> <span class="n">done</span><span class="o">=</span><span class="n">done</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">t</span><span class="p">)</span>

        <span class="c1"># None is returned for compatibility with other algorithms</span>
        <span class="k">return</span> <span class="n">rn</span><span class="p">,</span><span class="n">sn</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span><span class="kc">None</span><span class="p">,</span> <span class="n">done</span>

    <span class="c1"># accomodates Sarsa style algorithms</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">step_an</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>                          
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">skipstep</span><span class="p">:</span> <span class="k">return</span> <span class="mi">0</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">True</span>
        <span class="n">sn</span><span class="p">,</span> <span class="n">rn</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
        <span class="n">an</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">policy</span><span class="p">(</span><span class="n">sn</span><span class="p">)</span>

        <span class="c1"># we added s=s for compatibility with deep learning later</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">store_</span><span class="p">(</span><span class="n">s</span><span class="o">=</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">a</span><span class="p">,</span> <span class="n">rn</span><span class="o">=</span><span class="n">rn</span><span class="p">,</span> <span class="n">sn</span><span class="o">=</span><span class="n">sn</span><span class="p">,</span> <span class="n">an</span><span class="o">=</span><span class="n">an</span><span class="p">,</span> <span class="n">done</span><span class="o">=</span><span class="n">done</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">t</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">rn</span><span class="p">,</span><span class="n">sn</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span><span class="n">an</span><span class="p">,</span> <span class="n">done</span>

    <span class="c1">#------------------------------------ 🌖 online learning and interaction --------------------------------</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">interact</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">resume</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">grid_img</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">episodes</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">episodes</span><span class="o">=</span><span class="n">episodes</span>
        <span class="k">if</span> <span class="n">train</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">resume</span><span class="p">:</span> <span class="c1"># train from scratch or resume training</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>                                        <span class="c1"># user defined init() before all episodes</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_metrics</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">allocate</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">plot0</span><span class="p">()</span>                                       <span class="c1"># useful to see initial V values</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="o">**</span><span class="n">kw</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ep</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="c1">#+ (not train)*(self.episodes-1)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">t_</span> <span class="o">=</span> <span class="mi">0</span>                                        <span class="c1"># steps counter for all episodes</span>

        <span class="k">if</span> <span class="n">resume</span><span class="p">:</span> 
            <span class="bp">self</span><span class="o">.</span><span class="n">extend_metrics</span><span class="p">()</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="c1">#for self.ep in range(self.episodes):</span>
            <span class="k">while</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">stop_exp</span><span class="p">():</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">ep</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">t</span>  <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>                                    <span class="c1"># steps counter for curr episode</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">Σr</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="c1">#print(self.ep)</span>
                <span class="c1"># initial step</span>
                <span class="n">s</span><span class="p">,</span><span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_0</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">step0</span><span class="p">()</span>                                    <span class="c1"># user defined init of each episode</span>
                <span class="c1"># an episode is a set of steps, interact and learn from experience, online or offline.</span>
                <span class="k">while</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">stop_ep</span><span class="p">(</span><span class="n">done</span><span class="p">):</span>
                    <span class="c1">#print(self.t_)</span>

                    <span class="c1"># take one step</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">t</span> <span class="o">+=</span> <span class="mi">1</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">t_</span><span class="o">+=</span> <span class="mi">1</span>

                    <span class="n">rn</span><span class="p">,</span><span class="n">sn</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span><span class="n">an</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">t</span><span class="p">)</span>  <span class="c1"># takes a step in env and store tarjectory if needed</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">online</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">rn</span><span class="p">,</span><span class="n">sn</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span><span class="n">an</span><span class="p">)</span> <span class="k">if</span> <span class="n">train</span> <span class="k">else</span> <span class="kc">None</span> <span class="c1"># to learn online, pass a one step trajectory</span>

                    <span class="bp">self</span><span class="o">.</span><span class="n">Σr</span> <span class="o">+=</span> <span class="n">rn</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">rn</span> <span class="o">=</span> <span class="n">rn</span>
                    <span class="n">s</span><span class="p">,</span><span class="n">a</span> <span class="o">=</span> <span class="n">sn</span><span class="p">,</span><span class="n">an</span>

                    <span class="c1"># render last view episodes, for games ep might&gt;episodes</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">visual</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">episodes</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">ep</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">episodes</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">view</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="o">**</span><span class="n">kw</span><span class="p">)</span>

                <span class="c1"># to learn offline and plot episode</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">()</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">offline</span><span class="p">()</span> <span class="k">if</span> <span class="n">train</span> <span class="k">else</span> <span class="kc">None</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">plot_ep</span><span class="p">()</span>

        <span class="k">except</span><span class="p">:</span> <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;training was interrupted.......!&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">pause</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>

        <span class="c1"># plot experience   </span>
        <span class="bp">self</span><span class="o">.</span><span class="n">plot_exp</span><span class="p">(</span><span class="o">**</span><span class="n">kw</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span>  
    <span class="c1">#------------------------------------- policies types 🧠-----------------------------------</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">stationary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="c1">#return choice(self.As, 1, p=self.pAs)[0] # this gives better experiements quality but is less efficient</span>
        <span class="k">return</span> <span class="n">choices</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">As</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pAs</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">nA</span><span class="o">!=</span><span class="mi">2</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>

    <span class="c1">#---------------------------------------perfromance metrics📏 ------------------------------</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># we use %self.episodes so that when we use a different criterion to stop_exp() code will run</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Ts</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">ep</span><span class="o">%</span><span class="bp">self</span><span class="o">.</span><span class="n">episodes</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Rs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">ep</span><span class="o">%</span><span class="bp">self</span><span class="o">.</span><span class="n">episodes</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Σr</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Es</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">ep</span><span class="o">%</span><span class="bp">self</span><span class="o">.</span><span class="n">episodes</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Error</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">print</span><span class="p">:</span> <span class="nb">print</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__str__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># mean works regardless of where we stored the episode metrics (we use %self.episodes)     </span>
        <span class="n">Rs</span><span class="p">,</span> <span class="n">R</span> <span class="o">=</span> <span class="n">circular_n</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Rs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ep</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">last</span><span class="p">)</span> <span class="c1"># this function is defined above</span>
        <span class="n">metrics</span> <span class="o">=</span> <span class="s1">&#39;step </span><span class="si">%d</span><span class="s1">, episode </span><span class="si">%d</span><span class="s1">, r </span><span class="si">%.2f</span><span class="s1">, mean r last </span><span class="si">%d</span><span class="s1"> ep </span><span class="si">%.2f</span><span class="s1">, ε </span><span class="si">%.2f</span><span class="s1">&#39;</span>
        <span class="n">values</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">t_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ep</span><span class="p">,</span> <span class="n">R</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">last</span><span class="p">,</span> <span class="n">Rs</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="nb">round</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ε</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">metrics</span><span class="o">%</span><span class="n">values</span>

    <span class="c1">#------------------------functions that can be overridden in the child class-----------------</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">init</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">step0</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">Error</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">0</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">stop_early</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">plot0</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">plot_t</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">plot_ep</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">plot_exp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="k">pass</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">offline</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">pass</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">online</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="k">pass</span>
    <span class="c1">#---------------------------------------visualise ✍️----------------------------------------</span>
    <span class="c1"># overload the env render function</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">render</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">rn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> <span class="n">rn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">rn</span>
        <span class="n">param</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;V&#39;</span><span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">V_</span><span class="p">()}</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">underhood</span><span class="o">==</span><span class="s1">&#39;V&#39;</span> <span class="k">else</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="o">**</span><span class="n">param</span><span class="p">,</span> 
                        <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="o">+</span><span class="s1">&#39; reward=</span><span class="si">%d</span><span class="s1">, t=</span><span class="si">%d</span><span class="s1">, ep=</span><span class="si">%d</span><span class="s1">&#39;</span><span class="o">%</span><span class="p">(</span><span class="n">rn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ep</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> 
                        <span class="n">underhood</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">underhood</span><span class="p">,</span> 
                        <span class="o">**</span><span class="n">kw</span><span class="p">)</span>
</code></pre></div>
<p>As we can see, we defined a form of Markov Decision Process-MDP called Markov Reward Process-MRP. Like an MDP, an MRP is a stochastic process that concentrates on the rewards and states only and neutralizes the effect of actions. It is useful to study the predictive capabilities of an RL method where there are no decisions(actions) to be taken, and only we try to guess(predict) the returns of a process. </p>
<p>Whenever there is a predictive algorithm, we will use <strong>MRP</strong>, while when we develop a control algorithm, we will use <strong>MDP</strong>. </p>
<p>A typical example of an MRP is a random walk process, where an agent randomly moves left or right in a straight line of cells. A terminal state is at the end of each direction (left and right). The agent can be rewarded differently in each cell. Often, we reward the agent for moving to the far-right terminal state by 1 and everywhere else with 0. Another type of reward is to give the agent a negative -1 penalty on the far-left terminal state and 1 on the far-right state, and 0 everywhere else. See page 125 of the book.</p>
<p>Note that the only assumption about the environment is to provide a reset() and a step() functions that abide by the following general form:
1. reset() must return a value of the initial state with a proper representation. So, when we move to function approximation, it must return a vector representing the state.
2. step() must return four values,  the first is the state (observation) that is compatible with what is returned by reset(). The second is the reward for the current state, and the third is a flag to signal the end of an episode; usually, when the agent achieves the required task or fails for some reason, each would have a corresponding suitable reward. A fourth is an empty dictionary of information we provided for compatibility with openAI environments.</p>
<p>Let us now move to define our 1<sup>st</sup>-visit Monte Carlo <em>prediction</em> method. This method averages the return for only the first visit of a state in each episode.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">MC1st</span><span class="p">(</span><span class="n">MRP</span><span class="o">=</span><span class="n">MRP</span><span class="p">):</span>
    <span class="k">class</span><span class="w"> </span><span class="nc">MC1st</span><span class="p">(</span><span class="n">MRP</span><span class="p">):</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">init</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">store</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ΣV</span>   <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="o">*</span><span class="mi">0</span>      <span class="c1"># the sum of returns for all episodes</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ΣepV</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="o">*</span><span class="mi">0</span>      <span class="c1"># counts for numbers of times we add to the return  </span>

        <span class="c1"># ----------------------------- 🌘 offline, MC learning: end-of-episode learning 🧑🏻‍🏫 --------------------------------    </span>
        <span class="c1"># MC1stVisit average all past visits to a state in all episodes to get its return estimates</span>
        <span class="c1"># we simply override the offline() function of the parent class</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">offline</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

            <span class="c1">#initialise the values</span>
            <span class="n">Vs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="o">*</span><span class="mi">0</span>
            <span class="n">epV</span><span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="o">*</span><span class="mi">0</span>

            <span class="c1"># obtain the return for the latest episode</span>
            <span class="n">Gt</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">t</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
                <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">s</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
                <span class="n">rn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>

                <span class="n">Gt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">γ</span><span class="o">*</span><span class="n">Gt</span> <span class="o">+</span> <span class="n">rn</span>
                <span class="n">Vs</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">Gt</span>
                <span class="n">epV</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

            <span class="c1"># add the counts to the experience and obtain the average as per MC estimates</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ΣV</span>   <span class="o">+=</span> <span class="n">Vs</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ΣepV</span> <span class="o">+=</span> <span class="n">epV</span>
            <span class="n">ind</span> <span class="o">=</span> <span class="n">epV</span><span class="o">&gt;</span><span class="mi">0</span> <span class="c1"># avoids /0</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ΣV</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">ΣepV</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span> 

    <span class="k">return</span> <span class="n">MC1st</span>
</code></pre></div>
<p>Let us now try our new class to predict the values of a random walk MRP.</p>
<div class="highlight"><pre><span></span><code><span class="n">MC</span> <span class="o">=</span> <span class="n">MC1st</span><span class="p">(</span><span class="n">MRP</span><span class="p">)</span>
<span class="n">MC</span> <span class="o">=</span> <span class="n">MC</span><span class="p">(</span><span class="n">episodes</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">MC</span><span class="o">.</span><span class="n">V</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>[0.15780998 0.31454784 0.477      0.65342466 0.83831283]
</code></pre></div>
<p>As we can see the values are close to the analytical true values for this process given below.</p>
<div class="highlight"><pre><span></span><code><span class="n">pr</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="n">randwalk</span><span class="p">()</span><span class="o">.</span><span class="n">nS</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># 1/6 </span>
<span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">pr</span><span class="p">,</span><span class="mi">1</span><span class="o">-</span><span class="n">pr</span><span class="p">,</span> <span class="n">pr</span><span class="p">)</span>   <span class="c1"># true values</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>array([0.16666667, 0.33333333, 0.5       , 0.66666667, 0.83333333])
</code></pre></div>
<h2 id="mrp-with-visualisation">MRP with visualisation</h2>
<p>To help us to visualize the learning that is taking place in each episode, we have created a set of visualization functions that we will add to the MRP class. Familiarize yourself with these functions, they are self-explanatory. Mainly we have one function for plotting after each episode, not surprisingly called plot_ep(), and another function called plot_exp() that will be called at the end of the experience (after finishing all episodes). In addition, we have an Error() function to calculate the RMSE  between the true values and the predicted values of the states as well as plot_V() function that visualises the predicted values and true values to see visually how the algorithm is doing to come closer towards the true values.</p>
<p>As we did with the Grid class, we will call the child name the same name as the parent (MRP) to help us keep the code consistent and simplify the treatments of our classes when we import a class. The downside is that you would have to re-execute the first parent and its subsequent children if you want to make some changes to the class since it will keep adding to previous definitions, so please be mindful of this point.</p>
<p>We have also tried to reduce the overhead as much as possible for the new class by setting up visualisation only when it is necessary (when one of the plot functions is called)</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">MRP</span><span class="p">(</span><span class="n">MRP</span><span class="p">):</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">plotV</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="n">plotT</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">plotR</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">plotE</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">animate</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">Vstar</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kw</span><span class="p">)</span>

        <span class="c1"># visualisation related</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">plotT</span> <span class="o">=</span> <span class="n">plotT</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">plotR</span> <span class="o">=</span> <span class="n">plotR</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">plotE</span> <span class="o">=</span> <span class="n">plotE</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">plotV</span> <span class="o">=</span> <span class="n">plotV</span> 
        <span class="bp">self</span><span class="o">.</span><span class="n">animate</span> <span class="o">=</span> <span class="n">animate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eplist</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="n">nS</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">nS</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Vstar</span> <span class="o">=</span> <span class="n">Vstar</span> <span class="k">if</span> <span class="n">Vstar</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">Vstar</span>
    <span class="c1">#------------------------------------------- metrics📏 -----------------------------------------------  </span>
    <span class="c1"># returns RMSE but can be overloaded if necessary</span>
    <span class="c1"># when Vstar=0, it shows how V is evolving via training </span>
    <span class="k">def</span><span class="w"> </span><span class="nf">Error</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">Vstar</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> <span class="k">return</span> <span class="mi">0</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(((</span><span class="bp">self</span><span class="o">.</span><span class="n">V_</span><span class="p">()</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">Vstar</span><span class="p">)[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span> <span class="c1">#if self.Vstar is not None else 0</span>

    <span class="c1">#--------------------------------------------visualise ✍️----------------------------------------------</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">plot0</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">plotV</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">plot_V</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">plot_exp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">plot_ep</span><span class="p">(</span><span class="n">animate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">plot_exp</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">plot_ep</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">animate</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">plot_exp</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">):</span> 
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">eplist</span><span class="p">)</span><span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">episodes</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">eplist</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ep</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">animate</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> <span class="n">animate</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">animate</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">animate</span><span class="p">:</span> <span class="k">return</span>
        <span class="n">frmt</span><span class="o">=</span><span class="s1">&#39;.--&#39;</span><span class="k">if</span> <span class="ow">not</span> <span class="n">plot_exp</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">ep</span><span class="o">==</span><span class="mi">0</span> <span class="k">else</span> <span class="s1">&#39;--&#39;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">visual</span><span class="p">:</span> 
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">ep</span><span class="o">==</span><span class="bp">self</span><span class="o">.</span><span class="n">episodes</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">animate</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="c1"># shows the policy </span>
            <span class="k">else</span><span class="p">:</span>                        <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">animate</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> 
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">plotV</span><span class="p">:</span>  <span class="bp">self</span><span class="o">.</span><span class="n">plot_V</span><span class="p">(</span><span class="n">ep</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ep</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>        

        <span class="n">i</span><span class="o">=</span><span class="mi">2</span>
        <span class="k">for</span> <span class="n">plot</span><span class="p">,</span> <span class="n">ydata</span><span class="p">,</span> <span class="n">label_</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">plotT</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">plotR</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">plotE</span><span class="p">],</span> 
                                      <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">Ts</span><span class="p">,</span>    <span class="bp">self</span><span class="o">.</span><span class="n">Rs</span><span class="p">,</span>    <span class="bp">self</span><span class="o">.</span><span class="n">Es</span>   <span class="p">],</span> 
                                      <span class="p">[</span><span class="s1">&#39;steps   &#39;</span><span class="p">,</span> <span class="s1">&#39;Σrewards&#39;</span><span class="p">,</span> <span class="s1">&#39;Error   &#39;</span><span class="p">]):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">plot</span><span class="p">:</span> <span class="k">continue</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="nb">min</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">eplist</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">ep</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="n">ydata</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">ep</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="n">frmt</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label_</span><span class="o">+</span><span class="n">label</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;episodes&#39;</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
            <span class="n">i</span><span class="o">+=</span><span class="mi">1</span>

        <span class="c1"># if there is any visualisation required then we need to care for special cases    </span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">plotV</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">plotE</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">plotT</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">plotR</span><span class="p">:</span>
            <span class="n">figsizes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">get_size_inches</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">figsize0</span><span class="p">))</span>
            <span class="n">figsize</span>  <span class="o">=</span> <span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="n">figsizes</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">min</span><span class="p">(</span><span class="n">figsizes</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">plotV</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">plotE</span> <span class="k">else</span> <span class="n">figsizes</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">]]</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="n">figsize</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">figsize</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">clear_output</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">plot_exp</span><span class="p">:</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>


    <span class="k">def</span><span class="w"> </span><span class="nf">plot_V</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ep</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">ax0</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># to add this axis next to a another axis to save some spaces</span>
<span class="c1">#         plt.gcf().set_size_inches(16, 2)</span>

        <span class="c1"># get letter as state names if no more than alphabet else just give them numbers</span>
        <span class="n">letters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">letters_list</span><span class="p">()[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">nS</span><span class="o">&lt;</span><span class="mi">27</span> <span class="k">else</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">nS</span><span class="o">-</span><span class="mi">2</span><span class="p">))</span>

        <span class="c1"># plot the estimated values against the optimal values</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">letters</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">V_</span><span class="p">()[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;.-&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;V episode=</span><span class="si">%d</span><span class="s1">&#39;</span><span class="o">%</span><span class="p">(</span><span class="n">ep</span><span class="p">))</span> <span class="c1"># useful for randwalk</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">letters</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Vstar</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="s1">&#39;.-k&#39;</span><span class="p">)</span>

        <span class="c1"># set up the figure</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;State&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Estimated value for </span><span class="si">%d</span><span class="s1"> non-terminal states&#39;</span><span class="o">%</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">nS</span><span class="o">-</span><span class="mi">2</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;right&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s1">&#39;top&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</code></pre></div>
<p>Ok to reap the benefit of this newly defined MRP, we would have had to go back to the Jupyter-cell where we defined the MC1st class and run. However, because we have used a class factory function with MC1st we can just call it and it will dynamically update the MV1st class with teh new MRP base class. Below we show you how to do it.</p>
<h3 id="applying-mc1st-on-a-prediction-problem">Applying MC1st on a prediction problem</h3>
<p>Let us now run MC1st with the latest useful visualisation.</p>
<div class="highlight"><pre><span></span><code><span class="n">MC1st</span> <span class="o">=</span> <span class="n">MC1st</span><span class="p">(</span><span class="n">MRP</span><span class="p">)</span>
</code></pre></div>
<p>Note that if you get an error after changing something in the MRP class and rerunning then simply restart the kernel and run from scratch.</p>
<div class="highlight"><pre><span></span><code><span class="n">mc</span> <span class="o">=</span> <span class="n">MC1st</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">randwalk</span><span class="p">(),</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">plotV</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">plotE</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_40_0.png" /></p>
<p>As you can see we have called 
MC = MC1st(MRP)
to make sure that we are dealing with latest MRP definition.</p>
<div class="highlight"><pre><span></span><code><span class="n">mc</span> <span class="o">=</span> <span class="n">MC1st</span><span class="p">(</span><span class="n">episodes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">plotE</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_42_0.png" /></p>
<div class="highlight"><pre><span></span><code><span class="n">mc</span> <span class="o">=</span> <span class="n">MC1st</span><span class="p">(</span><span class="n">episodes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">plotV</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_43_0.png" /></p>
<p>Ok one more thing, to avoid passing the value of plotE=True, plotV=True, animate=True, whenever we want to demo a prediction algorithm, we can create a dictionary and store these values in it and then pass the reference to the MC1st call, below we show how.</p>
<div class="highlight"><pre><span></span><code><span class="n">demoV</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;plotE&#39;</span><span class="p">:</span><span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;plotV&#39;</span><span class="p">:</span><span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;animate&#39;</span><span class="p">:</span><span class="kc">True</span><span class="p">}</span> <span class="c1"># suitable for prediction</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">mc</span> <span class="o">=</span> <span class="n">MC1st</span><span class="p">(</span><span class="n">episodes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="o">**</span><span class="n">demoV</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_46_0.png" /></p>
<div class="highlight"><pre><span></span><code><span class="n">mc</span><span class="o">.</span><span class="n">ep</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>99
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">mc</span><span class="o">.</span><span class="n">V</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>array([0.        , 0.16666667, 0.34210526, 0.5       , 0.63291139,
       0.79365079, 0.        ])
</code></pre></div>
<h2 id="mdp-environment-for-control">MDP environment for control</h2>
<p>Let us now extend our MRP class to deal with control. We would need to deal with the Q action-value function instead of the value function V. Also lacking is a set of non-stationary policies that allows us to take advantage of the Q action-value function. Below we show this implementation. </p>
<p>We also use a class factory to define our MDP class. Doing so will save us from redefining the class again when we amend our MRP class. We will need to amend the MRP when we change the state representation to use function approximation in the next unit.</p>
<p>The q0 is the initial set of values we might want to set up for all our Q estimates. We can also opt for completely random values for each action-value pair, we have left this out for simplicity of the coverage, but you can try it yourself.  ε is the percentage of time we want our agent to explore. </p>
<p>The class defines a set of policy-related functions that revolve around the ε-greedy policy. We have implemented a simple, deterministic greedy policy that always chooses the first max Q action greedy_(). The main difference between greedy_() and εgreedy() for ε=0 is that the latter stochastically chooses between multiple <em>optimum</em> actions with the same action-value function. This is useful when we use exploration by optimistic initialisation since the greedy_() function can cause action starvation (a phenomenon where the action is never selected). Nevertheless, greedy_() is useful to test the optimality of a learned policy (once learning finishes) and is used within πisoptimal() function.</p>
<p>The πisoptimal() function returns whether the current policy is optimal by checking if the agent can reach the goal in a predefined number of steps stored in self.Tstar. The π() function returns the probability of taking a certain action under the ε-greedy policy. Finally, the render() function deals with rendering a policy.</p>
<p>Below we show a simple example of how choices will work when we use weights to choose an action according to its Q value.</p>
<div class="highlight"><pre><span></span><code><span class="n">Qs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">])</span>
<span class="n">τ</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">exp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">Qs</span><span class="o">/</span><span class="n">τ</span><span class="p">)</span>
<span class="n">choices</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">weights</span><span class="o">=</span><span class="n">exp</span><span class="o">/</span><span class="n">exp</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>1
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">Qs</span><span class="o">==</span><span class="n">Qs</span><span class="o">.</span><span class="n">max</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>array([1, 2, 3])
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">a</span><span class="o">=</span><span class="mi">4</span>
<span class="n">maxAs</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span> <span class="p">,</span><span class="mi">3</span><span class="p">]</span>
<span class="n">a</span> <span class="ow">in</span> <span class="n">maxAs</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>False
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">choices</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">6</span><span class="p">])</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>[6]
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">MDP</span><span class="p">(</span><span class="n">MRP</span><span class="o">=</span><span class="n">MRP</span><span class="p">):</span>
    <span class="k">class</span><span class="w"> </span><span class="nc">MDP</span><span class="p">(</span><span class="n">MRP</span><span class="p">):</span>
        <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(),</span> <span class="n">commit_ep</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ε</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">εmin</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">dε</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">εT</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">q0</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">Tstar</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">):</span> 

            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">)</span>
            <span class="c1"># set up hyper parameters</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ε</span> <span class="o">=</span> <span class="n">ε</span> 
            <span class="bp">self</span><span class="o">.</span><span class="n">ε0</span> <span class="o">=</span> <span class="n">ε</span>  <span class="c1"># store initial </span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dε</span> <span class="o">=</span> <span class="n">dε</span> <span class="c1"># for exp decay</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">εT</span> <span class="o">=</span> <span class="n">εT</span> <span class="c1"># for lin decay</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">εmin</span> <span class="o">=</span> <span class="n">εmin</span>

            <span class="c1"># override the policy to εgreedy to make control possible</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">policy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">εgreedy</span>

            <span class="c1"># initial Q values</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">q0</span> <span class="o">=</span> <span class="n">q0</span>

            <span class="c1"># which episode to commit changes</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">commit_ep</span> <span class="o">=</span> <span class="n">commit_ep</span>

            <span class="c1"># number of steps for optimal policy</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">Tstar</span> <span class="o">=</span> <span class="n">Tstar</span>

        <span class="c1"># set up the Q table</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">init_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">init_</span><span class="p">()</span> <span class="c1"># initialises V</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">nS</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">nA</span><span class="p">))</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">q0</span>

        <span class="c1">#------------------------------------- add some more policies types 🧠-------------------------------</span>
        <span class="c1"># useful for inheritance, gives us a vector of actions values</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">Q_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="k">if</span> <span class="n">s</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span>

        <span class="c1"># directly calculates V as a π[s] policy expectation of Q[s] </span>
        <span class="k">def</span><span class="w"> </span><span class="nf">V_from_Q</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q_</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="nd">@self</span><span class="o">.</span><span class="n">π</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

        <span class="c1"># returns a pure greedy action, **not to be used in learning**</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">greedy_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Q_</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>


        <span class="c1"># greedy stochastic MaxQ</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">greedy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span> 
            <span class="bp">self</span><span class="o">.</span><span class="n">isamax</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="c1"># instead of returning np.argmax(Q[s]) get all max actions and return one of the max actions randomly</span>
            <span class="n">Qs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q_</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
            <span class="c1">#print(Qs)</span>
            <span class="k">if</span> <span class="n">Qs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">==</span><span class="mi">1</span><span class="p">:</span> <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;something might be wrong number of actions ==1&#39;</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">choices</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">Qs</span><span class="o">==</span><span class="n">Qs</span><span class="o">.</span><span class="n">max</span><span class="p">())[</span><span class="mi">0</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># more efficient than choice</span>
            <span class="c1">#return choice(np.where(Qs==Qs.max())[0])</span>


        <span class="c1"># returns a greedy action most of the time</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">εgreedy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
            <span class="c1"># there is pr=ε/nA that a max action is chosen but is not considered max, we ignored it in favour of efficiency</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">isamax</span> <span class="o">=</span> <span class="kc">False</span> 
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dε</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">ε</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">εmin</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ε</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">dε</span><span class="p">)</span>              <span class="c1"># exponential decay</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">εT</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">ε</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">εmin</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ε0</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">εT</span><span class="p">)</span> <span class="c1"># linear      decay</span>

            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">greedy</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">if</span> <span class="n">rand</span><span class="p">()</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">ε</span> <span class="k">else</span> <span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">nA</span><span class="p">)</span>

        <span class="c1"># returns the policy probabilities (of selecting a specific action)</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">π</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sn</span><span class="p">,</span>  <span class="n">a</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
            <span class="n">ε</span><span class="p">,</span> <span class="n">nA</span><span class="p">,</span> <span class="n">Qsn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ε</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">nA</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q_</span><span class="p">(</span><span class="n">sn</span><span class="p">)</span>
            <span class="n">π_</span> <span class="o">=</span> <span class="n">Qsn</span><span class="o">*</span><span class="mi">0</span> <span class="o">+</span> <span class="n">ε</span><span class="o">/</span><span class="n">nA</span>
            <span class="n">π_</span><span class="p">[</span><span class="n">Qsn</span><span class="o">.</span><span class="n">argmax</span><span class="p">()]</span> <span class="o">+=</span> <span class="mi">1</span><span class="o">-</span><span class="n">ε</span>
            <span class="k">return</span> <span class="n">π_</span> <span class="k">if</span> <span class="n">a</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">π_</span><span class="p">[</span><span class="n">a</span><span class="p">]</span>

        <span class="c1"># returns whether the current policy is optimal by checking if agent can reach the goal in self.Tstar</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">πisoptimal</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
            <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Tstar</span><span class="p">):</span>
                <span class="n">s</span><span class="p">,</span><span class="n">_</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">greedy_</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
            <span class="k">return</span> <span class="n">done</span>

        <span class="c1">#---------------------------------------visualise ✍️----------------------------------------</span>
        <span class="c1"># override the render function</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">render</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">rn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> <span class="n">rn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">rn</span>
            <span class="n">param</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;Q&#39;</span><span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">Q_</span><span class="p">()}</span> <span class="k">if</span> <span class="s1">&#39;Q&#39;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">underhood</span> <span class="k">else</span> <span class="p">{}</span> <span class="c1"># &#39;maxQ&#39; or &#39;Q&#39;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="o">**</span><span class="n">param</span><span class="p">,</span> 
                            <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="o">+</span><span class="s1">&#39; reward=</span><span class="si">%d</span><span class="s1">, t=</span><span class="si">%d</span><span class="s1">, ep=</span><span class="si">%d</span><span class="s1">&#39;</span><span class="o">%</span><span class="p">(</span><span class="n">rn</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ep</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> 
                            <span class="n">underhood</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">underhood</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">MDP</span>
</code></pre></div>
<p>You might have realised that we used a class factory for MDP. This is because we want our class to be flexible later to accommodate for changes in the MRP parent class. That is if we change the MRP class in later lessons we do not need to restate the MDP definition to inherit from the new MRP class, instead we just pass MDP(MRP) where MRP will be taken as the latest definition. This is will be appreciated in later lessons.</p>
<h2 id="first-visit-mc-control">First-visit MC control</h2>
<p>Now we extend this class to overload the offline function to offload it with our 1<sup>st</sup>-visit Monte Carlo method for control.</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">MC1stControl</span><span class="p">(</span><span class="n">MDP</span><span class="p">()):</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">init</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">store</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ΣQ</span>   <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="o">*</span><span class="mi">0</span>      <span class="c1"># the sum of returns for all episodes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ΣepQ</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="o">*</span><span class="mi">0</span>      <span class="c1"># counts for numbers of times we add to the return  </span>

    <span class="k">def</span><span class="w"> </span><span class="nf">offline</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> 
        <span class="c1">#initialise the values</span>
        <span class="n">Qs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="o">*</span><span class="mi">0</span>
        <span class="n">epQ</span><span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="o">*</span><span class="mi">0</span>

        <span class="c1"># obtain the return for the latest episode</span>
        <span class="n">Gt</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">t</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">s</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
            <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
            <span class="n">rn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>

            <span class="n">Gt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">γ</span><span class="o">*</span><span class="n">Gt</span> <span class="o">+</span> <span class="n">rn</span>
            <span class="n">Qs</span><span class="p">[</span><span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">]</span> <span class="o">=</span> <span class="n">Gt</span>
            <span class="n">epQ</span><span class="p">[</span><span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="c1"># add the counts to the experience and obtain the average as per MC estimates</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ΣQ</span>   <span class="o">+=</span> <span class="n">Qs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ΣepQ</span> <span class="o">+=</span> <span class="n">epQ</span>
        <span class="n">ind</span> <span class="o">=</span> <span class="n">epQ</span><span class="o">&gt;</span><span class="mi">0</span> <span class="c1"># avoid /0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ΣQ</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">ΣepQ</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span> 
</code></pre></div>
<h3 id="applying-mc-on-a-control-problem">Applying MC on a control problem</h3>
<p>Similar to what we did for prediction, we get help from a dictionary that stores a set of useful configurations that we use often. In the case of control, the most useful is plotting the number of steps the agent took to reach a terminal state in each episode or the sum of rewards the agent collected in each episode. Each one of these plots can be useful for certain tasks. Bear in mind that if the reward is given only for reaching the goal location or terminal state, the sum of the rewards plot would be a constant line that does not convey useful information. Below we show each.</p>
<div class="highlight"><pre><span></span><code><span class="n">demoT</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;plotT&#39;</span><span class="p">:</span><span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;visual&#39;</span><span class="p">:</span><span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;underhood&#39;</span><span class="p">:</span><span class="s1">&#39;maxQ&#39;</span><span class="p">}</span>                 <span class="c1"># suitable for control</span>
<span class="n">demoR</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;plotR&#39;</span><span class="p">:</span><span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;visual&#39;</span><span class="p">:</span><span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;underhood&#39;</span><span class="p">:</span><span class="s1">&#39;maxQ&#39;</span><span class="p">}</span>                 <span class="c1"># suitable for control</span>
<span class="n">demoTR</span><span class="o">=</span> <span class="p">{</span><span class="s1">&#39;plotT&#39;</span><span class="p">:</span><span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;plotR&#39;</span><span class="p">:</span><span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;visual&#39;</span><span class="p">:</span><span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;underhood&#39;</span><span class="p">:</span><span class="s1">&#39;maxQ&#39;</span><span class="p">}</span>   <span class="c1"># suitable for control</span>
<span class="n">demoQ</span> <span class="o">=</span> <span class="n">demoT</span> <span class="c1"># alias</span>
</code></pre></div>
<p>We can go a bit further and define a set of useful functions that we can utilise in all of our lessons which saves us from having to redefine the above dictionaries as follows.</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">demo</span><span class="p">(</span><span class="n">what</span><span class="o">=</span><span class="s1">&#39;V&#39;</span><span class="p">):</span>
    <span class="n">switch</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;V&#39;</span><span class="p">:</span>    <span class="p">{</span><span class="s1">&#39;plotE&#39;</span><span class="p">:</span><span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;plotV&#39;</span><span class="p">:</span><span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;animate&#39;</span><span class="p">:</span><span class="kc">True</span><span class="p">},</span>                    <span class="c1"># suitable for prediction</span>
        <span class="s1">&#39;T&#39;</span><span class="p">:</span>    <span class="p">{</span><span class="s1">&#39;plotT&#39;</span><span class="p">:</span><span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;visual&#39;</span><span class="p">:</span><span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;underhood&#39;</span><span class="p">:</span><span class="s1">&#39;maxQ&#39;</span><span class="p">},</span>               <span class="c1"># suitable for control</span>
        <span class="s1">&#39;R&#39;</span><span class="p">:</span>    <span class="p">{</span><span class="s1">&#39;plotR&#39;</span><span class="p">:</span><span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;visual&#39;</span><span class="p">:</span><span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;underhood&#39;</span><span class="p">:</span><span class="s1">&#39;maxQ&#39;</span><span class="p">},</span>               <span class="c1"># suitable for control</span>
        <span class="s1">&#39;TR&#39;</span><span class="p">:</span>   <span class="p">{</span><span class="s1">&#39;plotT&#39;</span><span class="p">:</span><span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;plotR&#39;</span><span class="p">:</span><span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;visual&#39;</span><span class="p">:</span><span class="kc">True</span><span class="p">,</span><span class="s1">&#39;underhood&#39;</span><span class="p">:</span><span class="s1">&#39;maxQ&#39;</span><span class="p">},</span>  <span class="c1"># suitable for control</span>
        <span class="s1">&#39;Game&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;plotT&#39;</span><span class="p">:</span><span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;plotR&#39;</span><span class="p">:</span><span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;visual&#39;</span><span class="p">:</span><span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;animate&#39;</span><span class="p">:</span><span class="kc">True</span><span class="p">}</span>      <span class="c1"># suitable for games</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">switch</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">what</span><span class="p">,{})</span>
<span class="k">def</span><span class="w"> </span><span class="nf">demoV</span><span class="p">():</span> <span class="k">return</span> <span class="n">demo</span><span class="p">(</span><span class="s1">&#39;V&#39;</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">demoT</span><span class="p">():</span> <span class="k">return</span> <span class="n">demo</span><span class="p">(</span><span class="s1">&#39;T&#39;</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">demoQ</span><span class="p">():</span> <span class="k">return</span> <span class="n">demo</span><span class="p">(</span><span class="s1">&#39;T&#39;</span><span class="p">)</span><span class="c1"># alias</span>
<span class="k">def</span><span class="w"> </span><span class="nf">demoR</span><span class="p">():</span> <span class="k">return</span> <span class="n">demo</span><span class="p">(</span><span class="s1">&#39;R&#39;</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">demoTR</span><span class="p">():</span> <span class="k">return</span> <span class="n">demo</span><span class="p">(</span><span class="s1">&#39;TR&#39;</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">demoGame</span><span class="p">():</span> <span class="k">return</span> <span class="n">demo</span><span class="p">(</span><span class="s1">&#39;Game&#39;</span><span class="p">)</span>
</code></pre></div>
<p>Ok, back to our MC algorithm.
Unfortunately, applying the MC control algorithm with the default reward function will not yield a useful policy. This is because the explore-start condition is not satisfied (refer to section 5.4 of our book). In addition, averaging solutions may not perform well because they do not track a changing policy well for non-stationary problems (most of the control problems are non-stationary). To see this, uncomment the lines in the cell below and run it. (Note that we have set up the priorities of the actions in a way that will show this issue (right comes before left and down before up)</p>
<div class="highlight"><pre><span></span><code><span class="n">mc</span> <span class="o">=</span> <span class="n">MC1stControl</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(),</span> <span class="n">γ</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>  <span class="n">seed</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="o">**</span><span class="n">demoQ</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_67_0.png" /></p>
<h3 id="the-role-of-the-discount-factor-gamma-for-delayed-reward">The role of the discount factor <span class="arithmatex">\(\gamma\)</span> for delayed reward</h3>
<p><strong>Important Note</strong>
It is always the case that when we use a <em>delayed reward</em> (which is the default reward for our Grid class), the discount factor <span class="arithmatex">\(\gamma\)</span> <strong>must not be set to 1</strong>. This is because the sum of the discounted rewards of each visited state will be equal to the delayed reward itself, which will not give any particular advantage to follow a shorter path, yielding a useless policy. Therefore, we can solve this issue 
1. either by providing a discounted value for <span class="arithmatex">\(\gamma\)</span> that &lt; 1.
1. or by changing the reward to have intermediate steps reward, which, when accumulated, will provide distinguished sums for the different paths and hence help distinguish the shortest path or the policy that will yield an optimal reward.</p>
<h3 id="solution-1">Solution 1</h3>
<p>Below we show how we can simply reduce <span class="arithmatex">\(\gamma\)</span> to solve this issue.</p>
<div class="highlight"><pre><span></span><code><span class="n">mc</span> <span class="o">=</span> <span class="n">MC1stControl</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(),</span> <span class="n">γ</span><span class="o">=</span><span class="mf">.99</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="o">**</span><span class="n">demoTR</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_70_0.png" /></p>
<h3 id="solution-2">Solution 2</h3>
<p>Also we can compensate for the above issue, we would need to set up a reward function that allows the agent to quickly realise when it stuck in some not useful policy.</p>
<div class="highlight"><pre><span></span><code><span class="n">env1</span> <span class="o">=</span> <span class="n">grid</span><span class="p">(</span><span class="n">reward</span><span class="o">=</span><span class="s1">&#39;reward_1&#39;</span><span class="p">)</span>
<span class="n">mcc</span> <span class="o">=</span> <span class="n">MC1stControl</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">env1</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">demoTR</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_72_0.png" /></p>
<p>Compare the above policy with the one produced by the DP solution in lesson 2. You will notice that the MC solution does not give a comprehensive solution from all states because we do not start from different cells. The starting position is fixed. The exploration nature of the policy allowed the agent to develop an <em>understanding</em> through its Q function of where it should head if it finds itself in a specific cell. The Markovian property is essential in guaranteeing that this can be safely assumed.</p>
<p>You might have noticed that although the task is very straightforward, the agent detoured a bit from the simplest straight path that leads to the goal. Bear in mind that we are adopting an εgreedy policy by default, which means that the agent will take some explorative actions 10% of the time. But this should not have prevented the maxQ policy from pointing towards the goal. This is because of the nature of MC itself and its sampling averages. The next section demonstrates how we can overcome this difficulty.</p>
<p>We can play with the exploration but that is needs lots of trail and is not straightforward.</p>
<div class="highlight"><pre><span></span><code><span class="n">mc</span> <span class="o">=</span> <span class="n">MC1stControl</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(),</span> <span class="n">γ</span><span class="o">=</span><span class="mf">.97</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">ε</span><span class="o">=</span><span class="mf">.5</span><span class="p">,</span> <span class="n">dε</span><span class="o">=</span><span class="mf">.99</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="o">**</span><span class="n">demoTR</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_76_0.png" /></p>
<div class="highlight"><pre><span></span><code><span class="n">mc</span> <span class="o">=</span> <span class="n">MC1stControl</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(),</span> <span class="n">γ</span><span class="o">=</span><span class="mf">.97</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">ε</span><span class="o">=</span><span class="mf">.5</span><span class="p">,</span> <span class="n">εT</span><span class="o">=</span><span class="mi">3000</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="o">**</span><span class="n">demoTR</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_77_0.png" /></p>
<div class="highlight"><pre><span></span><code><span class="n">mcc</span> <span class="o">=</span> <span class="n">MC1stControl</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(</span><span class="n">reward</span><span class="o">=</span><span class="s1">&#39;reward_1&#39;</span><span class="p">),</span> <span class="n">γ</span><span class="o">=</span><span class="mf">.97</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">ε</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">dε</span><span class="o">=</span><span class="mf">.999</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="o">**</span><span class="n">demoTR</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_78_0.png" /></p>
<h3 id="demos-related">Demos Related</h3>
<p>Note how the arrows represent the policy change from one episode to another. We have turned off showing the agent's movements inside all but the last episode because it is usually unnecessary. If you want to see a specific episode, just set the 'episodes' variable to it, and you will be able to. For example, if you want to see what is happening in episode 3, set episodes=3 (to guarantee seeing exactly the same episode every time you repeat the experiment, you would need to fix the seed).</p>
<p>Please differentiate between seeing the arrows changing from one episode to another and when you see them changing inside an episode. Inside an episode, the arrow of a state changes only when the agent visits a state. The exception to this rule is when we use planning (or eligibility traces), where an arrow of a cell can change way after it has been visited. This is because we store those visits in these methods and reuse them in our updates. We will examine planning and eligibility traces in later lessons.</p>
<p>We can choose to plot and animate at the same time. Bear in mind that this will slow down the process a bit. 
If the learning is slow anyway, such as in Atari, then it makes sense to animate and plot as it will keep you informed about which episode your agent is in and how well it is doing so far!
It is better to keep the animation turned off for ordinary classical environments as those do not take time anyway.</p>
<p><strong>Important Notes Regarding Demos</strong>
Notice how the visualisation behaves for demoGame() vs demoTR(), demoT() and demoR(). </p>
<p>When we use demoTR(), demoT() or demoR(), the algorithm will train the agent silently without showing the plots and then at the last few episodes (as per view variable which is usually 1; meaning last episode), it shows a demo and then shows the performance plots. demoTR(), demoT() and demoR() are usually more efficient and take up less time.</p>
<p>On the other hand, when we use demoGame(), the algorithm will show the performance plots progress live from one episode to another. Then, in the final few episode (according to view), it shows a demo and the performance plots. During the demo, the plots disappear, and they reappear at the end. This helps keep our code as tidy and efficient as possible.</p>
<div class="highlight"><pre><span></span><code><span class="n">mcc</span> <span class="o">=</span> <span class="n">MC1stControl</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(</span><span class="n">reward</span><span class="o">=</span><span class="s1">&#39;reward_1&#39;</span><span class="p">),</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">demoGame</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_82_0.png" /></p>
<h3 id="plots-without-demos">Plots without Demos</h3>
<p>Let us see how to plot only
The most efficient way is just to turn off animate and set plotT or plotR to True.</p>
<div class="highlight"><pre><span></span><code><span class="n">mcc</span> <span class="o">=</span> <span class="n">MC1stControl</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(</span><span class="n">reward</span><span class="o">=</span><span class="s1">&#39;reward_1&#39;</span><span class="p">),</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">plotT</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">animate</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_84_0.png" /></p>
<p>we can also plot live as the algorithm is training</p>
<div class="highlight"><pre><span></span><code><span class="n">mcc</span> <span class="o">=</span> <span class="n">MC1stControl</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(</span><span class="n">reward</span><span class="o">=</span><span class="s1">&#39;reward_1&#39;</span><span class="p">),</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">plotT</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">animate</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_86_0.png" /></p>
<h3 id="demos-without-training">Demos without Training</h3>
<p>We can also run without training, Unfortunately that means that we would have to loose the training traces of the last few episodes that we want to visualise and replace them with the latest performance after training. </p>
<div class="highlight"><pre><span></span><code><span class="n">mcc</span><span class="o">.</span><span class="n">ep</span> <span class="o">=</span> <span class="n">mcc</span><span class="o">.</span><span class="n">ep</span> <span class="o">-</span> <span class="mi">5</span>
<span class="n">mcc</span><span class="o">.</span><span class="n">plotT</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">mcc</span><span class="o">.</span><span class="n">visual</span> <span class="o">=</span> <span class="kc">True</span>
<span class="c1"># mcc.underhood=&#39;maxQ&#39; # uncomment to see also the policy</span>
<span class="n">mcc</span><span class="o">.</span><span class="n">interact</span><span class="p">(</span><span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">demoGame</span><span class="p">())</span>
</code></pre></div>
<p><img alt="png" src="output_88_0.png" /></p>
<div class="highlight"><pre><span></span><code>&lt;__main__.MC1stControl at 0x12bc096a0&gt;
</code></pre></div>
<p><img alt="png" src="output_88_2.png" /></p>
<h3 id="resume-training-after-stopping-it-during-the-allocated-episodes">Resume training after stopping it during the allocated episodes</h3>
<p>We can also resume training after we have stopped it. This can be very useful when we are faced with an error outside our control while training. For example when we train a robot simulation it is sometimes necessary to stop training if the environment become irresponsive. This mechanism is tested below, to do so, run the first cell which train for 1000000 episode, stop the training by pressing on the stop button above or by esc then i i, then execute the next cell.</p>
<div class="highlight"><pre><span></span><code><span class="n">mcc</span> <span class="o">=</span> <span class="n">MC1stControl</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(</span><span class="n">reward</span><span class="o">=</span><span class="s1">&#39;reward_1&#39;</span><span class="p">),</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">plotT</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">animate</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_90_0.png" /></p>
<div class="highlight"><pre><span></span><code><span class="n">mcc</span><span class="o">.</span><span class="n">interact</span><span class="p">(</span><span class="n">resume</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>&lt;__main__.MC1stControl at 0x12dbb09b0&gt;
</code></pre></div>
<p><img alt="png" src="output_91_1.png" /></p>
<h3 id="extend-training-beyond-the-initial-number-of-episodes">Extend training beyond the initial number of episodes</h3>
<p>We can also extend training, for example we trained for a 100 episodes and then we would like to extend training for another 100 episodes. To do so we just call interact(episodes=120). We show this below.</p>
<div class="highlight"><pre><span></span><code><span class="n">mcc</span><span class="o">.</span><span class="n">interact</span><span class="p">(</span><span class="n">resume</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">120</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>&lt;__main__.MC1stControl at 0x12dbb09b0&gt;
</code></pre></div>
<p><img alt="png" src="output_93_1.png" /></p>
<p>Decreasing the number of episodes will not result any training and it will not remove early training as it should. Below we show that and we also show that it will not matter whether we pass the episodes in interact() or via the algorithms instance.</p>
<div class="highlight"><pre><span></span><code><span class="n">mcc</span><span class="o">.</span><span class="n">episodes</span> <span class="o">=</span> <span class="mi">90</span>
<span class="n">mcc</span><span class="o">.</span><span class="n">interact</span><span class="p">(</span><span class="n">resume</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>&lt;__main__.MC1stControl at 0x12dbb09b0&gt;
</code></pre></div>
<p><img alt="png" src="output_95_1.png" /></p>
<h2 id="incremental-constant-mc-every-visit-mc-prediction">Incremental constant-α MC: Every-visit MC Prediction</h2>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">MC</span><span class="p">(</span><span class="n">MRP</span><span class="p">):</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">init</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">store</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="c1"># ----------------------------- 🌘 offline, MC learning: end-of-episode learning ----------------------    </span>
    <span class="k">def</span><span class="w"> </span><span class="nf">offline</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># obtain the return for the latest episode</span>
        <span class="n">Gt</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">t</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">s</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
            <span class="n">rn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>

            <span class="n">Gt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">γ</span><span class="o">*</span><span class="n">Gt</span> <span class="o">+</span> <span class="n">rn</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">α</span><span class="o">*</span><span class="p">(</span><span class="n">Gt</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">])</span>
</code></pre></div>
<p>This type of algorithmic design is more flexible and will be used in general in RL instead of the implementation that requires storing the sums or averages.</p>
<h3 id="apply-incremental-mc-on-prediction-problem">Apply incremental MC on prediction problem</h3>
<p>Let us try our new shiny prediction algorithm on the random walk problem.</p>
<div class="highlight"><pre><span></span><code><span class="n">mc</span> <span class="o">=</span> <span class="n">MC</span><span class="p">(</span> <span class="n">α</span><span class="o">=</span><span class="mf">.02</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="o">**</span><span class="n">demoV</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_101_0.png" /></p>
<p>Notice how jumpy the MC is.</p>
<h2 id="incremental-mcc-every-visit-mc-control">Incremental MCC: Every-visit MC Control</h2>
<div class="highlight"><pre><span></span><code><span class="c1"># note that the name has double C: we are dealing with MC+Control</span>
<span class="k">class</span><span class="w"> </span><span class="nc">MCC</span><span class="p">(</span><span class="n">MDP</span><span class="p">()):</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">init</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">store</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="c1"># ---------------------------- 🌘 offline, MC learning: end-of-episode learning 🧑🏻‍🏫 -----------------------    </span>
    <span class="k">def</span><span class="w"> </span><span class="nf">offline</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>  
        <span class="c1"># obtain the return for the latest episode</span>
        <span class="n">Gt</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">t</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">s</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
            <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
            <span class="n">rn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>

            <span class="n">Gt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">γ</span><span class="o">*</span><span class="n">Gt</span> <span class="o">+</span> <span class="n">rn</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">α</span><span class="o">*</span><span class="p">(</span><span class="n">Gt</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">])</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">mcc</span> <span class="o">=</span> <span class="n">MCC</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(</span><span class="n">reward</span><span class="o">=</span><span class="s1">&#39;reward1&#39;</span><span class="p">),</span> <span class="n">α</span><span class="o">=</span><span class="mf">.2</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">demoQ</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_105_0.png" /></p>
<div class="highlight"><pre><span></span><code><span class="n">env2x3</span> <span class="o">=</span> <span class="n">Grid</span><span class="p">(</span><span class="n">gridsize</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>  <span class="n">s0</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">goals</span><span class="o">=</span><span class="p">[</span><span class="mi">5</span><span class="p">],</span> <span class="n">figsize</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
<span class="n">V0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">.3</span><span class="p">,</span> <span class="mf">.4</span><span class="p">,</span> <span class="mf">.5</span><span class="p">,</span> <span class="mf">.2</span><span class="p">,</span> <span class="mf">.3</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span><span class="o">*</span><span class="mi">10</span>
<span class="n">env2x3</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="n">underhood</span><span class="o">=</span><span class="s1">&#39;V&#39;</span><span class="p">,</span> <span class="n">V</span><span class="o">=</span><span class="n">V0</span><span class="p">)</span>
</code></pre></div>
<p><img alt="png" src="output_106_0.png" /></p>
<div class="highlight"><pre><span></span><code><span class="n">mcc</span> <span class="o">=</span> <span class="n">MCC</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">env2x3</span><span class="p">,</span> <span class="n">α</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">γ</span><span class="o">=</span><span class="mf">.9</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">demoQ</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_107_0.png" /></p>
<div class="highlight"><pre><span></span><code><span class="n">mcc</span><span class="o">.</span><span class="n">s</span><span class="p">[:</span><span class="n">mcc</span><span class="o">.</span><span class="n">t</span><span class="o">+</span><span class="mi">2</span><span class="p">]</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>array([0, 3, 4, 5], dtype=uint32)
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">mcc</span><span class="o">.</span><span class="n">Q</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>array([[0.   , 0.   , 0.   , 0.081],
       [0.   , 0.   , 0.   , 0.   ],
       [0.   , 0.   , 0.   , 0.   ],
       [0.   , 0.09 , 0.   , 0.   ],
       [0.   , 0.1  , 0.   , 0.   ],
       [0.   , 0.   , 0.   , 0.   ]])
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">s</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">ε</span><span class="o">=</span><span class="mf">.1</span>
<span class="n">πε</span> <span class="o">=</span> <span class="p">[</span><span class="n">ε</span><span class="o">/</span><span class="mi">4</span><span class="p">]</span><span class="o">*</span><span class="n">env2x3</span><span class="o">.</span><span class="n">nA</span> 
<span class="c1"># print(sum(πε))</span>
<span class="n">πε</span><span class="p">[</span><span class="n">mcc</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">s</span><span class="p">]</span><span class="o">.</span><span class="n">argmax</span><span class="p">()]</span><span class="o">+=</span><span class="mi">1</span><span class="o">-</span><span class="n">ε</span>

<span class="c1"># print((πε))</span>
<span class="n">mcc</span><span class="o">.</span><span class="n">π</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="c1"># mcc.Q#*[ε/4, ε/4, ε/4, 1-ε+ε/4]</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">mcc</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">nS</span><span class="p">)</span>
<span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">mcc</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">nS</span><span class="p">):</span>
    <span class="c1"># V[s]= (mcc.Q[s]*mcc.π(s)).sum()</span>
    <span class="c1"># V[s]= (mcc.Q[s]@mcc.π(s))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">mcc</span><span class="o">.</span><span class="n">V_from_Q</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>

<span class="c1"># mcc.V</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>0.07493
0.0
0.0
0.08325
0.0925
0.0
</code></pre></div>
<h3 id="apply-incremental-mc-on-control-problem">Apply incremental MC on control problem</h3>
<div class="highlight"><pre><span></span><code><span class="n">mcc</span> <span class="o">=</span> <span class="n">MCC</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(</span><span class="n">reward</span><span class="o">=</span><span class="s1">&#39;reward100&#39;</span><span class="p">),</span> <span class="n">α</span><span class="o">=</span><span class="mf">.2</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">demoQ</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_112_0.png" /></p>
<p>We can also pass the seed to the interact() function</p>
<div class="highlight"><pre><span></span><code><span class="n">mcc</span> <span class="o">=</span> <span class="n">MCC</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(</span><span class="n">reward</span><span class="o">=</span><span class="s1">&#39;reward1&#39;</span><span class="p">),</span> <span class="n">α</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>  <span class="o">**</span><span class="n">demoQ</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div>
<p><img alt="png" src="output_114_0.png" /></p>
<div class="highlight"><pre><span></span><code><span class="n">mcc</span> <span class="o">=</span> <span class="n">MCC</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(),</span> <span class="n">α</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>  <span class="o">**</span><span class="n">demoQ</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div>
<p><img alt="png" src="output_115_0.png" /></p>
<p>As we can see, although we solved the issue of tracking a non-stationary policy when we used a constant learning rate α, and we tried to use a reward function that gives immediate feedback to each step instead of a delayed reward, but still the performance is not as good as we wished for. This is due to our final issue, which is the action precedence that we set up to prefer left over right. If we change this precedence, it will help the agent to immediately find the goal, however, we set it up this way to make the problem more challenging. Consider changing this precedence to see the effect.</p>
<div class="highlight"><pre><span></span><code><span class="n">mcc</span><span class="o">.</span><span class="n">α</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>0.1
</code></pre></div>
<p>Let us animate and show progress at the same time, as we said earlier this will slow the learning due to animation overhead.</p>
<div class="highlight"><pre><span></span><code><span class="n">mcc</span> <span class="o">=</span> <span class="n">MCC</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(</span><span class="n">reward</span><span class="o">=</span><span class="s1">&#39;reward1&#39;</span><span class="p">),</span> <span class="n">α</span><span class="o">=</span><span class="mf">.001</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">animate</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">demoQ</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div>
<p><img alt="png" src="output_119_0.png" /></p>
<p>We can also just visualise the last 2 episodes.</p>
<div class="highlight"><pre><span></span><code><span class="n">mcc</span> <span class="o">=</span> <span class="n">MCC</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(</span><span class="n">reward</span><span class="o">=</span><span class="s1">&#39;reward1&#39;</span><span class="p">),</span> <span class="n">α</span><span class="o">=</span><span class="mf">.001</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">view</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">visual</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">interact</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div>
<p><img alt="png" src="output_121_0.png" /></p>
<p><img alt="png" src="output_121_1.png" /></p>
<div class="highlight"><pre><span></span><code><span class="n">mcc</span><span class="o">.</span><span class="n">view</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>2
</code></pre></div>
<h2 id="reinforce-mc-for-policy-gradient">REINFORCE: MC for Policy Gradient</h2>
<p>So far, we have only seen how to estimate a value function to deduce a policy from this value function and then improve the policy by preferring a greedy action with a bit of exploration (as in ε-greedy policy). When we allow the agent to act according to this new policy, its value function might change, so we must re-estimate the value function. We go into iterations of this process until the policy and value function are both stable (converge). We also saw that we could integrate both operations seamlessly into one iteration, as in the value-iteration algorithm in Dynamic Programming. We can even do both stages in one <em>step</em> as in Q-learning or Sarsa, as we shall see in the next lesson. The policy improvement theorem and the Generalised Policy Iteration process guarantee all of this. The primary approach we took to achieve learning for an <strong>action-value</strong> method is to <strong>minimise an error function</strong> between our estimate of a value function and the actual value function. Since the real value function is unavailable, we replaced it with some samples (unbiased as in MC and biased as in TD that we will see later).</p>
<p><strong>Policy gradient</strong> algorithms, on the other hand, attempt to <strong>maximise an objective function</strong> instead of minimising an error function. 
Can you think of a function that, if we maximise, will help us solve the RL problem...? pause for a moment and think.</p>
<p>As you might have guessed, the value function can be used as an objective function. The objective here is to change the policy to maximise the value function. </p>
<p>Directly estimating the policy means we are not using a value function to express the policy as in the e-greedy. Instead, we are using the value function to learn the policy directly. So, our algorithm does not need to learn the value function explicitly; it can learn a set of parameters that will maximise the value function without knowing what the value function is. It will come as a consequence of learning a policy. In the same way that we did not need to learn a policy in the value-function approach, we learned a value function, and as a consequence of minimising the error, we can deduce a policy from the learned value function. This is the fundamental difference between value function approaches and policy gradient approaches.</p>
<p>Estimating the policy directly means we do not need to restrict the policy parameters to value function estimates and their ranges. The policy parameters that represent the preferences to select an action are free to take on any range of values as long as they comparatively form a cohesive policy that maximises the value function by dictating which action to choose in a specific state. This is a major advantage because the value function is strictly tied to the sum of rewards values, while a policy need not have this coupling. This will give us more freedom in using classification architectures when we use function approximation which excels in deducing the best action for a state, instead of using a regression architecture to regress a value function which is usually more prone to initial condition issues and are harder to train.</p>
<p>The best policy representation in a policy gradient method is the action selection softmax policy we came across in our last few lessons. This is a smooth function that, unlike ε-greedy, allows the changes in the probabilities to be continuous and integrates very well with policy gradient methods. One of the significant advantages of policy gradient methods (the policy is differentiable everywhere, unlike stepwise ε-greedy functions) is that it provides better guarantees of convergence than ε-greedy due to this smoothness (ε-greedy can change abruptly due to small changes in the action-value functions, while softmax just smoothly increases or decrease the probability of selecting ana action when its action-value function changes).</p>
<p>We start our coverage for policy gradient methods with an offline method; REINFORCE. REINFORCE is an algorithm that takes a <em>policy gradient</em> approach instead of an action-value function approach. The idea is simple, given that an episode provides a sample of returns for the visited states, at the end of an episode, we will take the values of the states and use them to guide our search to find the optimal policy that maximises the value function. </p>
<p><strong>Note</strong> that policy gradient sections in this lesson, and the next are based on chapter 13 of our book. They can be read as they appear in the notebook or delayed until the end of lesson 9.</p>
<h2 id="policy-gradient-class">Policy Gradient Class</h2>
<p>The softmax is the default policy selection procedure for Policy Gradient methods. <span class="arithmatex">\(\tau\)</span> acts like an exploration factor (more on that later) and we need to one-hot encoding for the actions.</p>
<div class="highlight"><pre><span></span><code><span class="n">Ia</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Ia</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Ia</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Ia</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>[[1. 0. 0. 0.]
 [0. 1. 0. 0.]
 [0. 0. 1. 0.]
 [0. 0. 0. 1.]]
(4,)
[0. 1. 0. 0.]
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">pi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="n">pi</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">=</span><span class="mi">1</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pi</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>[0. 1. 0. 0.]
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">PG</span><span class="p">(</span><span class="n">MDP</span><span class="o">=</span><span class="n">MDP</span><span class="p">(</span><span class="n">MRP</span><span class="p">)):</span>
    <span class="k">class</span><span class="w"> </span><span class="nc">PG</span><span class="p">(</span><span class="n">MDP</span><span class="p">):</span>
        <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">τ</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">τmin</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">dτ</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">Tτ</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">):</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kw</span><span class="p">)</span>
            <span class="c1"># set up hyper parameters</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">τ</span> <span class="o">=</span> <span class="n">τ</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">τ0</span> <span class="o">=</span> <span class="n">τ</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dτ</span> <span class="o">=</span> <span class="n">dτ</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">Tτ</span> <span class="o">=</span> <span class="n">Tτ</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">τmin</span> <span class="o">=</span> <span class="n">τmin</span>

            <span class="c1"># softmax is the default policy selection procedure for Policy Gradient methods</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">policy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">τsoftmax</span>

        <span class="c1">#------------------------------------- add some more policies types 🧠-------------------------------</span>

        <span class="c1"># returns a softmax action</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">τsoftmax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
            <span class="n">Qs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q_</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dτ</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">τ</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">τmin</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">τ</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">dτ</span><span class="p">)</span>              <span class="c1"># exponential decay</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">Tτ</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">τ</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">τmin</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">τ0</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">Tτ</span><span class="p">)</span> <span class="c1"># linear      decay</span>

            <span class="n">exp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">Qs</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">τ</span><span class="p">)</span>
            <span class="n">maxAs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">Qs</span><span class="o">==</span><span class="n">Qs</span><span class="o">.</span><span class="n">max</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span>
            <span class="c1">#a = choice(self.env.nA, 1, p=exp/exp.sum())[0]</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">choices</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">nA</span><span class="p">),</span> <span class="n">weights</span><span class="o">=</span><span class="n">exp</span><span class="o">/</span><span class="n">exp</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">isamax</span> <span class="o">=</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">maxAs</span>
            <span class="k">return</span> <span class="n">a</span>

        <span class="c1"># overriding π() in parent class MDP: </span>
        <span class="c1"># in MDP π() returns probabilities according to a εgreedy,</span>
        <span class="c1"># in PG  π() returns probabilities accroding to a τsoftmax, while</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">π</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
            <span class="n">Qs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q_</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
            <span class="n">exp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">Qs</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">τ</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">exp</span><span class="o">/</span><span class="n">exp</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="k">if</span> <span class="n">a</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">(</span><span class="n">exp</span><span class="o">/</span><span class="n">exp</span><span class="o">.</span><span class="n">sum</span><span class="p">())[</span><span class="n">a</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">PG</span>
</code></pre></div>
<p>Ok, so now we are ready to define our REINFORCE algorithm. This algorithm and other policy gradient algorithm always have two updates, one for V and one for Q. In other words, the action-value function update will be guided by the state-value update. We usually call the first update deals that with V, the critic and the second update that deals with Q the actor.</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">REINFORCE</span><span class="p">(</span><span class="n">PG</span><span class="p">()):</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">init</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">store</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="c1"># -------------------- 🌘 offline, REINFORCE: MC for policy gradient methdos ----------------------</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">offline</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">π</span><span class="p">,</span> <span class="n">γ</span><span class="p">,</span> <span class="n">α</span><span class="p">,</span> <span class="n">τ</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">π</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">γ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">α</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">τ</span>
        <span class="c1"># obtain the return for the latest episode</span>
        <span class="n">Gt</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">γt</span> <span class="o">=</span> <span class="n">γ</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">t</span>                  <span class="c1"># efficient way to calculate powers of γ backwards</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">t</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span> <span class="c1"># reversed to make it easier to calculate Gt</span>
            <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">s</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
            <span class="n">a</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
            <span class="n">rn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>

            <span class="n">Gt</span> <span class="o">=</span> <span class="n">γ</span><span class="o">*</span><span class="n">Gt</span> <span class="o">+</span> <span class="n">rn</span>
            <span class="n">δ</span> <span class="o">=</span> <span class="n">Gt</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">]</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="o">+=</span> <span class="n">α</span><span class="o">*</span><span class="n">δ</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="n">α</span><span class="o">*</span><span class="n">δ</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">π</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">))</span><span class="o">*</span><span class="n">γt</span><span class="o">/</span><span class="n">τ</span>
            <span class="n">γt</span> <span class="o">/=</span> <span class="n">γ</span>
</code></pre></div>
<h2 id="the-role-of-discount-factor-gamma-in-policy-gradient-methods">The Role of Discount Factor <span class="arithmatex">\(\gamma\)</span> in Policy Gradient Methods</h2>
<p><span class="arithmatex">\(\gamma\)</span> seems to play a more important role in policy gradient methods than in action-value methods.
The next few examples show how <span class="arithmatex">\(\gamma\)</span> can make the difference between convergence and divergence.
The main issue is, as usual, whether the <em>reward</em> is delayed or there is an intermediate reward. If the reward is delayed, we would need to assign <span class="arithmatex">\(\gamma\)</span> values that are &lt; 1 so that the sum of the rewards is discounted, which helps the agent differentiate between longer and shorter paths solution. However, <span class="arithmatex">\(\gamma\)</span> also plays a role in convergence when the reward is not delayed. It complements the role that <span class="arithmatex">\(\tau\)</span> plays in the SoftMax policy. Therefore, instead of tuning <span class="arithmatex">\(\tau\)</span> we can reduce <span class="arithmatex">\(\gamma\)</span> specifically when the goal reward is 0, and the intermediate reward is -1 (reward_0) function. Let us see some examples:</p>
<p>The below shows that REINFORCE diverges when τ=1, γ=1, for (reward='reward_1').</p>
<div class="highlight"><pre><span></span><code><span class="n">reinforce</span> <span class="o">=</span> <span class="n">REINFORCE</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(</span><span class="n">reward</span><span class="o">=</span><span class="s1">&#39;reward0&#39;</span><span class="p">),</span> <span class="n">α</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">τ</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">γ</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">plotT</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_132_0.png" /></p>
<div class="highlight"><pre><span></span><code><span class="n">reinforce</span> <span class="o">=</span> <span class="n">REINFORCE</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(</span><span class="n">reward</span><span class="o">=</span><span class="s1">&#39;reward_1&#39;</span><span class="p">),</span> <span class="n">α</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">τ</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">γ</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">plotT</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_133_0.png" /></p>
<p>Below we increase the value of <span class="arithmatex">\(\tau\)</span> to deal with this issue of diveregnce.</p>
<div class="highlight"><pre><span></span><code><span class="n">reinforce</span> <span class="o">=</span> <span class="n">REINFORCE</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(</span><span class="n">reward</span><span class="o">=</span><span class="s1">&#39;reward0&#39;</span><span class="p">),</span> <span class="n">α</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">τ</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">γ</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">10</span> <span class="p">,</span> <span class="o">**</span><span class="n">demoQ</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_135_0.png" /></p>
<div class="highlight"><pre><span></span><code><span class="n">reinforce</span> <span class="o">=</span> <span class="n">REINFORCE</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(</span><span class="n">reward</span><span class="o">=</span><span class="s1">&#39;reward0&#39;</span><span class="p">),</span> <span class="n">α</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">τ</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">γ</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">10</span> <span class="p">,</span> <span class="o">**</span><span class="n">demoQ</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_136_0.png" /></p>
<div class="highlight"><pre><span></span><code><span class="n">s</span><span class="o">=</span><span class="mi">31</span>
<span class="nb">print</span><span class="p">(</span><span class="n">reinforce</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">s</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">reinforce</span><span class="o">.</span><span class="n">π</span><span class="p">(</span><span class="n">s</span><span class="p">))</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code>[-9.39998085  5.37572334 -6.32250724 -6.20289915]
[0.00692454 0.95365926 0.01931528 0.02010092]
</code></pre></div>
<p>As we can see REINFORCE converged when we increase <span class="arithmatex">\(\tau\)</span> which helped the values in SoftMax to become appropriatly smaller to help the algorithm to converge.</p>
<p>Let us now decrease the value of <span class="arithmatex">\(\gamma&lt;1\)</span> and keep <span class="arithmatex">\(\tau=1\)</span></p>
<div class="highlight"><pre><span></span><code><span class="n">reinforce</span> <span class="o">=</span> <span class="n">REINFORCE</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(</span><span class="n">reward</span><span class="o">=</span><span class="s1">&#39;reward0&#39;</span><span class="p">),</span> <span class="n">α</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">τ</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">γ</span><span class="o">=</span><span class="mf">.98</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="o">**</span><span class="n">demoQ</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_140_0.png" /></p>
<div class="highlight"><pre><span></span><code><span class="n">reinforce</span> <span class="o">=</span> <span class="n">REINFORCE</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(</span><span class="n">reward</span><span class="o">=</span><span class="s1">&#39;reward_1&#39;</span><span class="p">),</span> <span class="n">α</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">τ</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">γ</span><span class="o">=</span><span class="mf">.98</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="o">**</span><span class="n">demoQ</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_141_0.png" /></p>
<p>As we can see decreasing <span class="arithmatex">\(\gamma\)</span> helped REINFORCE immensely to converge. Although the reward that we used is 'reward_1' which is not delayed, but discounting the return helped the value function to be more meaningful for the problem in hand which helped in turn the policy to be more appropriate for the problem in hand.  </p>
<p>Let us now increase <span class="arithmatex">\(\tau\)</span> and keep <span class="arithmatex">\(\gamma&lt;1\)</span> this will reveal another role for <span class="arithmatex">\(\tau\)</span>.</p>
<div class="highlight"><pre><span></span><code><span class="n">reinforce</span> <span class="o">=</span> <span class="n">REINFORCE</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(</span><span class="n">reward</span><span class="o">=</span><span class="s1">&#39;reward0&#39;</span><span class="p">),</span> <span class="n">α</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">τ</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">γ</span><span class="o">=</span><span class="mf">.98</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="o">**</span><span class="n">demoQ</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_144_0.png" /></p>
<div class="highlight"><pre><span></span><code><span class="n">reinforce</span> <span class="o">=</span> <span class="n">REINFORCE</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(</span><span class="n">reward</span><span class="o">=</span><span class="s1">&#39;reward_1&#39;</span><span class="p">),</span> <span class="n">α</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">τ</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">γ</span><span class="o">=</span><span class="mf">.98</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="o">**</span><span class="n">demoQ</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_145_0.png" /></p>
<p>As we can see increasing <span class="arithmatex">\(\tau\)</span> while using <span class="arithmatex">\(\gamma &lt;1\)</span> did not help. We will mostly therefore use <span class="arithmatex">\(\gamma &lt;1\)</span> for our policy gradient methods.  </p>
<h2 id="delayed-reward-and-reinforce">Delayed Reward and REINFORCE</h2>
<p>Let us now look at a delayed reward</p>
<div class="highlight"><pre><span></span><code><span class="n">reinforce</span> <span class="o">=</span> <span class="n">REINFORCE</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(),</span> <span class="n">α</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">τ</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">γ</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">plotT</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_148_0.png" /></p>
<div class="highlight"><pre><span></span><code><span class="n">reinforce</span> <span class="o">=</span> <span class="n">REINFORCE</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(),</span> <span class="n">α</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">τ</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">γ</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">plotT</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_149_0.png" /></p>
<p>Note that whether we increase or decrease <span class="arithmatex">\(\tau\)</span> her, it will not help REINFORCE to converge since the value function that the algorithmm is learning is not appropriate when <span class="arithmatex">\(\gamma=1\)</span>.</p>
<div class="highlight"><pre><span></span><code><span class="n">reinforce</span> <span class="o">=</span> <span class="n">REINFORCE</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(),</span> <span class="n">α</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">τ</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">γ</span><span class="o">=</span><span class="mf">.98</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="o">**</span><span class="n">demoQ</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_151_0.png" /></p>
<p>As we can see exploration is actually good, so let us decrease this exploration and see if that helps to reach faster convergence given that the environment is rather simple.</p>
<div class="highlight"><pre><span></span><code><span class="n">reinforce</span> <span class="o">=</span> <span class="n">REINFORCE</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(),</span> <span class="n">α</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">τ</span><span class="o">=</span><span class="mf">.2</span><span class="p">,</span> <span class="n">γ</span><span class="o">=</span><span class="mf">.98</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="o">**</span><span class="n">demoQ</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_153_0.png" /></p>
<p>Note how the algorithm converged faster but to sub-optimal solution.</p>
<div class="highlight"><pre><span></span><code><span class="n">reinforce</span> <span class="o">=</span> <span class="n">REINFORCE</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(),</span> <span class="n">α</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">τ</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">γ</span><span class="o">=</span><span class="mf">.98</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="o">**</span><span class="n">demoQ</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_155_0.png" /></p>
<p>Note how exploration lead to a fully covered environment but to a slower convergence.</p>
<h2 id="conclusion">Conclusion</h2>
<p>In this lesson, we studied the properties of Monte Carlo algorithms for prediction and control. We started by covering a basic first visit MC method that averages the returns similar to what we did in lesson 1, this time for the associative problem (i.e., when we have states that we select specific actions for, un-associated problems do not have states and have been studied in lesson 1). We have then created an incremental MC algorithm that allows us to average the returns in a step-by-step manner. To that end, we have developed an essential MRP class that will carry the step-by-step and episode-by-episode interaction with an MRP environment, and then we added a useful set of visualisation routines. We have further inherited the MRP class in an MDP class that defines policies that depend on the Q function to obtain a suitable policy for an agent (i.e., control).
We noted that MC needed to wait until the episode was finished to carry out updates. In the next unit, we will study full online algorithms that mitigate this shortcoming of MC with the cost of bootstrapping. We will be using the MRP and MDP classes that we developed here.</p>
<h1 id="units-conclusion">Unit's conclusion</h1>
<p>This lesson concludes our unit where we have studied important formulations of RL all of which assumed that we use a table representation for our state space. In the next unit, we will study other offline and fully online RL algorithms that use bootstrapping. Additionally, we will study planning algorithms and then use function approximation instead of a table to represent the state space that can be continuous and infinite.</p>
<h2 id="your-turn">Your turn</h2>
<ol>
<li>Change the probabilities of the actions in the stationary policy of an RMP class, use this policy in a random walk process and see the effect on the results.</li>
<li>Alter the MDP class to include a softmax policy, use this policy in a maze environment, instead of the e-greedy, and see the effect on the results.</li>
<li>There might be some potential for saving compute time if we check if the maxQ is unique, try to alter the greedy policy and observe if this potential can be realised.</li>
<li>Create a new class MCsoft algorithm that inherent from PG. This new class would have access to a SoftMax policy which is its default policy. Now apply it on the grid() and see the result.</li>
</ol>







  
  






                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright Abdulrahman Altahhan
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.sections"], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.f1b6f286.min.js"></script>
      
        <script src="../../javascript/tablecontentsoverride.js"></script>
      
        <script src="../../javascript/config.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>