
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
        <meta name="author" content="Dr Abdulrahman Altahhan">
      
      
      
        <link rel="prev" href="../lesson5/lesson5.html">
      
      
        <link rel="next" href="../lesson7/lesson7.html">
      
      
      <link rel="icon" href="../../img/favicon.jpg">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.4">
    
    
      
        <title>6. Monte Carlo - Reinforcement Learning and Robotics</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.8608ea7d.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../css/extra.css">
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mediaelement/4.2.16/mediaelementplayer.min.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#lesson-6-tabular-methods-monte-carlo" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
        <aside class="md-banner">
          <div class="md-banner__inner md-grid md-typeset">
            
            
    <div id="versionIndicator"><b>Version:</b> 04.06.21.a</div>
    <img id="customlogo" src="../../img/logo.svg" alt="University of Leeds logo.">

          </div>
          
        </aside>
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../index.html" title="Reinforcement Learning and Robotics" class="md-header__button md-logo" aria-label="Reinforcement Learning and Robotics" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Reinforcement Learning and Robotics
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              6. Monte Carlo
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../../index.html" class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit1/lesson1/lesson1.html" class="md-tabs__link">
          
  
  Unit 1

        </a>
      </li>
    
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../lesson5/lesson5.html" class="md-tabs__link">
          
  
  Unit 2

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit3/lesson8/lesson8.html" class="md-tabs__link">
          
  
  Unit 3

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit4/lesson12/lesson12.html" class="md-tabs__link">
          
  
  Unit 4

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit5/lesson15/lesson15.html" class="md-tabs__link">
          
  
  Unit 5

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../unit6/lesson18/lesson18.html" class="md-tabs__link">
          
  
  Unit 6

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../index.html" title="Reinforcement Learning and Robotics" class="md-nav__button md-logo" aria-label="Reinforcement Learning and Robotics" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Reinforcement Learning and Robotics
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../index.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 1
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Unit 1
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit1/lesson1/lesson1.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    1. Tabular Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit1/lesson2/lesson2.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    2. K-Arm Bandit
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit1/lesson3/lesson3.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    3. MDP
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit1/lesson4/lesson4.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    4. ROS
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Unit 2
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Unit 2
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lesson5/lesson5.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    5. Dynamic Programming
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    6. Monte Carlo
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="lesson6.html" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    6. Monte Carlo
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#overview" class="md-nav__link">
    <span class="md-ellipsis">
      Overview
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#plan" class="md-nav__link">
    <span class="md-ellipsis">
      Plan
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Plan">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bellman-equationsreminder" class="md-nav__link">
    <span class="md-ellipsis">
      Bellman Equations(reminder)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dynamic-programming-is-model-based" class="md-nav__link">
    <span class="md-ellipsis">
      Dynamic Programming is Model based
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#monte-carlo-model-free-methods" class="md-nav__link">
    <span class="md-ellipsis">
      Monte Carlo: Model-Free Methods
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#first-visit-monte-carlo-policy-evaluation-prediction" class="md-nav__link">
    <span class="md-ellipsis">
      First-Visit Monte Carlo Policy Evaluation (prediction)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="First-Visit Monte Carlo Policy Evaluation (prediction)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#random-walk-problem" class="md-nav__link">
    <span class="md-ellipsis">
      Random Walk Problem
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Random Walk Problem">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#problem-setup" class="md-nav__link">
    <span class="md-ellipsis">
      Problem Setup
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#policies" class="md-nav__link">
    <span class="md-ellipsis">
      Policies
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Policies">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#epsilon-greedy-policy" class="md-nav__link">
    <span class="md-ellipsis">
      \(\epsilon\)-Greedy Policy
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#softmax-policy" class="md-nav__link">
    <span class="md-ellipsis">
      Softmax Policy
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#first-visit-mc-control" class="md-nav__link">
    <span class="md-ellipsis">
      First-visit MC control
    </span>
  </a>
  
    <nav class="md-nav" aria-label="First-visit MC control">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#applying-mc-on-a-control-problem" class="md-nav__link">
    <span class="md-ellipsis">
      Applying MC on a control problem
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exploration-exploitation-and-exploring-starts" class="md-nav__link">
    <span class="md-ellipsis">
      Exploration-Exploitation and Exploring Starts
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-role-of-the-discount-factor-gamma-for-delayed-reward" class="md-nav__link">
    <span class="md-ellipsis">
      The role of the discount factor \(\gamma\) for delayed reward
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#solution-1" class="md-nav__link">
    <span class="md-ellipsis">
      Solution 1
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#solution-2" class="md-nav__link">
    <span class="md-ellipsis">
      Solution 2
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#every-visit-mc-prediction" class="md-nav__link">
    <span class="md-ellipsis">
      Every-visit MC Prediction
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Every-visit MC Prediction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#first-visit-vs-every-visit-mc" class="md-nav__link">
    <span class="md-ellipsis">
      First-Visit vs. Every-Visit MC
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#constant-mc-incremental-every-visit-mc-prediction" class="md-nav__link">
    <span class="md-ellipsis">
      Constant-α MC: Incremental every-visit MC Prediction
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mrp-mdp-and-pg-classes" class="md-nav__link">
    <span class="md-ellipsis">
      MRP, MDP and PG classes
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MRP, MDP and PG classes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#incremental-constant-mc-prediction-with-python" class="md-nav__link">
    <span class="md-ellipsis">
      Incremental constant-α MC (prediction) with Python
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#incremental-mcc-every-visit-mc-control" class="md-nav__link">
    <span class="md-ellipsis">
      Incremental MCC: Every-visit MC Control
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Incremental MCC: Every-visit MC Control">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#incremental-constant-mc-with-python" class="md-nav__link">
    <span class="md-ellipsis">
      Incremental constant-α MC with Python
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reinforce-mc-for-policy-gradient" class="md-nav__link">
    <span class="md-ellipsis">
      REINFORCE: MC for Policy Gradient
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-role-of-discount-factor-gamma-in-policy-gradient-methods" class="md-nav__link">
    <span class="md-ellipsis">
      The Role of Discount Factor \(\gamma\) in Policy Gradient Methods
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conclusion" class="md-nav__link">
    <span class="md-ellipsis">
      Conclusion
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#your-turn" class="md-nav__link">
    <span class="md-ellipsis">
      Your turn
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lesson7/lesson7.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    7. Mobile Robots
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 3
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Unit 3
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson8/lesson8.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    8. Temporal Difference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson9/lesson9.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    9. n-Step Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson10/lesson10.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    10. Planning in RL(optional)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit3/lesson11/lesson11.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    11. Localisation and SLAM
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 4
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Unit 4
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit4/lesson12/lesson12.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    12. Function Approximation Methods
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit4/lesson13/lesson13.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    13. Linear Approximation for Prediction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit4/lesson14/lesson14.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    14. Linear Approximation for Control
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 5
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Unit 5
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit5/lesson15/lesson15.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    15. Linear Approximation with Eligibility Traces(prediction and control)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit5/lesson16/lesson16.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    16. Nonlinear Approximation for Control
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit5/lesson17/lesson17.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    17. Application on Robot Navigation
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Unit 6
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Unit 6
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../unit6/lesson18/lesson18.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    18. Application on Games(optional)
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#overview" class="md-nav__link">
    <span class="md-ellipsis">
      Overview
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#plan" class="md-nav__link">
    <span class="md-ellipsis">
      Plan
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Plan">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#bellman-equationsreminder" class="md-nav__link">
    <span class="md-ellipsis">
      Bellman Equations(reminder)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dynamic-programming-is-model-based" class="md-nav__link">
    <span class="md-ellipsis">
      Dynamic Programming is Model based
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#monte-carlo-model-free-methods" class="md-nav__link">
    <span class="md-ellipsis">
      Monte Carlo: Model-Free Methods
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#first-visit-monte-carlo-policy-evaluation-prediction" class="md-nav__link">
    <span class="md-ellipsis">
      First-Visit Monte Carlo Policy Evaluation (prediction)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="First-Visit Monte Carlo Policy Evaluation (prediction)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#random-walk-problem" class="md-nav__link">
    <span class="md-ellipsis">
      Random Walk Problem
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Random Walk Problem">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#problem-setup" class="md-nav__link">
    <span class="md-ellipsis">
      Problem Setup
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#policies" class="md-nav__link">
    <span class="md-ellipsis">
      Policies
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Policies">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#epsilon-greedy-policy" class="md-nav__link">
    <span class="md-ellipsis">
      \(\epsilon\)-Greedy Policy
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#softmax-policy" class="md-nav__link">
    <span class="md-ellipsis">
      Softmax Policy
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#first-visit-mc-control" class="md-nav__link">
    <span class="md-ellipsis">
      First-visit MC control
    </span>
  </a>
  
    <nav class="md-nav" aria-label="First-visit MC control">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#applying-mc-on-a-control-problem" class="md-nav__link">
    <span class="md-ellipsis">
      Applying MC on a control problem
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exploration-exploitation-and-exploring-starts" class="md-nav__link">
    <span class="md-ellipsis">
      Exploration-Exploitation and Exploring Starts
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-role-of-the-discount-factor-gamma-for-delayed-reward" class="md-nav__link">
    <span class="md-ellipsis">
      The role of the discount factor \(\gamma\) for delayed reward
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#solution-1" class="md-nav__link">
    <span class="md-ellipsis">
      Solution 1
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#solution-2" class="md-nav__link">
    <span class="md-ellipsis">
      Solution 2
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#every-visit-mc-prediction" class="md-nav__link">
    <span class="md-ellipsis">
      Every-visit MC Prediction
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Every-visit MC Prediction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#first-visit-vs-every-visit-mc" class="md-nav__link">
    <span class="md-ellipsis">
      First-Visit vs. Every-Visit MC
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#constant-mc-incremental-every-visit-mc-prediction" class="md-nav__link">
    <span class="md-ellipsis">
      Constant-α MC: Incremental every-visit MC Prediction
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#mrp-mdp-and-pg-classes" class="md-nav__link">
    <span class="md-ellipsis">
      MRP, MDP and PG classes
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MRP, MDP and PG classes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#incremental-constant-mc-prediction-with-python" class="md-nav__link">
    <span class="md-ellipsis">
      Incremental constant-α MC (prediction) with Python
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#incremental-mcc-every-visit-mc-control" class="md-nav__link">
    <span class="md-ellipsis">
      Incremental MCC: Every-visit MC Control
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Incremental MCC: Every-visit MC Control">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#incremental-constant-mc-with-python" class="md-nav__link">
    <span class="md-ellipsis">
      Incremental constant-α MC with Python
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#reinforce-mc-for-policy-gradient" class="md-nav__link">
    <span class="md-ellipsis">
      REINFORCE: MC for Policy Gradient
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-role-of-discount-factor-gamma-in-policy-gradient-methods" class="md-nav__link">
    <span class="md-ellipsis">
      The Role of Discount Factor \(\gamma\) in Policy Gradient Methods
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conclusion" class="md-nav__link">
    <span class="md-ellipsis">
      Conclusion
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#your-turn" class="md-nav__link">
    <span class="md-ellipsis">
      Your turn
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<div><h1 id="lesson-6-tabular-methods-monte-carlo">Lesson 6-Tabular Methods: Monte Carlo</h1>
<p><strong>Learning outcomes</strong></p>
<p>By the end of this lesson, you will be able to: </p>
<ol>
<li>understand the difference between learning the expected return and computing it via dynamic programming</li>
<li>understand the strengths and weaknesses of MC methods</li>
<li>appreciating that MC methods need to wait till the end of the task to obtain its estimate of the expected return</li>
<li>compare MC methods with dynamic programming methods</li>
<li>understand the implication of satisfying and not satisfying the explore-start requirement for the MC control and how to mitigate it via the reward function</li>
<li>understand how to move from prediction to control by extending the V function to a Q function and make use of the idea of generalised policy iteration-GPI</li>
<li>understand how policy gradient methods work and appreciate how they differ from value function methods</li>
</ol>
<h2 id="overview">Overview</h2>
<p>In this lesson, we develop the ideas of Monte Carlo methods. Monte Carlo methods are powerful and widely used in settings other than RL. You may have encountered them in a previous module where they were mainly used for sampling. We will also use them here to sample observations and average their expected returns. Because they average the returns, Monte Carlo methods have to wait until <em>all</em> trajectories are available to estimate the return. Later, we will find out that Temporal Difference methods do not wait until the end of the episode to update their estimate and outperform MC methods.</p>
<p>Note that we have now moved to <em>learning</em> instead of <em>computing</em> the value function and its associated policy. This is because we expect our agent to learn from <em>interacting with the environment</em> instead of using the dynamics of the environment, which is usually hard to compute except for a simple lab-confined environment. </p>
<p>Remember that we are dealing with <em>expected return</em>, and we are either finding an <em>exact solution for this expected return</em> as when we solve the set of Bellman equations or finding an <em>approximate solution for the expected return</em> as in DP or MC.
Remember also that the expected return for a state is the future cumulative discounted rewards given that the agent follows a specific policy.</p>
<p>One pivotal observation that summarises the justification for using MC methods over DP methods is that it is often the case that we are able to interact with the environment instead of obtaining its dynamics due to its complexity and intractability. In other words, interacting with the envoronment is often more direct and easier than obtaining the model of the environment. Therefore, we say that MC are model-free methods.</p>
<h2 id="plan">Plan</h2>
<p>As usual, in general, there are two types of RL problems that we will attempt to design methods to deal with </p>
<ol>
<li>
<p>Prediction problem
For These problems, we will design Policy Evaluation Methods that attempt to find the best estimate for the value function given a policy.</p>
</li>
<li>
<p>Control problems 
For These problems, we will design Value Iteration methods that utilise Generalised Policy Iteration. They attempt to find the best policy by estimating an action-value function for a current policy and then moving to a better and improved one by often choosing a greedy action. They minimise an error function to improve their value function estimate, used to deduce a policy.
We will then move to Policy Gradient methods that directly estimate a useful policy for the agent by maximising its value function.</p>
</li>
</ol>
<p>We start by assuming that the policy is fixed. This will help us develop an algorithm that predicts the state space's value function (expected return). Then we will move to the policy improvement methods, i.e. these methods that help us to compare and improve our policy with respect to other policies and move to a better policy when necessary. Then we move to the control case (policy iteration methods).</p>
<!-- ## Monte Carlo Methods -->

<h3 id="bellman-equationsreminder">Bellman Equations(reminder)</h3>
<p>As we have seen in a previous lesson, the Bellman equations form the foundation of many RL methods. They define the relationship between the value of a state and the values of successor states.</p>
<p>For a given policy <span class="arithmatex">\( \pi \)</span>, the Bellman Equation for the state-value function is:</p>
<div class="arithmatex">\[
V^\pi(s) = \mathbb{E}_\pi \left[ R_{t+1} + \gamma V^\pi(S_{t+1}) \mid S_t = s \right]
\]</div>
<p>For the action-value function <span class="arithmatex">\( Q^\pi(s, a) \)</span>:</p>
<div class="arithmatex">\[
Q^\pi(s, a) = \mathbb{E}_\pi \left[ R_{t+1} + \gamma Q^\pi(S_{t+1}, A_{t+1}) \mid S_t = s, A_t = a \right]
\]</div>
<h3 id="dynamic-programming-is-model-based">Dynamic Programming is Model based</h3>
<p>As we have seen in the last lesson, Dynamic Programming uses Bellman equations as update rules to iteratively compute value functions and policies using two key steps:</p>
<ol>
<li><strong>Policy Evaluation</strong>: Uses the Bellman equation to obtain an evaluation for a given policy.</li>
<li><strong>Policy Improvement</strong>: Uses the Bellman optimality equation to update the policy towards a better policy.</li>
</ol>
<p>However, DP has a major limitation: it requires complete knowledge of the dynamics model <span class="arithmatex">\( p(s',r | s, a) \)</span>, or transition model <span class="arithmatex">\( p(s'| s, a) \)</span> which is often unavailable in real-world scenarios. In other words, it is a model-based methods.</p>
<h2 id="monte-carlo-model-free-methods">Monte Carlo: Model-Free Methods</h2>
<p>Monte Carlo (MC) methods are a class of RL algorithms used to estimate value functions and optimise policies by using <strong>sampled episodes</strong> instead of full models of the environment. Unlike Dynamic Programming (DP), which requires a known dynamics model <span class="arithmatex">\( p(s',r | s, a) \)</span>, Monte Carlo methods learn <strong>directly from experience</strong> by averaging observed rewards.</p>
<p>Monte Carlo (MC) methods estimate value functions without requiring a dynamics model. Instead, they rely on sampled episodes of experience. The key idea is to approximate value functions by averaging observed returns over multiple episodes.</p>
<p>The fundamental principle behind MC methods is that averaging samples from a distribution approximates its expectation. Since state-value and action-value functions are defined as expected returns, averaging returns over a sufficiently large number of episodes provides a reliable estimate of these functions.</p>
<p>There are some technical conditions that must be met for this approximation to hold, which we will reference when necessary.</p>
<!-- ### Return Definition -->

<p>For an episode consisting of states, actions, and rewards:</p>
<div class="arithmatex">\[
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots
\]</div>
<p>The Monte Carlo estimate of <span class="arithmatex">\( V(s) \)</span> is the average return from all episodes where state <span class="arithmatex">\( s \)</span> was visited.</p>
<div class="arithmatex">\[
V(s) \approx \frac{1}{N} \sum_{i=1}^{N} G_i(s)
\]</div>
<p>Where:</p>
<ul>
<li><span class="arithmatex">\(i\)</span> refers to the index over the episodes, not the time steps.</li>
<li><span class="arithmatex">\(G_i(s)\)</span> is the return (sum of discounted rewards) observed from episode <span class="arithmatex">\(i\)</span>, starting at state <span class="arithmatex">\(s\)</span>.</li>
<li><span class="arithmatex">\(N\)</span> is the total number of episodes used to average the returns.</li>
</ul>
<h2 id="first-visit-monte-carlo-policy-evaluation-prediction">First-Visit Monte Carlo Policy Evaluation (prediction)</h2>
<p>Because MC methods depend entirely on experience, a natural way to approximate the cumulative future discounted reward is by taking their average once they become available through experience. So we must collect the cumulative <em>future</em> discounted reward once this experience has elapsed. In other words, we need to take the sum <em>after</em> the agent has finished an episode for all the rewards obtained from the current state to the end of the episode. Then we average those returns over all of the available episodes. Note that MC methods only apply for episodic tasks, which is one of its limitations in addition to having to wait until the episode is finished.</p>
<p>Note also that the agent can visit the same state more than once inside the same episode. One question that arises from the above averaging is: which visit should be counted? </p>
<p>We can take the sum starting from the first visit, or every visit, each yields a different algorithm. The first-visit algorithm is more suitable for tabular methods, while the every-visit algorithm is more suitable when using function approximation methods (such as neural networks).</p>
<p>One could argue that since we want to estimate the value of a state based on the full horizon of rewards obtained until the end of an episode, we should include only the <em>first visit of the state</em> in the average. This is the basis of the First-visit Monte Carlo (MC) policy evaluation method. First-visit MC estimates the value of a state by averaging the returns from the first time that state is encountered in each episode. Below we show the pseudocode for this algorithm.</p>
<div class="arithmatex">\[
\begin{array}{ll}
\textbf{Algorithm: }  \text{First-Visit Monte Carlo Policy Evaluation} \\
\textbf{Input: } \text{Episodes generated under policy } \pi \\
\textbf{Initialize: }  V(S) \leftarrow 0, N(S) \leftarrow 0, \forall S \in \mathcal{S} \\
\textbf{For each episode: } &amp; \\
\quad \text{Generate an episode: } (S_0, A_0, R_1, S_1, \dots, S_T) &amp; \\
\quad G \leftarrow 0 &amp; \\
\quad \textbf{For each step } t \textbf{ from } T-1 \textbf{ to } 0: &amp; \\
\quad \quad G \leftarrow \gamma G + R_{t+1} &amp; \\
\quad \quad \text{If } S_t \text{ appears first in the episode:} &amp; \\
\quad \quad \quad N(S_t) \leftarrow N(S_t) + 1 &amp; \\
\quad \quad \quad V(S_t) \leftarrow V(S_t) + \frac{1}{N(S_t)}(G - V(S_t)) &amp; \\
\textbf{Return: } V(S), \forall S \in \mathcal{S} \\
\end{array}
\]</div>
<h3 id="random-walk-problem">Random Walk Problem</h3>
<p>The Random Walk problem is a simple <strong>Markov Reward Process (MRP)</strong> used to illustrate value estimation methods. It consists of a finite, linear chain of states, where an agent moves randomly left or right until reaching one of the two terminal states.</p>
<h4 id="problem-setup">Problem Setup</h4>
<ul>
<li>Typically, there are five non-terminal states labeled <span class="arithmatex">\( A, B, C, D, E \)</span> on a 1-d grid world. Another variaiton use 21 states.</li>
<li>Two terminal states exist at both ends.</li>
<li>The agent starts in the center and moves <em>randomly</em> left or right with equal probability.</li>
<li>The episode ends when the agent reaches a terminal state.</li>
<li>A <em>reward of +1</em> is received upon reaching the right terminal state, while the left terminal state gives <em>0 reward</em>.</li>
</ul>
<!-- - The discount factor \( \gamma \) is used to determine state values. -->

<p>This random walk problem is often used to demonstrate <strong>Monte Carlo</strong> and <strong>Temporal-Difference (TD)</strong> learning methods for estimating state-value function. Below we show this problem.</p>
<p><img alt="png" src="output_9_0.png">  </p>
<div class="highlight"><pre><span></span><code>        A     B     C     D     E
</code></pre></div>
<p>Let’s now run MC1st with a useful visualization. The plot displays the true state values of the random walk as black points connected by a black line. The agent’s estimated values are represented by blue points and a blue line, allowing us to visually assess how closely the learned values for states A–D match their actual values. This is acheived by passing plotV=True.</p>
<p>Next to this, an error plot tracks the total error per episode, providing insight into the learning process. Each episode begins in the middle state (C) and ends upon reaching either the far-left or far-right terminal states, which are unnamed since they do not have values to estimate. This is acheived by passing plotE=True.</p>
<p></p><div class="highlight"><pre><span></span><code><span class="n">mc</span> <span class="o">=</span> <span class="n">MC1st</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">randwalk</span><span class="p">(),</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">plotV</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">plotE</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<img alt="png" src="output_40_0.png">
<p>We use **demoV to implicitly pass plotE=True, plotV=True, animate=True, whenever we want to demo a prediction algorithm.</p>
<h2 id="policies">Policies</h2>
<p>Before we move into control, we need to breifly discuss types of policies that balance exporation and exploitation.</p>
<h3 id="epsilon-greedy-policy"><span class="arithmatex">\(\epsilon\)</span>-Greedy Policy</h3>
<p>The <span class="arithmatex">\(\epsilon\)</span>-greedy policy is a popular action selection strategy that balances exploration and exploitation. The policy chooses the action with the highest estimated value most of the time, but with probability <span class="arithmatex">\(\epsilon\)</span>, it selects an action randomly to encourage exploration.</p>
<p>Mathematically, the <span class="arithmatex">\(\epsilon\)</span>-greedy policy can be defined as:</p>
<div class="arithmatex">\[
\pi(a|s) = 
\begin{cases} 
\frac{\epsilon}{|A|}, &amp; \text{with probability } \epsilon \\
1 - \epsilon + \frac{\epsilon}{|A|}, &amp; \text{for the action with the highest value} \\
0, &amp; \text{for all other actions}
\end{cases}
\]</div>
<p>Where:
- <span class="arithmatex">\(\epsilon\)</span> is the probability of exploring (random action).
- <span class="arithmatex">\(|A|\)</span> is the total number of actions available.
- The action with the highest value <span class="arithmatex">\(Q(s,a)\)</span> is selected with probability <span class="arithmatex">\(1-\epsilon + \frac{\epsilon}{|A|}\)</span>.</p>
<h3 id="softmax-policy">Softmax Policy</h3>
<p>The softmax policy selects actions based on a probability distribution that is a function of the action values. The policy assigns a higher probability to actions with higher expected returns, and the probability of selecting action <span class="arithmatex">\(a\)</span> in state <span class="arithmatex">\(s\)</span> is proportional to the exponential of its action-value <span class="arithmatex">\(Q(s, a)\)</span>.</p>
<p>Mathematically, the softmax policy is given by:</p>
<div class="arithmatex">\[
\pi(a|s) = \frac{e^{Q(s,a)/\tau}}{\sum_{b \in A} e^{Q(s,b)/\tau}}
\]</div>
<p>Where:
- <span class="arithmatex">\(\tau\)</span> is the temperature parameter that controls the level of exploration. A high <span class="arithmatex">\(\tau\)</span> encourages more exploration (more uniform distribution), and a low <span class="arithmatex">\(\tau\)</span> leads to more exploitation (choosing the highest value action).
- <span class="arithmatex">\(Q(s,a)\)</span> is the action-value function.</p>
<p>In this way, the softmax policy ensures that actions with higher values are more likely to be chosen, but there is always a chance to explore other actions.</p>
<h2 id="first-visit-mc-control">First-visit MC control</h2>
<p>Now let us extend our first-visit MC prediction to control by updating the Q action-value function instead of the state-value function V.</p>
<div class="arithmatex">\[
\begin{array}{ll}
\textbf{Algorithm: }  \text{First-Visit Monte Carlo Control (Exploring Starts)} \\
\textbf{Input: } \text{Episodes generated under an exploring-starts policy} \\
\textbf{Initialize: }  Q(S, A) \leftarrow 0, N(S, A) \leftarrow 0, \forall S \in \mathcal{S}, A \in \mathcal{A}(S), \pi(S) \leftarrow \text{arbitrary policy}, \forall S \in \mathcal{S} \\
\textbf{For each episode: } &amp; \\
\quad \text{Generate an episode: } (S_0, A_0, R_1, S_1, A_1, \dots, S_T) &amp; \\
\quad G \leftarrow 0 &amp; \\
\quad \textbf{For each step } t \textbf{ from } T-1 \textbf{ to } 0: &amp; \\
\quad \quad G \leftarrow \gamma G + R_{t+1} &amp; \\
\quad \quad \text{If } (S_t, A_t) \text{ appears first in the episode:} &amp; \\
\quad \quad \quad N(S_t, A_t) \leftarrow N(S_t, A_t) + 1 &amp; \\
\quad \quad \quad Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \frac{1}{N(S_t, A_t)}(G - Q(S_t, A_t)) &amp; \\
\quad \quad \text{Update policy: } \pi(S_t) \leftarrow \arg\max_A Q(S_t, A) &amp; \\
\textbf{Return: } Q(S, A), \pi(S), \forall S \in \mathcal{S}, A \in \mathcal{A}(S) \\
\end{array}
\]</div>
<h3 id="applying-mc-on-a-control-problem">Applying MC on a control problem</h3>
<p>Similar to what we did for prediction, we get help from a dictionary that stores a set of useful configurations that we use often. In the case of control, the most useful is plotting the number of steps the agent took to reach a terminal state in each episode or the sum of rewards the agent collected in each episode. Each one of these plots can be useful for certain tasks. Bear in mind that if the reward is given only for reaching the goal location or terminal state, the sum of the rewards plot would be a constant line that does not convey useful information. Below we show each.</p>
<p>Unfortunately, applying the MC control algorithm with the default reward function will not yield a useful policy. This is because the explore-start condition is not satisfied (refer to section 5.4 of our book). In addition, averaging solutions may not perform well because they do not track a changing policy well for non-stationary problems (most of the control problems are non-stationary). To see this, uncomment the lines in the cell below and run it. (Note that we have set up the priorities of the actions in a way that will show this issue (right comes before left and down before up). demoQ is a dicitonary that passes visualisaiton values to the MDP control algorithm.</p>
<p></p><div class="highlight"><pre><span></span><code><span class="n">mc</span> <span class="o">=</span> <span class="n">MC1stControl</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(),</span> <span class="n">γ</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>  <span class="n">seed</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="o">**</span><span class="n">demoQ</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<img alt="png" src="output_67_0.png">
<h3 id="exploration-exploitation-and-exploring-starts">Exploration-Exploitation and Exploring Starts</h3>
<ul>
<li>Exploration-Exploitation Tradeoff: In reinforcement learning, the agent must balance exploration (trying new actions to discover better long-term rewards) with exploitation (choosing known actions that yield high rewards based on current knowledge).</li>
</ul>
<p>In exploration, the agent tries actions that it has not explored enough yet. This is necessary to gather more information about the environment. In exploitation, the agent uses its current knowledge to select actions that maximize the immediate reward based on its existing value estimates. Both of these needs to happen for the agent to learn effectively. <span class="arithmatex">\(\epsilon\)</span>-Greedy and softmax policies has a built-in exploraiton-exploitation capabilities.</p>
<!-- - $\epsilon$-Greedy Policy: A commonly used policy to handle exploration-exploitation:
    - With probability \( \epsilon \), the agent chooses a random action (exploration).
    - With probability \( 1 - \epsilon \), the agent chooses the action that maximizes the value estimate (exploitation).

- Softmax Policy: Instead of choosing actions randomly, the softmax policy selects actions based on a probability distribution derived from the action values. Actions with higher values are more likely to be chosen, but there is always a non-zero chance of selecting less optimal actions (exploration). -->

<ul>
<li>Exploring Starts: This is a method used to ensure complete exploration of the state-action space. In exploring starts, each episode begins with a random state and a random action. This guarantees that over many episodes, every state-action pair will eventually be visited, ensuring unbiased value estimation. This condition dictates that all states must be randomly started with to gurantee convergance. It is needed for 1stMCC since it relies on the getting enough coverage for all of the states.</li>
</ul>
<p>Exploring starts guarantees initial exploration and helps ensure all state-action pairs are visited, making it easier for the agent to discover good policies without biases. Exploration-exploitation policies (like <span class="arithmatex">\(\epsilon\)</span>-greedy or softmax) are methods to balance exploration and exploitation during the agent's learning process. These policies allow for exploration to continue throughout the agent's learning, which is particularly useful when the environment is unknown. Exploration-exploitation balance plays an important role when we move into online methods. But they are still important to achieve an end of episode, and <span class="arithmatex">\(\epsilon\)</span>-greedy is useful to avoid the exploring-start condition. </p>
<p>In short, exploring starts ensures full exploration at the beginning of episodes, while exploration-exploitation policies control the balance between exploration and exploitation throughout the learning process.</p>
<h3 id="the-role-of-the-discount-factor-gamma-for-delayed-reward">The role of the discount factor <span class="arithmatex">\(\gamma\)</span> for delayed reward</h3>
<p><strong>Important Note</strong>
It is always the case that when we use a <em>delayed reward</em> (which is the default reward for our Grid class), the discount factor <span class="arithmatex">\(\gamma\)</span> <strong>must not be set to 1</strong>. This is because the sum of the discounted rewards of each visited state will be equal to the delayed reward itself, which will not give any particular advantage to follow a shorter path, yielding a useless policy. Therefore, we can solve this issue 
1. either by providing a discounted value for <span class="arithmatex">\(\gamma\)</span> that &lt; 1.
1. or by changing the reward to have intermediate steps reward, which, when accumulated, will provide distinguished sums for the different paths and hence help distinguish the shortest path or the policy that will yield an optimal reward.</p>
<h3 id="solution-1">Solution 1</h3>
<p>Below we show how we can simply reduce <span class="arithmatex">\(\gamma\)</span> to solve this issue.</p>
<p></p><div class="highlight"><pre><span></span><code><span class="n">mc</span> <span class="o">=</span> <span class="n">MC1stControl</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(),</span> <span class="n">γ</span><span class="o">=</span><span class="mf">.99</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="o">**</span><span class="n">demoTR</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<img alt="png" src="output_70_0.png">
<h3 id="solution-2">Solution 2</h3>
<p>Also we can compensate for the above issue, we would need to set up a reward function that allows the agent to quickly realise when it stuck in some not useful policy.</p>
<p></p><div class="highlight"><pre><span></span><code><span class="n">env1</span> <span class="o">=</span> <span class="n">grid</span><span class="p">(</span><span class="n">reward</span><span class="o">=</span><span class="s1">'reward_1'</span><span class="p">)</span>
<span class="n">mcc</span> <span class="o">=</span> <span class="n">MC1stControl</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">env1</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">demoTR</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<img alt="png" src="output_72_0.png">
<p>Compare the above policy with the one produced by the DP solution in lesson 2. You will notice that the MC solution does not give a comprehensive solution from all states because we do not start from different cells. The starting position is fixed. The exploration nature of the policy allowed the agent to develop an <em>understanding</em> through its Q function of where it should head if it finds itself in a specific cell. The Markovian property is essential in guaranteeing that this can be safely assumed.</p>
<p>You might have noticed that although the task is very straightforward, the agent detoured a bit from the simplest straight path that leads to the goal. Bear in mind that we are adopting an εgreedy policy by default, which means that the agent will take some explorative actions 10% of the time. But this should not have prevented the maxQ policy from pointing towards the goal. This is because of the nature of MC itself and its sampling averages. The next section demonstrates how we can overcome this difficulty.</p>
<p>We can play with the exploration but that needs lots of trail and is not straightforward.</p>
<div class="highlight"><pre><span></span><code><span class="n">mc</span> <span class="o">=</span> <span class="n">MC1stControl</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(),</span> <span class="n">γ</span><span class="o">=</span><span class="mf">.97</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">ε</span><span class="o">=</span><span class="mf">.5</span><span class="p">,</span> <span class="n">dε</span><span class="o">=</span><span class="mf">.99</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="o">**</span><span class="n">demoTR</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_76_0.png"></p>
<div class="highlight"><pre><span></span><code><span class="n">mcc</span> <span class="o">=</span> <span class="n">MC1stControl</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(</span><span class="n">reward</span><span class="o">=</span><span class="s1">'reward_1'</span><span class="p">),</span> <span class="n">γ</span><span class="o">=</span><span class="mf">.97</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">ε</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">dε</span><span class="o">=</span><span class="mf">.999</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="o">**</span><span class="n">demoTR</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_78_0.png"></p>
<h2 id="every-visit-mc-prediction">Every-visit MC Prediction</h2>
<p>In every-visit Monte Carlo, the value of a state is updated based on the average of returns observed from <em>all</em> visits to that state, not just the first visit. For a state <span class="arithmatex">\( s \)</span>, let <span class="arithmatex">\( G_j(s) \)</span> be the return from the <span class="arithmatex">\( j \)</span>-th occurrence of <span class="arithmatex">\( s \)</span> in some episode. The every-visit MC estimate of <span class="arithmatex">\( V^\pi(s) \)</span> is given by: </p>
<div class="arithmatex">\[
V^\pi(s) \approx \frac{1}{N_s} \sum_{j=1}^{N_s} G_j(s)
\]</div>
<p>where:<br>
- <span class="arithmatex">\( N_s \)</span> is the total number of times state <span class="arithmatex">\( s \)</span> was visited across all episodes.<br>
- <span class="arithmatex">\( G_j(s) \)</span> is the return obtained from the <span class="arithmatex">\( j \)</span>-th visit to <span class="arithmatex">\( s \)</span>, computed as the sum of rewards from that visit to the end of the episode.  </p>
<h3 id="first-visit-vs-every-visit-mc">First-Visit vs. Every-Visit MC</h3>
<table>
<thead>
<tr>
<th></th>
<th><strong>First-Visit MC</strong></th>
<th><strong>Every-Visit MC</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Update Rule</strong></td>
<td>Uses the return from the first visit to a state in each episode</td>
<td>Uses returns from <strong>all</strong> visits to a state in each episode</td>
</tr>
<tr>
<td><strong>Bias/Variance</strong></td>
<td>Lower bias but higher variance</td>
<td>Higher bias but lower variance</td>
</tr>
<tr>
<td><strong>Suitability</strong></td>
<td>Works well when visits to a state within an episode are correlated</td>
<td>Suitable when visits are independent and representative of overall state behavior</td>
</tr>
</tbody>
</table>
<!-- ### When to Use Which? -->
<!-- - First-Visit MC is generally preferred when state occurrences within an episode are highly correlated, as it prevents bias due to repeated visits with varying returns.
- Every-Visit MC is computationally simpler and can work well if state visits are independent and uncorrelated.   -->

<h2 id="constant-mc-incremental-every-visit-mc-prediction">Constant-α MC: Incremental every-visit MC Prediction</h2>
<p>We move now into developing the every-visit MC prediction algorithm further, by allowing it to be incremental. Incrementality is a key aspect in RL, since it allows an algorithm to interact and learn from the environment at the same time in each step, without having to wait until the end of the experience. However, for MC since we have to obtain the returns Gt, we will wait until the end of the episode to obtain the full experience rewards, but then we will <em>apply</em> the updates incrementally, instead of accumulating the full set in one go by averaging. </p>
<p>The first step towards realising this target is to use look into how the averaging can be adjusted gradually (incrementally). Let us assume that the we observed the state <span class="arithmatex">\(s\)</span> at time step <span class="arithmatex">\(t\)</span> with a return <span class="arithmatex">\(G_t\)</span> and the number of times <span class="arithmatex">\(s\)</span> has been visited is <span class="arithmatex">\(N_s\)</span>. We want now to obtain a new estimate <span class="arithmatex">\(V^\pi_{k+1}(s)\)</span> based on our previous estimate <span class="arithmatex">\(V^\pi_{k}(s)\)</span>. The number of ties <span class="arithmatex">\(s\)</span> was observed when we had the estimate <span class="arithmatex">\(V^\pi_{k}(s)\)</span> is <span class="arithmatex">\(N_s-1\)</span>. From the estimate definition we have that: </p>
<div class="arithmatex">\[
    \begin{align*}
        V^\pi_{k+1}(s) =&amp; \frac{1}{N_s} \left(\sum_{j=1}^{N_s-1} G_j(s) + G_t(s)\right) \\
                        =&amp; \frac{1}{N_s} \left((1-N_s)V^\pi_{k}(s) + G_t(s)\right) \\
                        =&amp; V^\pi_{k} (s) + \frac{1}{N_s} \left(G_t -  V^\pi_{k}(s) \right)
    \end{align*}
\]</div>
<p>This idea is similar to what we have discussed in the simple bandit algorihtm.
One issue we have we the averaging is that when <span class="arithmatex">\(N_s\)</span> becomes large the effect of the newly obtained <span class="arithmatex">\(G_t\)</span> diminishes with time since <span class="arithmatex">\(\frac{1}{N_s}\)</span> tends to 0 with time. One way to overcome this issue is by replacing <span class="arithmatex">\(\frac{1}{N_s}\)</span> with a constant <span class="arithmatex">\(\alpha\)</span> that is fixed.
This is much better for when the policy is not stationary (during learning an obtimal policy).</p>
<p>In other words, when we obtian the <span class="arithmatex">\(G_t\)</span> for each time step <span class="arithmatex">\(t\)</span>, we can adjust the estimation of <span class="arithmatex">\(s\)</span> as follow:</p>
<div class="arithmatex">\[
V(S_t) \leftarrow V(S_t) + \alpha \left( G_t - V(S_t) \right)
\]</div>
<p>This method updates the state-value function incrementally using a constant step-size parameter <span class="arithmatex">\( \alpha \)</span> for each visit, allowing the agent to continuously refine its estimates as it observes more returns.</p>
<p>Where:</p>
<ul>
<li><span class="arithmatex">\( V(s) \)</span> is the value of state <span class="arithmatex">\(s\)</span></li>
<li><span class="arithmatex">\( G_t \)</span> is the return (sum of discounted rewards) from state <span class="arithmatex">\( s \)</span> onwards</li>
<li><span class="arithmatex">\( \alpha \)</span> is the constant step-size parameter</li>
</ul>
<p>This method is useful in scenarios where multiple visits to the same state provide valuable updates, and it works well for episodic tasks. Below we show the full algorithm in pseudocode.</p>
<div class="arithmatex">\[
\begin{array}{ll}
\textbf{Algorithm: }  \text{Incremental Constant-}\alpha \text{ Monte Carlo Prediction} \\
\textbf{Input: } \text{Episodes generated under policy } \pi \\
\textbf{Initialize: }  V(s) \leftarrow 0, \forall s \in \mathcal{S}, 0 &lt; \alpha \leq 1 \\
\textbf{For each episode: } &amp; \\
\quad \text{Generate an episode: } (S_0, R_1, S_1, \dots, S_T) &amp; \\
\quad G \leftarrow 0 &amp; \\
\quad \textbf{For each step } t \textbf{ from } T-1 \textbf{ to } 0: &amp; \\
\quad \quad G \leftarrow \gamma G + R_{t+1} &amp; \\
\quad \quad \text{Update state-value estimate:} &amp; \\
\quad \quad \quad V(S_t) \leftarrow V(S_t) + \alpha (G - V(S_t)) &amp; \\
\textbf{Return: } V(s), \forall s \in \mathcal{S} \\
\end{array}
\]</div>
<!-- ### Monte Carlo Control with \(\epsilon\)-Greedy Exploration

To optimize policies, we extend MC to **control**, using the **Generalized Policy Iteration (GPI)** framework. The key idea is:

1. **Policy Evaluation:** Estimate \( Q^\pi(s, a) \) using MC.
2. **Policy Improvement:** Use an **\(\epsilon\)-greedy** strategy to gradually improve the policy.

### Algorithm:

\[
\begin{array}{ll}
\textbf{Algorithm:} & \text{Monte Carlo Control with } \epsilon \text{-greedy policy} \\
\textbf{Initialize:} & Q(s, a) \leftarrow 0, N(s, a) \leftarrow 0, \forall s, a \\
\textbf{For each episode:} & \\
\quad \text{Generate an episode using } \pi_{\epsilon} & \\
\quad G \leftarrow 0 & \\
\quad \textbf{For each step t from T to 1:} & \\
\quad \quad G \leftarrow \gamma G + r_{t+1} & \\
\quad \quad \text{If } (s_t, a_t) \text{ appears first in episode:} & \\
\quad \quad \quad N(s_t, a_t) \leftarrow N(s_t, a_t) + 1 & \\
\quad \quad \quad Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \frac{1}{N(s_t, a_t)}(G - Q(s_t, a_t)) & \\
\quad \quad \text{Update policy } \pi \text{ using } \epsilon \text{-greedy:} & \\
\quad \quad \quad \pi(s) \leftarrow \arg\max_a Q(s, a) \text{ with probability } (1-\epsilon) & \\
\quad \quad \quad \text{Choose random action with probability } \epsilon & \\
\textbf{Return:} & Q(s, a), \pi(s) \forall s, a \\
\end{array}
\] -->

<h2 id="mrp-mdp-and-pg-classes">MRP, MDP and PG classes</h2>
<p>We will evaluate the effectiveness of prediction methods by applying them to a random walk problem. This setup isolates the prediction aspect of an algorithm, focusing purely on estimating the state-value function without involving decision-making. By doing so, we can assess whether a given update rule or algorithm works effectively in the prediction setting.</p>
<p>Once we understand the prediction process, we can extend these methods to control by modifying the update rule to incorporate action-value estimates (Q-values). This transition is typically done within the MDP framework and is a key step in value-based reinforcement learning methods.</p>
<p>Beyond value-based approaches, there is another class of methods called policy-based or <em>policy gradient (PG)</em> methods, which optimise the policy directly instead of using a value function. Unlike value-based methods, policy gradient approaches are typically applied to control problems, such as grid-world mazes, rather than random walk problems.</p>
<p><strong>We use a parent class MRP for any prediciton model-free method which has access to V. We use MDP parent class for any control model-free method, which has access to Q as well as to an <span class="arithmatex">\(\epsilon-\)</span>greedy policy, while the MRP have no policy (it is actually an arbitrary policy). We use PG class for any policy-gradient method, PG has access to both V and Q as well as to a softmax policy.</strong></p>
<h3 id="incremental-constant-mc-prediction-with-python">Incremental constant-α MC (prediction) with Python</h3>
<p>Below we show a direct <em>interpretaiton</em> of the above pseudocode prediction model-free method into a python code. We use MRP as its parent class.</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">MC</span><span class="p">(</span><span class="n">MRP</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">init</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">store</span> <span class="o">=</span> <span class="kc">True</span> <span class="c1"># stores the trajectory of the episode, always needed for offline</span>
    <span class="c1"># ------------------ 🌘 offline, MC learning: end-of-episode learning ------------------</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">offline</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span> <span class="c1"># called at the end of the latest episode</span>
        <span class="n">Gt</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">t</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>          <span class="c1"># go throgh experience backwards as per Gt in update</span>
            <span class="n">s</span><span class="p">,</span> <span class="n">rn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">s</span><span class="p">[</span><span class="n">t</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>       <span class="c1"># retrieve the state and reward for past step t </span>
            <span class="n">Gt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">γ</span><span class="o">*</span><span class="n">Gt</span> <span class="o">+</span> <span class="n">rn</span>                  <span class="c1"># calculate the return for past time step t</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">α</span><span class="o">*</span><span class="p">(</span><span class="n">Gt</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">])</span> <span class="c1"># update the state-value function</span>
</code></pre></div>
<p>This type of algorithmic design is more flexible and will be used in general in RL instead of the implementation that requires storing the sums or averages.</p>
<p>Let us try our new shiny prediction algorithm on the random walk problem.</p>
<p></p><div class="highlight"><pre><span></span><code><span class="n">mc</span> <span class="o">=</span> <span class="n">MC</span><span class="p">(</span> <span class="n">α</span><span class="o">=</span><span class="mf">.02</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="o">**</span><span class="n">demoV</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<img alt="png" src="output_101_0.png">
<h2 id="incremental-mcc-every-visit-mc-control">Incremental MCC: Every-visit MC Control</h2>
<p><strong>Incremental MCC (Monte Carlo Control)</strong> for <strong>Every-visit MC</strong> Control is an approach used for estimating optimal policies through interaction with the environment. It incrementally updates both the state-action value function <span class="arithmatex">\( Q\)</span> and the policy using every visit to a state-action pair. The method involves updating the action-value function based on the observed returns, and it uses a constant step-size parameter <span class="arithmatex">\( \alpha \)</span> to control the magnitude of updates:</p>
<div class="arithmatex">\[
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left( G_t - Q(S_t, A_t) \right)
\]</div>
<p>Where:</p>
<ul>
<li><span class="arithmatex">\( Q(s, a) \)</span> is the action-value function,</li>
<li><span class="arithmatex">\( G_t \)</span> is the return from state-action pair <span class="arithmatex">\( (s, a) \)</span>,</li>
<li><span class="arithmatex">\( \alpha \)</span> is the constant step-size parameter.</li>
</ul>
<p>This method is applied iteratively to improve the policy, choosing actions greedily with respect to the estimated <span class="arithmatex">\( Q \)</span>-values. Below we show the pseudocode for thsi algorithm.</p>
<div class="arithmatex">\[
\begin{array}{ll}
\textbf{Algorithm: }  \text{Incremental Constant-}\alpha \text{ Monte Carlo Control} \\
\textbf{Input: } \text{Episodes generated under an } \varepsilon\text{-greedy policy } \pi \\
\textbf{Initialize: }  Q(S, A) \leftarrow 0, \forall S \in \mathcal{S}, A \in \mathcal{A}(S), 0 &lt; \alpha \leq 1 \\
\textbf{For each episode: } &amp; \\
\quad \text{Generate an episode: } (S_0, A_0, R_1, S_1, A_1, \dots, S_T) &amp; \\
\quad G \leftarrow 0 &amp; \\
\quad \textbf{For each step } t \textbf{ from } T-1 \textbf{ to } 0: &amp; \\
\quad \quad G \leftarrow \gamma G + R_{t+1} &amp; \\
\quad \quad \text{Update action-value estimate:} &amp; \\
\quad \quad \quad Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (G - Q(S_t, A_t)) &amp; \\
\quad \quad \text{Update policy: } \pi(S_t) \leftarrow \arg\max_A Q(S_t, A) \text{ (with } \varepsilon\text{-greedy exploration)} &amp; \\
\textbf{Return: } Q(S, A), \pi(S), \forall S \in \mathcal{S}, A \in \mathcal{A}(S) \\
\end{array}
\]</div>
<h3 id="incremental-constant-mc-with-python">Incremental constant-α MC with Python</h3>
<p>Below we show a direct <em>interpretaiton</em> of the above pseudocode into a python code. We use a parent class MDP parent class for this control model-free method.</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">MCC</span><span class="p">(</span><span class="n">MDP</span><span class="p">()):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">init</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">store</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="c1"># ------------------ 🌘 offline, MC learning: end-of-episode learning 🧑🏻‍🏫 ---------------</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">offline</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>  <span class="c1"># called at the end of the latest episode</span>
        <span class="n">Gt</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">t</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">rn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">s</span><span class="p">[</span><span class="n">t</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="p">[</span><span class="n">t</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># retrieve the state, action and reward for past step t </span>
            <span class="n">Gt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">γ</span><span class="o">*</span><span class="n">Gt</span> <span class="o">+</span> <span class="n">rn</span>                          <span class="c1"># update Gt incrementally</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">α</span><span class="o">*</span><span class="p">(</span><span class="n">Gt</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">])</span>     <span class="c1"># update the action-value function</span>
</code></pre></div>
<div class="highlight"><pre><span></span><code><span class="n">mcc</span> <span class="o">=</span> <span class="n">MCC</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(</span><span class="n">reward</span><span class="o">=</span><span class="s1">'reward1'</span><span class="p">),</span> <span class="n">α</span><span class="o">=</span><span class="mf">.2</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">demoQ</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_105_0.png"></p>
<!-- ### Apply incremental MC on control problem -->

<p>As we can see, although we solved the issue of tracking a non-stationary policy when we used a constant learning rate α, and we tried to use a reward function that gives immediate feedback to each step instead of a delayed reward, but still the performance is not as good as we wished for. This is due to our final issue, which is the action precedence that we set up to prefer left over right. If we change this precedence, it will help the agent to immediately find the goal, however, we set it up this way to make the problem more challenging. Consider changing this precedence to see the effect.</p>
<h2 id="reinforce-mc-for-policy-gradient">REINFORCE: MC for Policy Gradient</h2>
<p>So far, we have only seen how to estimate a value function to deduce a policy from this value function and then improve the policy by preferring a greedy action with a bit of exploration (as in ε-greedy policy). When we allow the agent to act according to this new policy, its value function might change, so we must re-estimate the value function. We go into iterations of this process until the policy and value function are both stable (converge). We also saw that we could integrate both operations seamlessly into one iteration, as in the value-iteration algorithm in Dynamic Programming. We can even do both stages in one <em>step</em> as in Q-learning or Sarsa, as we shall see in the next lesson. The policy improvement theorem and the Generalised Policy Iteration process guarantee all of this. The primary approach we took to achieve learning for an <strong>action-value</strong> method is to <strong>minimise an error function</strong> between our estimate of a value function and the actual value function. Since the real value function is unavailable, we replaced it with some samples (unbiased as in MC and biased as in TD that we will see later).</p>
<p><strong>Policy gradient</strong> algorithms, on the other hand, attempt to <strong>maximise an objective function</strong> instead of minimising an error function. 
Can you think of a function that, if we maximise, will help us solve the RL problem...? pause for a moment and think.</p>
<p>As you might have guessed, the value function can be used as an objective function. The objective here is to change the policy to maximise the value function. </p>
<p>Directly estimating the policy means we are not using a value function to express the policy as in the e-greedy. Instead, we are using the value function to learn the policy directly. So, our algorithm does not need to learn the value function explicitly; it can learn a set of parameters that will maximise the value function without knowing what the value function is. It will come as a consequence of learning a policy. In the same way that we did not need to learn a policy in the value-function approach, we learned a value function, and as a consequence of minimising the error, we can deduce a policy from the learned value function. This is the fundamental difference between value function approaches and policy gradient approaches.</p>
<p>Estimating the policy directly means we do not need to restrict the policy parameters to value function estimates and their ranges. The policy parameters that represent the preferences to select an action are free to take on any range of values as long as they comparatively form a cohesive policy that maximises the value function by dictating which action to choose in a specific state. This is a major advantage because the value function is strictly tied to the sum of rewards values, while a policy need not have this coupling. This will give us more freedom in using classification architectures when we use function approximation which excels in deducing the best action for a state, instead of using a regression architecture to regress a value function which is usually more prone to initial condition issues and are harder to train.</p>
<p>The best policy representation in a policy gradient method is the action selection softmax policy we came across in our last few lessons. This is a smooth function that, unlike ε-greedy, allows the changes in the probabilities to be continuous and integrates very well with policy gradient methods. One of the significant advantages of policy gradient methods (the policy is differentiable everywhere, unlike stepwise ε-greedy functions) is that it provides better guarantees of convergence than ε-greedy due to this smoothness (ε-greedy can change abruptly due to small changes in the action-value functions, while softmax just smoothly increases or decrease the probability of selecting ana action when its action-value function changes).</p>
<p>We start our coverage for policy gradient methods with an offline method; REINFORCE. REINFORCE is an algorithm that takes a <em>policy gradient</em> approach instead of an action-value function approach. The idea is simple, given that an episode provides a sample of returns for the visited states, at the end of an episode, we will take the values of the states and use them to guide our search to find the optimal policy that maximises the value function. </p>
<!-- **Note** that policy gradient sections in this lesson, and the next are based on chapter 13 of our book. They can be read as they appear in the notebook or delayed until the end of lesson 9. -->

<!-- ### Policy Gradient Class -->
<p>The softmax is the default policy selection procedure for Policy Gradient methods. <span class="arithmatex">\(\tau\)</span> acts like an exploration factor (more on that later) and we need to one-hot encoding for the actions.</p>
<p>Ok, so now we are ready to define our REINFORCE algorithm. This algorithm and other policy gradient algorithm always have two updates, one for V and one for Q. In other words, the action-value function update will be guided by the state-value update. We usually call the first update deals that with V, the critic and the second update that deals with Q the actor. Below we show the python code for this algorithm.</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">REINFORCE</span><span class="p">(</span><span class="n">PG</span><span class="p">()):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">init</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">store</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="c1"># -------------------- 🌘 offline, REINFORCE: MC for policy gradient methdos ----------------------</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">offline</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">π</span><span class="p">,</span> <span class="n">γ</span><span class="p">,</span> <span class="n">α</span><span class="p">,</span> <span class="n">τ</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">π</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">γ</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">α</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">τ</span>
        <span class="c1"># obtain the return for the latest episode</span>
        <span class="n">Gt</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">γt</span> <span class="o">=</span> <span class="n">γ</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">t</span>                           <span class="c1"># efficient way to calculate powers of γ backwards</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">t</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>          <span class="c1"># backwards</span>
            <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">rn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">s</span><span class="p">[</span><span class="n">t</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="p">[</span><span class="n">t</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">r</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">Gt</span> <span class="o">=</span> <span class="n">γ</span><span class="o">*</span><span class="n">Gt</span> <span class="o">+</span> <span class="n">rn</span>                        <span class="c1"># update Gt incrementally</span>
            <span class="n">δ</span> <span class="o">=</span> <span class="n">Gt</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">]</span>                    <span class="c1"># obtain the error</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="o">+=</span> <span class="n">α</span><span class="o">*</span><span class="n">δ</span>                    <span class="c1"># update V as per the error</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="n">α</span><span class="o">*</span><span class="n">δ</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">π</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">))</span><span class="o">*</span><span class="n">γt</span><span class="o">/</span><span class="n">τ</span>  <span class="c1"># update Q as per the erro with complement of the policy π</span>
            <span class="n">γt</span> <span class="o">/=</span> <span class="n">γ</span>
</code></pre></div>
<h2 id="the-role-of-discount-factor-gamma-in-policy-gradient-methods">The Role of Discount Factor <span class="arithmatex">\(\gamma\)</span> in Policy Gradient Methods</h2>
<p><span class="arithmatex">\(\gamma\)</span> seems to play a more important role in policy gradient methods than in action-value methods.
The next few examples show how <span class="arithmatex">\(\gamma\)</span> can make the difference between convergence and divergence.
The main issue is, as usual, whether the <em>reward</em> is delayed or there is an intermediate reward. If the reward is delayed, we would need to assign <span class="arithmatex">\(\gamma\)</span> values that are &lt; 1 so that the sum of the rewards is discounted, which helps the agent differentiate between longer and shorter paths solution. However, <span class="arithmatex">\(\gamma\)</span> also plays a role in convergence when the reward is not delayed. It complements the role that <span class="arithmatex">\(\tau\)</span> plays in the SoftMax policy. Therefore, instead of tuning <span class="arithmatex">\(\tau\)</span> we can reduce <span class="arithmatex">\(\gamma\)</span> specifically when the goal reward is 0, and the intermediate reward is -1 (reward_0) function. Let us see some examples:</p>
<p>Below we increase the value of <span class="arithmatex">\(\tau\)</span> to deal with this issue of diveregnce.</p>
<div class="highlight"><pre><span></span><code><span class="n">reinforce</span> <span class="o">=</span> <span class="n">REINFORCE</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(</span><span class="n">reward</span><span class="o">=</span><span class="s1">'reward0'</span><span class="p">),</span> <span class="n">α</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">τ</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">γ</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">10</span> <span class="p">,</span> <span class="o">**</span><span class="n">demoQ</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_135_0.png"></p>
<p>As we can see REINFORCE converged when we increase <span class="arithmatex">\(\tau\)</span> which helped the values in SoftMax to become appropriatly smaller to help the algorithm to converge.</p>
<p>Let us now decrease the value of <span class="arithmatex">\(\gamma&lt;1\)</span> and keep <span class="arithmatex">\(\tau=1\)</span></p>
<div class="highlight"><pre><span></span><code><span class="n">reinforce</span> <span class="o">=</span> <span class="n">REINFORCE</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(</span><span class="n">reward</span><span class="o">=</span><span class="s1">'reward0'</span><span class="p">),</span> <span class="n">α</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">τ</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">γ</span><span class="o">=</span><span class="mf">.98</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="o">**</span><span class="n">demoQ</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_140_0.png"></p>
<p>As we can see decreasing <span class="arithmatex">\(\gamma\)</span> helped REINFORCE immensely to converge. Although the reward that we used is 'reward_1' which is not delayed, but discounting the return helped the value function to be more meaningful for the problem in hand which helped in turn the policy to be more appropriate for the problem in hand.  </p>
<p>Let us now increase <span class="arithmatex">\(\tau\)</span> and keep <span class="arithmatex">\(\gamma&lt;1\)</span> this will reveal another role for <span class="arithmatex">\(\tau\)</span>.</p>
<div class="highlight"><pre><span></span><code><span class="n">reinforce</span> <span class="o">=</span> <span class="n">REINFORCE</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">grid</span><span class="p">(</span><span class="n">reward</span><span class="o">=</span><span class="s1">'reward0'</span><span class="p">),</span> <span class="n">α</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">τ</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">γ</span><span class="o">=</span><span class="mf">.98</span><span class="p">,</span> <span class="n">episodes</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="o">**</span><span class="n">demoQ</span><span class="p">())</span><span class="o">.</span><span class="n">interact</span><span class="p">()</span>
</code></pre></div>
<p><img alt="png" src="output_144_0.png"></p>
<p>As we can see increasing <span class="arithmatex">\(\tau\)</span> while using <span class="arithmatex">\(\gamma &lt;1\)</span> did not help. We will mostly therefore use <span class="arithmatex">\(\gamma &lt;1\)</span> for our policy gradient methods.  </p>
<p>Note how exploration lead to a fully covered environment but to a slower convergence.</p>
<h2 id="conclusion">Conclusion</h2>
<p>In this lesson, we studied the properties of Monte Carlo algorithms for prediction and control. We started by covering a basic first visit MC method that averages the returns similar to what we did in lesson 1, this time for the associative problem (i.e., when we have states that we select specific actions for, un-associated problems do not have states and have been studied in lesson 1). We have then created an incremental MC algorithm that allows us to average the returns in a step-by-step manner. To that end, we have developed an essential MRP class that will carry the step-by-step and episode-by-episode interaction with an MRP environment, and then we added a useful set of visualisation routines. We have further inherited the MRP class in an MDP class that defines policies that depend on the Q function to obtain a suitable policy for an agent (i.e., control).
We noted that MC needed to wait until the episode was finished to carry out updates. In the next unit, we will study full online algorithms that mitigate this shortcoming of MC with the cost of bootstrapping. We will be using the MRP and MDP classes that we developed here.</p>
<p>Monte Carlo methods provide a powerful alternative to Dynamic Programming for RL problems where the environment’s transition model is unknown. They estimate value functions from sampled episodes and improve policies using exploration strategies like <span class="arithmatex">\(\epsilon\)</span>-greedy. We covered two types of methods:</p>
<ul>
<li>Policy Evaluation: Use MC to estimate state values.</li>
<li>Policy Control: Improve policies using MC-based action-value estimates.</li>
</ul>
<p><strong>Advantages</strong></p>
<ul>
<li>Model-free: No need to know the environment’s transition probabilities.  </li>
<li>Simple and intuitive: Works by averaging sampled returns.  </li>
<li>Works well for episodic tasks.  </li>
</ul>
<p><strong>Limitations</strong></p>
<ul>
<li>Requires complete episodes, which may not always be feasible.  </li>
<li>Convergence can be slow compared to Temporal-Difference (TD) methods.  </li>
</ul>
<p>In the next unit, we will explore Temporal-Difference (TD) learning methods, which update state-value estimates without requiring complete episodes. Instead, TD methods rely on one-step updates, much like Value Iteration incrementally improves policies. We will also introduce TD-based action-value methods for control, including SARSA and Q-learning, which use the Q action-value function to learn optimal policies.</p>
<p><strong>Reading</strong>:
For further reading you can consult chapter 5 from the Sutton and Barto <a href="http://incompleteideas.net/book/RLbook2020.pdf">book</a>. The policy gradient sections in this lesson, and the next are based on chapter 13 of our book. They can be read as they appear in the notebook or delayed until the end of lesson 9.</p>
<h2 id="your-turn">Your turn</h2>
<p>Now it is time to experiment further and interact with code in <a href="../../workseets/worksheet6.ipynb">worksheet6</a>.</p></div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright Abdulrahman Altahhan
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.sections"], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.f1b6f286.min.js"></script>
      
        <script src="../../javascript/tablecontentsoverride.js"></script>
      
        <script src="../../javascript/config.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mediaelement/4.2.16/mediaelement-and-player.min.js"></script>
      
    
  </body>
</html>